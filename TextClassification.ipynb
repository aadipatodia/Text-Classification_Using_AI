{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadipatodia/Text-Classification_Using_AI/blob/main/TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "QSF9MRBmIUun",
        "outputId": "ab7ed7a3-8e05-458c-844a-a48f427d95ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=17f3077c0404ac431ad2261a9c4de92598b2e0011e00d8c491fa42699fbdb1c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect\n",
        "!pip install --upgrade sentence-transformers transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNSUPERVISED MODEL WITH TIME COLUMNS**"
      ],
      "metadata": {
        "id": "cRasd_M9gvvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yNKhhC7hI6d",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "from langdetect import detect, DetectorFactory\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import cudf\n",
        "from cuml.cluster import KMeans\n",
        "import google.generativeai as genai\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Set Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBHwfAgTs-RzC7uF4QzUSA30_HfMR9MwZQ\"\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "except KeyError:\n",
        "    print(\" ERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "    print(\"Set it in PowerShell: $env:GEMINI_API_KEY = 'your_api_key_here'\")\n",
        "    exit()\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# Download NLTK stopwords\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Define stopwords These words are considered less meaningful for categorization and are typically removed or ignored during text analysis.\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'area', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it', 'its', 'her', 'their', 'our',\n",
        "    'what', 'where', 'how', 'why', 'who', 'whom', 'which', 'whether',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hr', 'hrs', 'hour', 'hours', 'time', 'date', 'week', 'month', 'year', 'ago',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'code', 'id', 'location', 'address', 'phone', 'mobile', 'call', 'report', 'registered',\n",
        "    'ok', 'yes', 'no', 'not', 'hi', 'hello', 'sir', 'madam', 'pls', 'please', 'regards', 'type', 'urban', 'complaint', 'detail', 'general',\n",
        "    'kv', 'tf', 'na', 'service', 'request', 'feedback', 'query', 'regarding', 'about', 'given', 'areas', 'village'\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Removes extra whitespace and standardizes text (generic version).\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', str(text).strip()).lower()\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def vectorized_time_categorization(df, remark_col, duration_col):\n",
        "\n",
        "    # Optimized time categorization using pandas str.extract.\n",
        "\n",
        "    print(\"     [Preprocessing] Vectorized time categorization...\")\n",
        "\n",
        "    # Applies regular expressions (hour_pattern, day_pattern) across the entire Series to extract numerical values associated with time units\n",
        "    # (e.g., (\\d+\\.?\\d*) captures numbers, (?:hr|hrs|hour|hours|h) matches various hour terms non-capturingly).\n",
        "    # This avoids slow row-by-row Python loops.\n",
        "\n",
        "    start_time = time.time()\n",
        "    duration_series = df[duration_col] if duration_col in df.columns else df[remark_col]\n",
        "    duration_series = duration_series.fillna(\"\").str.lower().apply(clean_text)\n",
        "    hour_pattern = r'(\\d+\\.?\\d*)\\s*(?:hr|hrs|hour|hours|h)'\n",
        "    day_pattern = r'(\\d+\\.?\\d*)\\s*(?:day|days)'\n",
        "\n",
        "    # Extract hours and days\n",
        "    hours_extracted = duration_series.str.extract(hour_pattern)\n",
        "    days_extracted = duration_series.str.extract(day_pattern)\n",
        "\n",
        "    # Initialize categories and hours\n",
        "    categories = pd.Series([\"No Time Specified\"] * len(duration_series), index=df.index)\n",
        "    extracted_hours = pd.Series([None] * len(duration_series), index=df.index, dtype=float)\n",
        "\n",
        "    # Process hours\n",
        "    # Boolean Masking : for efficient conditional assignment of categories and extracted numerical hours\n",
        "    hour_mask = hours_extracted[0].notna()\n",
        "    hours = hours_extracted[0].astype(float)\n",
        "    extracted_hours[hour_mask] = hours[hour_mask]\n",
        "    categories[hour_mask & (hours < 4)] = \"Less than 4 hours\"\n",
        "    categories[hour_mask & (hours >= 4) & (hours < 12)] = \"More than 4 hours\"\n",
        "    categories[hour_mask & (hours >= 12) & (hours < 24)] = \"More than 12 hours\"\n",
        "    categories[hour_mask & (hours >= 24)] = \"More than 24 hours\"\n",
        "\n",
        "    # Process days (where hours not already set)\n",
        "    day_mask = days_extracted[0].notna() & hours_extracted[0].isna()\n",
        "    days = days_extracted[0].astype(float)\n",
        "    hours_from_days = days * 24\n",
        "    extracted_hours[day_mask] = hours_from_days[day_mask]\n",
        "    categories[day_mask & (hours_from_days < 4)] = \"Less than 4 hours\"\n",
        "    categories[day_mask & (hours_from_days >= 4) & (hours_from_days < 12)] = \"More than 4 hours\"\n",
        "    categories[day_mask & (hours_from_days >= 12) & (hours_from_days < 24)] = \"More than 12 hours\"\n",
        "    categories[day_mask & (hours_from_days >= 24)] = \"More than 24 hours\"\n",
        "\n",
        "    print(f\"     [Preprocessing] Completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return categories, extracted_hours\n",
        "\n",
        "\n",
        "\n",
        "def get_top_keywords(remarks: list[str], n_keywords: int = 10) -> list[str]:\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts top keywords using TF-IDF.\n",
        "    A statistical measure that evaluates how relevant a word is to a document in a collection.\n",
        "    It assigns a higher score to words that appear frequently in a specific document but rarely in the overall corpus\n",
        "    (after removing common words like stopwords).\n",
        "    \"\"\"\n",
        "\n",
        "    if not remarks or len(remarks) < 2:\n",
        "        return []\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            stop_words=list(UNIVERSAL_STOPWORDS), ngram_range=(1, 3), min_df=5, max_features=1000\n",
        "        # ngram_range=(1, 3) : Considers unigrams (single words), bigrams (two-word phrases), and trigrams to capture more contextual meaning\n",
        "        # min_df=5: Ignores terms that appear in fewer than 5 documents, helping to filter out rare or noisy terms\n",
        "        # max_features=1000: Limits the total number of unique keywords considered, reducing dimensionality.\n",
        "        )\n",
        "        tfidf_matrix = vectorizer.fit_transform([r for r in remarks if r][:1000])\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()  # used as a proxy for the overall importance of each term in the collection\n",
        "        top_indices = scores.argsort()[-n_keywords:][::-1]\n",
        "        return [feature_names[i] for i in top_indices]\n",
        "    except ValueError as e:\n",
        "        print(f\"   [Warning] TF-IDF failed: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "def get_genai_cluster_name(cluster_texts: list[str], top_keywords: list[str]) -> str:\n",
        "    \"\"\"Generates a category name using Gemini API.\"\"\"\n",
        "    print(\"    [Gen AI Naming] Sending prompt to Gemini model...\")\n",
        "    if not cluster_texts:\n",
        "        return \"Uncategorized Remarks\"\n",
        "\n",
        "    sample_size = min(20, len(cluster_texts))\n",
        "    text_sample = \"\\n\".join([t[:100] for t in cluster_texts[:sample_size] if t])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert at analyzing customer feedback in the energy sector. Provide a single, concise, professional category name for a group of similar remarks. The name must be 4-7 words, reflect the primary issue accurately, and avoid generic terms like 'Issues', 'Problems', or 'Reports' unless critical. Do not overlap with time-based categories (e.g., 'Less than 4 hours').\n",
        "\n",
        "    Top keywords: {', '.join(top_keywords[:5])}.\n",
        "    Sample remarks:\n",
        "    {text_sample}\n",
        "\n",
        "    Category name:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        name = response.text.strip().split(\"Category name:\")[-1].strip() if \"Category name:\" in response.text else response.text.strip()\n",
        "        if not name or len(name.split()) < 4 or len(name.split()) > 7 or any(t.lower() in name.lower() for t in ['less', 'more', 'hours', 'time']):\n",
        "            name = f\"{top_keywords[0].replace('_', ' ').title()} Incident Category\" if top_keywords else \"Uncategorized Remarks\"\n",
        "        return name[:50].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"    [Gen AI Naming] ERROR: API call failed. Falling back to keywords. Error: {e}\")\n",
        "        return f\"{top_keywords[0].replace('_', ' ').title()} Incident Category\"[:50].strip() if top_keywords else \"Uncategorized Remarks\"\n",
        "\n",
        "\n",
        "\n",
        "def get_unique_name(base_name: str, existing_names: set, suffix_identifier: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Generates a unique name.\n",
        "    It iteratively appends alphabetical (A, B, C...) or alphanumeric (A1, A2...) suffixes to the base_name until a unique name is found that\n",
        "    does not exist in the existing_names set. re.sub is used for initial cleaning of the base name\n",
        "\n",
        "    \"\"\"\n",
        "    name = re.sub(r'[^a-zA-Z\\s]', '', base_name).strip()\n",
        "    name = re.sub(r'\\s+', ' ', name).strip()\n",
        "    if not name:\n",
        "        name = \"Generic Category\"\n",
        "    original_base = name\n",
        "    alpha_suffix_idx = 0\n",
        "    numeric_suffix_idx = 0\n",
        "    while name.lower() in existing_names:\n",
        "        if alpha_suffix_idx < 26:\n",
        "            name = f\"{original_base} {chr(65 + alpha_suffix_idx)}\"\n",
        "            alpha_suffix_idx += 1\n",
        "        else:\n",
        "            numeric_suffix_idx += 1\n",
        "            alpha_suffix_idx_for_num = (alpha_suffix_idx - 26) % 26\n",
        "            name = f\"{original_base} {chr(65 + alpha_suffix_idx_for_num)}{numeric_suffix_idx}\"\n",
        "            alpha_suffix_idx += 1\n",
        "    return name[:50].strip()\n",
        "\n",
        "\n",
        "\n",
        "def is_semantically_similar(name1: str, name2: str) -> bool:\n",
        "    \"\"\"Uses Gemini to check if two column names are semantically similar.\"\"\"\n",
        "    print(f\"   [Gen AI Merging] Checking similarity between '{name1}' and '{name2}'...\")\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert at analyzing customer feedback in the energy sector. Determine if the following two category names are synonyms or convey the same meaning. Answer with a single word: \"YES\" or \"NO\".\n",
        "\n",
        "    Category 1: \"{name1}\"\n",
        "    Category 2: \"{name2}\"\n",
        "\n",
        "    Recommendation:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        recommendation = response.text.strip().split(\"Recommendation:\")[-1].strip() if \"Recommendation:\" in response.text else response.text.strip()\n",
        "        return recommendation.lower() == \"yes\"\n",
        "    except Exception as e:\n",
        "        print(f\"    [Gen AI Merging] ERROR: API call failed. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def merge_similar_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Merges non-time columns with semantically similar names\"\"\"\n",
        "    print(\"\\n--- Merging similar non-time columns (Semantic Match) ---\")\n",
        "    time_columns = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"No Time Specified\"\n",
        "    ]\n",
        "    columns_to_process = [col for col in df.columns if col not in time_columns]\n",
        "    merged_mapping = {}\n",
        "\n",
        "    max_non_time_columns_target = 4\n",
        "\n",
        "    did_merge = True # loop allows for multiple rounds of merging until no more similar pairs are found or the target column count is reached.\n",
        "    while did_merge and len(set(columns_to_process) - set(merged_mapping.keys())) > max_non_time_columns_target:\n",
        "        did_merge = False\n",
        "        current_active_cols = sorted([col for col in columns_to_process if col not in merged_mapping])\n",
        "        # Sorting ensures a consistent order of comparison\n",
        "\n",
        "        for i in range(len(current_active_cols)):\n",
        "            col1 = current_active_cols[i]\n",
        "            if col1 in merged_mapping:                # This means col1 has already been chosen as a source in this iteration\n",
        "                continue\n",
        "\n",
        "            for j in range(i + 1, len(current_active_cols)):\n",
        "                col2 = current_active_cols[j]\n",
        "                if col2 in merged_mapping:            # If col2 has already been chosen as a source in this iteration\n",
        "                    continue\n",
        "\n",
        "                if is_semantically_similar(col1, col2):  # The AI call for similarity assessment.\n",
        "                    current_unmerged_count = len(set(columns_to_process) - set(merged_mapping.keys()))\n",
        "                    if current_unmerged_count > max_non_time_columns_target:\n",
        "                        print(f\"    Merging '{col2}' into '{col1}' (Semantic Match)\")\n",
        "                        merged_mapping[col2] = col1\n",
        "                        did_merge = True\n",
        "                        break                          # Found a merge, break inner loop to re-evaluate current_active_cols\n",
        "            if did_merge:                              # If a merge happened in inner loop, break outer loop to restart while loop\n",
        "                break\n",
        "\n",
        "    temp_df = df.copy()\n",
        "    for source_col, target_col in merged_mapping.items():         # merged_mapping: A dictionary stores source_column_name: target_column_name pairs.\n",
        "        if target_col not in temp_df.columns:\n",
        "            temp_df[target_col] = np.nan\n",
        "        temp_df.loc[:, target_col] = temp_df[target_col].fillna(temp_df[source_col])  # This is a key pandas operation, it takes all non-null\n",
        "                                                                                      # values from source_col and fills corresponding NaN (missing)\n",
        "                                                                                      # spots in target_col. This effectively moves remarks without\n",
        "                                                                                      # overwriting existing data in the target.\n",
        "        temp_df = temp_df.drop(columns=[source_col])               # Removes the source column after its data has been transferred.\n",
        "\n",
        "    final_columns = []\n",
        "    for col in df.columns:\n",
        "        if col in time_columns:\n",
        "            final_columns.append(col)\n",
        "        elif col not in merged_mapping.keys():                         # If it's a non-time column and not a source of a merge\n",
        "            if col not in merged_mapping.values():                     # Ensure it's not a target that was just created\n",
        "                final_columns.append(col)\n",
        "\n",
        "    for target_col in set(merged_mapping.values()):\n",
        "        if target_col not in final_columns:\n",
        "            final_columns.append(target_col)\n",
        "\n",
        "    final_column_order = sorted(final_columns, key=lambda x: (x not in time_columns, x))\n",
        "    final_column_order = [col for col in final_column_order if col in temp_df.columns]\n",
        "\n",
        "    df_merged = temp_df[final_column_order]\n",
        "    print(\"    Merging complete.\")\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "\n",
        "def load_excel_file(file_path: str, column: str) -> tuple[list[str], pd.DataFrame]:\n",
        "    \"\"\"Loads remarks from an Excel file.\"\"\"\n",
        "    print(f\"Loading data from '{file_path}'...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, usecols=[column, \"From When Issue Is Coming\"] if \"From When Issue Is Coming\" in pd.read_excel(file_path, nrows=1).columns else [column])\n",
        "        print(f\"Loaded {len(df)} rows in {time.time() - start_time:.2f} seconds.\")\n",
        "        remarks_list = [str(r) for r in df[column] if not pd.isna(r)]\n",
        "        print(f\"Extracted {len(remarks_list)} valid remarks from column '{column}'.\")\n",
        "        return remarks_list, df\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\" ERROR: File '{file_path}' not found. {e}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\" ERROR: Failed to load Excel file. {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "def save_results(df: pd.DataFrame, output_path: str):\n",
        "    \"\"\"Saves results to an Excel file.\"\"\"\n",
        "    print(f\"\\nSaving results to '{output_path}'...\")\n",
        "    start_time = time.time()\n",
        "    df.to_excel(output_path, index=False)        # saves the DataFrame to an Excel file without writing the pandas internal index as a column.\n",
        "    print(f\"Saved successfully in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "\n",
        "def segregate_remarks_by_language(raw_remarks: list[str], min_text_for_detection: int = 10) -> tuple[list[tuple[int, str]], list[tuple[int, str]]]:\n",
        "    \"\"\"Segregates remarks into English and other languages.\"\"\"\n",
        "    print(f\"Starting language segregation for {len(raw_remarks)} remarks...\")\n",
        "    start_time = time.time()\n",
        "    def detect_lang(i, remark):\n",
        "        cleaned_remark = clean_text(remark.lower())\n",
        "        if len(cleaned_remark) < min_text_for_detection or not any(char.isalpha() for char in cleaned_remark):\n",
        "            return i, remark, False\n",
        "        try:\n",
        "            return i, remark, detect(cleaned_remark) == 'en'\n",
        "        except Exception:\n",
        "            return i, remark, False\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(delayed(detect_lang)(i, r) for i, r in enumerate(raw_remarks))\n",
        "    english_remarks_with_indices = [(i, r) for i, r, is_en in results if is_en]\n",
        "    other_remarks_with_indices = [(i, r) for i, r, is_en in results if not is_en]\n",
        "    print(f\"Segregation complete in {time.time() - start_time:.2f} seconds. English: {len(english_remarks_with_indices)}, Other: {len(other_remarks_with_indices)}\")\n",
        "    return english_remarks_with_indices, other_remarks_with_indices\n",
        "\n",
        "\n",
        "\n",
        "def cluster_remarks(remarks: list[str], n_clusters: int = 10, batch_size: int = 512) -> list[int]:\n",
        "    \"\"\"\n",
        "    Clusters remarks using sentence transformers and cuML KMeans.\n",
        "    Loads a pre-trained Transformer model. This model converts full sentences into high-dimensional numerical vectors (embeddings).\n",
        "    The key idea is that sentences with similar meanings will have embeddings that are numerically \"close\" to each other in this vector space.\n",
        "    use an \"attention mechanism\" to weigh the importance of different words in a sentence relative to each other. This allows them to capture\n",
        "    complex contextual relationships and produce high-quality semantic representations for entire sentences.\n",
        "\n",
        "    \"\"\"\n",
        "    if not remarks:\n",
        "        return []\n",
        "    print(\"     [Clustering] Encoding remarks with sentence-transformers...\")\n",
        "    start_time = time.time()\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "    embeddings = []\n",
        "    for i in range(0, len(remarks), batch_size):\n",
        "        batch = remarks[i:i + batch_size]\n",
        "        batch_embeddings = model.encode(batch, batch_size=batch_size, show_progress_bar=False, convert_to_numpy=True)\n",
        "        embeddings.append(batch_embeddings)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    print(f\"     [Clustering] Encoding completed in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    K-Means is an unsupervised clustering algorithm that aims to partition n observations into k clusters. It works by iteratively assigning each\n",
        "    data point to the closest cluster centroid and then re-calculating the centroids as the mean of the points in the cluster\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"     [Clustering] Performing KMeans clustering with cuML...\")\n",
        "    start_time = time.time()\n",
        "    gdf = cudf.DataFrame(embeddings)\n",
        "    clustering = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    \"\"\"\n",
        "    n_clusters: The desired number of clusters.\n",
        "    random_state: Ensures the centroid initialization is reproducible.\n",
        "    n_init=10: Runs the algorithm 10 times with different centroid initializations and picks the best result\n",
        "    (minimizing inertia/sum of squared distances)\n",
        "    \"\"\"\n",
        "\n",
        "    cluster_labels = clustering.fit_predict(gdf).to_numpy() # Performs the clustering on the GPU (gdf) and returns the cluster assignments for\n",
        "                                                            # each remark as a NumPy array.\n",
        "    print(f\"     [Clustering] Clustering completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return cluster_labels\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    excel_file_path = \"./Supply.xlsx\"\n",
        "    text_column_name = \"REMARKS\"\n",
        "    duration_column_name = \"From When Issue Is Coming\"\n",
        "    output_excel_path = \"./categorized_remarks_01.xlsx\"\n",
        "    max_remark_clusters_limit = 8                    # This limits initial number of clusters for English remarks\n",
        "    batch_size = 500\n",
        "\n",
        "    time_columns = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"No Time Specified\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Remark Categorization Script ---\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        raw_remarks_list, df = load_excel_file(excel_file_path, text_column_name)\n",
        "\n",
        "        print(f\"\\n--- Categorizing remarks ---\")\n",
        "        time_categorized_remarks = {col: [] for col in time_columns}\n",
        "        non_time_remarks_with_indices = []\n",
        "\n",
        "        categories, _ = vectorized_time_categorization(df, text_column_name, duration_column_name)\n",
        "        # Handle cases where `categories` might not be directly iterable if df is empty etc.\n",
        "        # Ensure `zip` handles potential length mismatch safely if remarks are cleaned/filtered.\n",
        "        for i, (remark, category) in enumerate(zip(raw_remarks_list, categories)):\n",
        "            if category != \"No Time Specified\":\n",
        "                time_categorized_remarks[category].append((i, remark))\n",
        "            else:\n",
        "                non_time_remarks_with_indices.append((i, remark))\n",
        "\n",
        "        print(f\"Time-based categorization complete. Counts: { {k: len(v) for k, v in time_categorized_remarks.items()} }\")\n",
        "        print(f\"Non-time remarks for further processing: {len(non_time_remarks_with_indices)}\")\n",
        "\n",
        "        print(\"\\n--- Segregating non-time remarks by language ---\")\n",
        "        english_remarks_with_indices, other_remarks_with_indices = segregate_remarks_by_language(  # to check if non time remarks are english or not\n",
        "            [r for _, r in non_time_remarks_with_indices]\n",
        "        )\n",
        "        # Re-map indices to original dataframe index\n",
        "        english_remarks_with_indices_original = [(non_time_remarks_with_indices[local_idx][0], r) for local_idx, r in english_remarks_with_indices]\n",
        "        other_remarks_with_indices_original = [(non_time_remarks_with_indices[local_idx][0], r) for local_idx, r in other_remarks_with_indices]\n",
        "\n",
        "\n",
        "        print(f\"Non-time English remarks: {len(english_remarks_with_indices_original)}\")\n",
        "        print(f\"Non-time other language remarks: {len(other_remarks_with_indices_original)}\")\n",
        "\n",
        "        # Initialize final_wide_data_columns with time-based categories\n",
        "        final_wide_data_columns = {k: [r for _, r in v] for k, v in time_categorized_remarks.items()}\n",
        "\n",
        "        # original_indexed_cluster_labels is used to map back cluster labels to original df rows\n",
        "        original_indexed_cluster_labels = np.full(len(raw_remarks_list), -2, dtype=int) # -2 for unclustered non-time\n",
        "\n",
        "        # final_column_name_map maps cluster IDs to generated category names\n",
        "        final_column_name_map = {}\n",
        "\n",
        "        if english_remarks_with_indices_original:\n",
        "            print(\"\\n--- Processing non-time English remarks for clustering ---\")\n",
        "            english_remark_texts = [r for _, r in english_remarks_with_indices_original]\n",
        "            english_remark_original_indices = [i for i, _ in english_remarks_with_indices_original]\n",
        "\n",
        "            # Determine number of clusters for KMeans, capping at max_remark_clusters_limit\n",
        "            n_clusters_for_kmeans = min(max_remark_clusters_limit, len(english_remark_texts))\n",
        "            if n_clusters_for_kmeans > 0: # Ensure we don't try to cluster with 0 clusters\n",
        "                cluster_labels = cluster_remarks(english_remark_texts, n_clusters_for_kmeans, batch_size)\n",
        "                print(f\"    [Clustering] Found {len(set(cluster_labels))} initial clusters.\")\n",
        "\n",
        "                # Apply cluster labels back to original remark indices\n",
        "                for i, clustered_label in enumerate(cluster_labels):\n",
        "                    original_indexed_cluster_labels[english_remark_original_indices[i]] = clustered_label\n",
        "\n",
        "                # Get unique cluster IDs for naming\n",
        "                final_unique_clusters = sorted([c for c in set(cluster_labels) if c != -1]) # Exclude noise (-1 if DBSCAN was used)\n",
        "                print(f\"    [Gen AI Naming] Naming {len(final_unique_clusters)} final clusters.\")\n",
        "\n",
        "                used_final_names = set(time_columns) # Keep track of names already in use (including time categories)\n",
        "                for cluster_id in final_unique_clusters:\n",
        "                    cluster_texts_original = [english_remark_texts[j] for j, label in enumerate(cluster_labels) if label == cluster_id]\n",
        "                    top_keywords = get_top_keywords(cluster_texts_original)\n",
        "                    print(f\"    [Keywords] Top keywords for cluster {cluster_id}: {', '.join(top_keywords)}\")\n",
        "\n",
        "                    proposed_final_name = get_genai_cluster_name(cluster_texts_original, top_keywords)\n",
        "                    final_name = get_unique_name(proposed_final_name, used_final_names, str(cluster_id))\n",
        "                    final_column_name_map[cluster_id] = final_name\n",
        "                    used_final_names.add(final_name.lower())\n",
        "                    print(f\"    Final Category Name: '{final_name}'\")\n",
        "                    final_wide_data_columns[final_name] = cluster_texts_original\n",
        "            else:\n",
        "                print(\"    [Clustering] Not enough English remarks for clustering.\")\n",
        "\n",
        "        # Handle unclustered English remarks (if any)\n",
        "        uncategorized_english_remarks = [(original_idx, r) for original_idx, r in english_remarks_with_indices_original if original_indexed_cluster_labels[original_idx] == -1] # Assuming -1 for noise/unclustered\n",
        "        if uncategorized_remarks := [r for _, r in uncategorized_english_remarks]:\n",
        "            col_name = get_unique_name(\"Uncategorized English Remarks\", set(final_wide_data_columns.keys()).union(set(final_column_name_map.values())), \"uncat_en\")\n",
        "            final_wide_data_columns[col_name] = uncategorized_remarks\n",
        "            print(f\"\\nAdded column: '{col_name}' for {len(uncategorized_remarks)} remarks.\")\n",
        "\n",
        "        # Handle other language remarks\n",
        "        if other_remarks := [r for _, r in other_remarks_with_indices_original]:\n",
        "            col_name = get_unique_name(\"Other Language Remarks\", set(final_wide_data_columns.keys()).union(set(final_column_name_map.values())), \"other_lang\")\n",
        "            final_wide_data_columns[col_name] = other_remarks\n",
        "            print(f\"Added column: '{col_name}' for {len(other_remarks)} remarks.\")\n",
        "\n",
        "        # Create wide format DataFrame\n",
        "        max_rows = max(len(remarks) for remarks in final_wide_data_columns.values()) if final_wide_data_columns else 0\n",
        "        df_results_wide = pd.DataFrame({\n",
        "            col: remarks + [\"\"] * (max_rows - len(remarks))\n",
        "            for col, remarks in final_wide_data_columns.items()\n",
        "        })\n",
        "\n",
        "        print(\"\\n--- Merging non-time columns ---\")\n",
        "        non_time_columns_pre_merge = [col for col in df_results_wide.columns if col not in time_columns]\n",
        "        if len(non_time_columns_pre_merge) > 4: # Only attempt merge if there are more than 4 non-time columns\n",
        "            df_results_wide = merge_similar_columns(df_results_wide)\n",
        "        else:\n",
        "            print(f\"Skipping semantic merging. Number of non-time columns ({len(non_time_columns_pre_merge)}) is already at or below the target of 4.\")\n",
        "\n",
        "\n",
        "        print(f\"\\nCategorization complete. Column counts: { {k: len([x for x in df_results_wide[k] if x]) for k in df_results_wide.columns} }\")\n",
        "        print(\"\\n--- Validating Categories ---\")\n",
        "        for col in df_results_wide.columns:\n",
        "            print(f\"Category '{col}' ({len([x for x in df_results_wide[col] if x])} remarks):\")\n",
        "            for r in df_results_wide[col][:min(5, len(df_results_wide[col]))]:\n",
        "                if r:\n",
        "                    print(f\"   - {r[:100]}...\")\n",
        "\n",
        "        save_results(df_results_wide, output_excel_path)\n",
        "        print(\"\\n--- Sample Results ---\")\n",
        "        print(df_results_wide.head())\n",
        "        print(f\"\\n--- Script completed in {time.time() - start_time:.2f} seconds ---\")\n",
        "\n",
        "    except FileNotFoundError as fnfe:\n",
        "        print(f\"\\nERROR: File not found. Please check 'excel_file_path'. Details: {fnfe}\")\n",
        "        exit(1)\n",
        "    except KeyError as ke:\n",
        "        print(f\"\\nERROR: Column not found. Please check 'text_column_name' or 'duration_column_name'. Details: {ke}\")\n",
        "        exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNSUPERVISED MODEL WITHOUT TIME CATEGORIES**"
      ],
      "metadata": {
        "id": "GQmZjtrrD8ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "from langdetect import detect, DetectorFactory\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import cudf\n",
        "from cuml.cluster import KMeans\n",
        "import google.generativeai as genai\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import torch\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyB3877RwLYTNB9Mhi7HhH8CBdG8ua8QtsM\"\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "except KeyError:\n",
        "    print(\" ERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "    exit()\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'area', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it', 'its', 'her', 'their', 'our',\n",
        "    'what', 'where', 'how', 'why', 'who', 'whom', 'which', 'whether',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hr', 'hrs', 'hour', 'hours', 'time', 'date', 'week', 'month', 'year', 'ago',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'code', 'id', 'location', 'address', 'phone', 'mobile', 'call', 'report', 'registered',\n",
        "    'ok', 'yes', 'no', 'not', 'hi', 'hello', 'sir', 'madam', 'pls', 'please', 'regards', 'type', 'urban', 'complaint', 'detail', 'general',\n",
        "    'kv', 'tf', 'na', 'service', 'request', 'feedback', 'query', 'regarding', 'about', 'given', 'areas', 'village'\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', str(text).strip()).lower()\n",
        "    return text\n",
        "\n",
        "def get_top_keywords(remarks: list[str], n_keywords: int = 10) -> list[str]:\n",
        "\n",
        "    if not remarks or len(remarks) < 2:\n",
        "        return []\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            stop_words=list(UNIVERSAL_STOPWORDS), ngram_range=(1, 3), min_df=5, max_features=1000\n",
        "        )\n",
        "        tfidf_matrix = vectorizer.fit_transform([r for r in remarks if r][:1000])\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n",
        "        top_indices = scores.argsort()[-n_keywords:][::-1]\n",
        "        return [feature_names[i] for i in top_indices]\n",
        "    except ValueError as e:\n",
        "        print(f\"   [Warning] TF-IDF failed: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "def get_genai_cluster_name(cluster_texts: list[str], top_keywords: list[str]) -> str:\n",
        "    print(\"      [Gen AI Naming] Sending prompt to Gemini model...\")\n",
        "\n",
        "    sample_size = min(20, len(cluster_texts))\n",
        "    text_sample = \"\\n\".join([t[:100] for t in cluster_texts[:sample_size] if t])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert at analyzing customer feedback in the energy sector. Provide a single, concise, professional category name for a group of similar remarks. The name must be 4-7 words, reflect the primary issue accurately, and avoid generic terms like 'Issues', 'Problems', or 'Reports' unless critical. Do not overlap with time-based categories (e.g., 'Less than 4 hours').\n",
        "\n",
        "    Top keywords: {', '.join(top_keywords[:5])}.\n",
        "    Sample remarks:\n",
        "    {text_sample}\n",
        "\n",
        "    Category name:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        name = response.text.strip().split(\"Category name:\")[-1].strip() if \"Category name:\" in response.text else response.text.strip()\n",
        "        if not name or len(name.split()) < 4 or len(name.split()) > 7 or any(t.lower() in name.lower() for t in ['less', 'more', 'hours', 'time']):\n",
        "            name = f\"{top_keywords[0].replace('_', ' ').title()} Incident Category\" if top_keywords else \"Uncategorized Remarks\"\n",
        "        return name[:50].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"      [Gen AI Naming] ERROR: API call failed. Falling back to keywords. Error: {e}\")\n",
        "        return f\"{top_keywords[0].replace('_', ' ').title()} Incident Category\"[:50].strip() if top_keywords else \"Uncategorized Remarks\"\n",
        "\n",
        "\n",
        "\n",
        "def get_unique_name(base_name: str, existing_names: set, suffix_identifier: str = \"\") -> str:\n",
        "    name = re.sub(r'[^a-zA-Z\\s]', '', base_name).strip()\n",
        "    name = re.sub(r'\\s+', ' ', name).strip()\n",
        "    if not name:\n",
        "        name = \"Generic Category\"\n",
        "    original_base = name\n",
        "    alpha_suffix_idx = 0\n",
        "    numeric_suffix_idx = 0\n",
        "    while name.lower() in existing_names:\n",
        "        if alpha_suffix_idx < 26:\n",
        "            name = f\"{original_base} {chr(65 + alpha_suffix_idx)}\"\n",
        "            alpha_suffix_idx += 1\n",
        "        else:\n",
        "            numeric_suffix_idx += 1\n",
        "            alpha_suffix_idx_for_num = (alpha_suffix_idx - 26) % 26\n",
        "            name = f\"{original_base} {chr(65 + alpha_suffix_idx_for_num)}{numeric_suffix_idx}\"\n",
        "            alpha_suffix_idx += 1\n",
        "    return name[:50].strip()\n",
        "\n",
        "\n",
        "\n",
        "def is_semantically_similar(name1: str, name2: str) -> bool:\n",
        "    print(f\"    [Gen AI Merging] Checking similarity between '{name1}' and '{name2}'...\")\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert at analyzing customer feedback in the energy sector. Determine if the following two category names are synonyms or convey the same meaning. Answer with a single word: \"YES\" or \"NO\".\n",
        "\n",
        "    Category 1: \"{name1}\"\n",
        "    Category 2: \"{name2}\"\n",
        "\n",
        "    Recommendation:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        recommendation = response.text.strip().split(\"Recommendation:\")[-1].strip() if \"Recommendation:\" in response.text else response.text.strip()\n",
        "        return recommendation.lower() == \"yes\"\n",
        "    except Exception as e:\n",
        "        print(f\"      [Gen AI Merging] ERROR: API call failed. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def merge_similar_columns(df: pd.DataFrame, max_target_columns: int) -> pd.DataFrame:\n",
        "    print(\"\\n--- Merging similar columns (Semantic Match) ---\")\n",
        "    columns_to_process = list(df.columns)\n",
        "    merged_mapping = {}\n",
        "\n",
        "    did_merge = True\n",
        "    while did_merge and len(set(columns_to_process) - set(merged_mapping.keys())) > max_target_columns:\n",
        "        did_merge = False\n",
        "        current_active_cols = sorted([col for col in columns_to_process if col not in merged_mapping])\n",
        "\n",
        "        for i in range(len(current_active_cols)):\n",
        "            col1 = current_active_cols[i]\n",
        "            if col1 in merged_mapping:\n",
        "                continue\n",
        "\n",
        "            for j in range(i + 1, len(current_active_cols)):\n",
        "                col2 = current_active_cols[j]\n",
        "                if col2 in merged_mapping:\n",
        "                    continue\n",
        "\n",
        "                if is_semantically_similar(col1, col2):\n",
        "                    current_unmerged_count = len(set(columns_to_process) - set(merged_mapping.keys()))\n",
        "                    if current_unmerged_count > max_target_columns:\n",
        "                        print(f\"      Merging '{col2}' into '{col1}' (Semantic Match)\")\n",
        "                        merged_mapping[col2] = col1\n",
        "                        did_merge = True\n",
        "                        break\n",
        "            if did_merge:\n",
        "                break\n",
        "\n",
        "    temp_df = df.copy()\n",
        "    for source_col, target_col in merged_mapping.items():\n",
        "        if target_col not in temp_df.columns:\n",
        "            temp_df[target_col] = np.nan\n",
        "        temp_df.loc[:, target_col] = temp_df[target_col].fillna(temp_df[source_col])\n",
        "        temp_df = temp_df.drop(columns=[source_col])\n",
        "\n",
        "    final_columns = []\n",
        "    for col in df.columns:\n",
        "        if col not in merged_mapping.keys():\n",
        "            if col not in merged_mapping.values():\n",
        "                final_columns.append(col)\n",
        "\n",
        "    for target_col in set(merged_mapping.values()):\n",
        "        if target_col not in final_columns:\n",
        "            final_columns.append(target_col)\n",
        "\n",
        "    final_column_order = sorted(final_columns)\n",
        "    final_column_order = [col for col in final_column_order if col in temp_df.columns]\n",
        "\n",
        "    df_merged = temp_df[final_column_order]\n",
        "    print(\"      Merging complete.\")\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "\n",
        "def load_excel_file(file_path: str, column: str) -> tuple[list[str], pd.DataFrame]:\n",
        "    print(f\"Loading data from '{file_path}'...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, usecols=[column])\n",
        "        print(f\"Loaded {len(df)} rows in {time.time() - start_time:.2f} seconds.\")\n",
        "        remarks_list = [str(r) for r in df[column] if not pd.isna(r)]\n",
        "        print(f\"Extracted {len(remarks_list)} valid remarks from column '{column}'.\")\n",
        "        return remarks_list, df\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\" ERROR: File '{file_path}' not found. {e}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\" ERROR: Failed to load Excel file. {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "def save_results(df: pd.DataFrame, output_path: str):\n",
        "    print(f\"\\nSaving results to '{output_path}'...\")\n",
        "    start_time = time.time()\n",
        "    df.to_excel(output_path, index=False)\n",
        "    print(f\"Saved successfully in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "\n",
        "def segregate_remarks_by_language(raw_remarks: list[str], min_text_for_detection: int = 10) -> tuple[list[tuple[int, str]], list[tuple[int, str]]]:\n",
        "    print(f\"Starting language segregation for {len(raw_remarks)} remarks...\")\n",
        "    start_time = time.time()\n",
        "    def detect_lang(i, remark):\n",
        "        cleaned_remark = clean_text(remark.lower())\n",
        "        if len(cleaned_remark) < min_text_for_detection or not any(char.isalpha() for char in cleaned_remark):\n",
        "            return i, remark, False\n",
        "        try:\n",
        "            return i, remark, detect(cleaned_remark) == 'en'\n",
        "        except Exception:\n",
        "            return i, remark, False\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(delayed(detect_lang)(i, r) for i, r in enumerate(raw_remarks))\n",
        "    english_remarks_with_indices = [(i, r) for i, r, is_en in results if is_en]\n",
        "    other_remarks_with_indices = [(i, r) for i, r, is_en in results if not is_en]\n",
        "    print(f\"Segregation complete in {time.time() - start_time:.2f} seconds. English: {len(english_remarks_with_indices)}, Other: {len(other_remarks_with_indices)}\")\n",
        "    return english_remarks_with_indices, other_remarks_with_indices\n",
        "\n",
        "\n",
        "\n",
        "def cluster_remarks(remarks: list[str], n_clusters: int = 10, batch_size: int = 512) -> list[int]:\n",
        "    if not remarks:\n",
        "        return []\n",
        "    print(\"      [Clustering] Encoding remarks with sentence-transformers...\")\n",
        "    start_time = time.time()\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "    embeddings = []\n",
        "    for i in range(0, len(remarks), batch_size):\n",
        "        batch = remarks[i:i + batch_size]\n",
        "        batch_embeddings = model.encode(batch, batch_size=batch_size, show_progress_bar=False, convert_to_numpy=True)\n",
        "        embeddings.append(batch_embeddings)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    print(f\"      [Clustering] Encoding completed in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "    print(\"      [Clustering] Performing KMeans clustering with cuML...\")\n",
        "    start_time = time.time()\n",
        "    gdf = cudf.DataFrame(embeddings)\n",
        "    clustering = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "\n",
        "    cluster_labels = clustering.fit_predict(gdf).to_numpy()\n",
        "    print(f\"      [Clustering] Clustering completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return cluster_labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    excel_file_path = \"./Bill.xlsx\"\n",
        "    text_column_name = \"REMARKS\"\n",
        "    output_excel_path = \"./unsup_non_time.xlsx\"\n",
        "    max_remark_clusters_target = 10\n",
        "    batch_size = 500\n",
        "\n",
        "    print(\"\\n--- Starting Remark Categorization Script (No Time Categories) ---\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        raw_remarks_list, df = load_excel_file(excel_file_path, text_column_name)\n",
        "\n",
        "        print(\"\\n--- Segregating remarks by language ---\")\n",
        "        english_remarks_with_indices, other_remarks_with_indices = segregate_remarks_by_language(\n",
        "            raw_remarks_list\n",
        "        )\n",
        "\n",
        "        print(f\"English remarks for clustering: {len(english_remarks_with_indices)}\")\n",
        "        print(f\"Other language remarks: {len(other_remarks_with_indices)}\")\n",
        "\n",
        "        final_wide_data_columns = {}\n",
        "\n",
        "        original_indexed_cluster_labels = np.full(len(raw_remarks_list), -2, dtype=int)\n",
        "\n",
        "        final_column_name_map = {}\n",
        "\n",
        "        if english_remarks_with_indices:\n",
        "            print(\"\\n--- Processing English remarks for clustering ---\")\n",
        "            english_remark_texts = [r for _, r in english_remarks_with_indices]\n",
        "            english_remark_original_indices = [i for i, _ in english_remarks_with_indices]\n",
        "\n",
        "            n_clusters_initial = min(len(english_remark_texts), 20)\n",
        "\n",
        "            if n_clusters_initial > 0:\n",
        "                cluster_labels = cluster_remarks(english_remark_texts, n_clusters_initial, batch_size)\n",
        "                print(f\"      [Clustering] Found {len(set(cluster_labels))} initial clusters.\")\n",
        "\n",
        "                for i, clustered_label in enumerate(cluster_labels):\n",
        "                    original_indexed_cluster_labels[english_remark_original_indices[i]] = clustered_label\n",
        "\n",
        "                initial_unique_clusters = sorted([c for c in set(cluster_labels) if c != -1])\n",
        "                print(f\"      [Gen AI Naming] Naming {len(initial_unique_clusters)} initial clusters.\")\n",
        "\n",
        "                temp_cluster_data = defaultdict(list)\n",
        "                for i, r in english_remarks_with_indices:\n",
        "                    cluster_id = original_indexed_cluster_labels[i]\n",
        "                    if cluster_id != -1:\n",
        "                        temp_cluster_data[cluster_id].append(r)\n",
        "\n",
        "                used_initial_names = set()\n",
        "                initial_named_clusters = {}\n",
        "                for cluster_id in initial_unique_clusters:\n",
        "                    cluster_texts_original = temp_cluster_data[cluster_id]\n",
        "                    if cluster_texts_original:\n",
        "                        top_keywords = get_top_keywords(cluster_texts_original)\n",
        "                        print(f\"      [Keywords] Top keywords for cluster {cluster_id}: {', '.join(top_keywords)}\")\n",
        "\n",
        "                        proposed_name = get_genai_cluster_name(cluster_texts_original, top_keywords)\n",
        "                        unique_initial_name = get_unique_name(proposed_name, used_initial_names)\n",
        "                        initial_named_clusters[unique_initial_name] = cluster_texts_original\n",
        "                        used_initial_names.add(unique_initial_name.lower())\n",
        "                        print(f\"      Initial Category Name for cluster {cluster_id}: '{unique_initial_name}'\")\n",
        "            else:\n",
        "                print(\"      [Clustering] Not enough English remarks for clustering.\")\n",
        "                initial_named_clusters = {}\n",
        "        else:\n",
        "            initial_named_clusters = {}\n",
        "\n",
        "        if initial_named_clusters:\n",
        "            max_len = max(len(v) for v in initial_named_clusters.values())\n",
        "            df_for_merging = pd.DataFrame({\n",
        "                name: data + [''] * (max_len - len(data))\n",
        "                for name, data in initial_named_clusters.items()\n",
        "            })\n",
        "        else:\n",
        "            df_for_merging = pd.DataFrame()\n",
        "\n",
        "\n",
        "        if not df_for_merging.empty and len(df_for_merging.columns) > max_remark_clusters_target:\n",
        "            df_merged_clusters = merge_similar_columns(df_for_merging, max_remark_clusters_target)\n",
        "            final_wide_data_columns.update({col: list(df_merged_clusters[col].dropna()) for col in df_merged_clusters.columns})\n",
        "        elif not df_for_merging.empty:\n",
        "            print(f\"Skipping semantic merging. Number of clusters ({len(df_for_merging.columns)}) is already at or below the target of {max_remark_clusters_target}.\")\n",
        "            final_wide_data_columns.update({col: list(df_for_merging[col].dropna()) for col in df_for_merging.columns})\n",
        "        else:\n",
        "            print(\"No English remarks to form initial clusters.\")\n",
        "\n",
        "\n",
        "        all_categorized_english_remarks_texts = set()\n",
        "        for col in final_wide_data_columns:\n",
        "            all_categorized_english_remarks_texts.update(final_wide_data_columns[col])\n",
        "\n",
        "        uncategorized_english_remarks = [(original_idx, r) for original_idx, r in english_remarks_with_indices if r not in all_categorized_english_remarks_texts]\n",
        "\n",
        "        if uncategorized_remarks_texts := [r for _, r in uncategorized_english_remarks]:\n",
        "            col_name = get_unique_name(\"Uncategorized English Remarks\", set(final_wide_data_columns.keys()), \"uncat_en\")\n",
        "            final_wide_data_columns[col_name] = uncategorized_remarks_texts\n",
        "            print(f\"\\nAdded column: '{col_name}' for {len(uncategorized_remarks_texts)} remarks.\")\n",
        "\n",
        "\n",
        "        if other_remarks := [r for _, r in other_remarks_with_indices]:\n",
        "            col_name = get_unique_name(\"Other Language Remarks\", set(final_wide_data_columns.keys()), \"other_lang\")\n",
        "            final_wide_data_columns[col_name] = other_remarks\n",
        "            print(f\"Added column: '{col_name}' for {len(other_remarks)} remarks.\")\n",
        "\n",
        "\n",
        "        max_rows = max(len(remarks) for remarks in final_wide_data_columns.values()) if final_wide_data_columns else 0\n",
        "        df_results_wide = pd.DataFrame({\n",
        "            col: remarks + [\"\"] * (max_rows - len(remarks))\n",
        "            for col, remarks in final_wide_data_columns.items()\n",
        "        })\n",
        "\n",
        "\n",
        "        print(f\"\\nCategorization complete. Column counts: { {k: len([x for x in df_results_wide[k] if x]) for k in df_results_wide.columns} }\")\n",
        "        print(\"\\n--- Validating Categories ---\")\n",
        "        for col in df_results_wide.columns:\n",
        "            print(f\"Category '{col}' ({len([x for x in df_results_wide[col] if x])} remarks):\")\n",
        "            for r in df_results_wide[col][:min(5, len(df_results_wide[col]))]:\n",
        "                if r:\n",
        "                    print(f\"    - {r[:100]}...\")\n",
        "\n",
        "        save_results(df_results_wide, output_excel_path)\n",
        "        print(\"\\n--- Sample Results ---\")\n",
        "        print(df_results_wide.head())\n",
        "        print(f\"\\n--- Script completed in {time.time() - start_time:.2f} seconds ---\")\n",
        "\n",
        "    except FileNotFoundError as fnfe:\n",
        "        print(f\"\\nERROR: File not found. Please check 'excel_file_path'. Details: {fnfe}\")\n",
        "        exit(1)\n",
        "    except KeyError as ke:\n",
        "        print(f\"\\nERROR: Column not found. Please check 'text_column_name'. Details: {ke}\")\n",
        "        exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qRB7_gh7ix9S",
        "outputId": "bc0df5a1-c94b-4064-c246-c7ad29e55c67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Remark Categorization Script (No Time Categories) ---\n",
            "Loading data from './Bill.xlsx'...\n",
            "Loaded 60729 rows in 1.90 seconds.\n",
            "Extracted 54072 valid remarks from column 'REMARKS'.\n",
            "\n",
            "--- Segregating remarks by language ---\n",
            "Starting language segregation for 54072 remarks...\n",
            "Segregation complete in 40.36 seconds. English: 25390, Other: 28682\n",
            "English remarks for clustering: 25390\n",
            "Other language remarks: 28682\n",
            "\n",
            "--- Processing English remarks for clustering ---\n",
            "      [Clustering] Encoding remarks with sentence-transformers...\n",
            "      [Clustering] Encoding completed in 15.78 seconds.\n",
            "      [Clustering] Performing KMeans clustering with cuML...\n",
            "      [Clustering] Clustering completed in 0.55 seconds.\n",
            "      [Clustering] Found 20 initial clusters.\n",
            "      [Gen AI Naming] Naming 20 initial clusters.\n",
            "      [Keywords] Top keywords for cluster 0: reading, bill, wrong, bill reading, done, reading done, wrong reading, wrong bill, without, generated\n",
            "      [Gen AI Naming] Sending prompt to Gemini model...\n",
            "      [Gen AI Naming] ERROR: API call failed. Falling back to keywords. Error: HTTPConnectionPool(host='localhost', port=33981): Read timed out. (read timeout=600.0)\n",
            "      Initial Category Name for cluster 0: 'Reading Incident Category'\n",
            "      [Keywords] Top keywords for cluster 1: bill, amount, bill amount, high, wrong, wrong bill, bill high, amount high, correct, much\n",
            "      [Gen AI Naming] Sending prompt to Gemini model...\n",
            "      [Gen AI Naming] ERROR: API call failed. Falling back to keywords. Error: HTTPConnectionPool(host='localhost', port=33981): Read timed out. (read timeout=600.0)\n",
            "      Initial Category Name for cluster 1: 'Bill Incident Category'\n",
            "      [Keywords] Top keywords for cluster 2: bill connection domestic, bill connection, wrong bill connection, problem wrong, problem wrong bill, bill, connection domestic issue, domestic issue coming, domestic issue, wrong\n",
            "      [Gen AI Naming] Sending prompt to Gemini model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-683301352.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-683301352.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    322\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"      [Keywords] Top keywords for cluster {cluster_id}: {', '.join(top_keywords)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                         \u001b[0mproposed_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_genai_cluster_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_texts_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                         \u001b[0munique_initial_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_unique_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposed_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_initial_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                         \u001b[0minitial_named_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique_initial_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_texts_original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-683301352.py\u001b[0m in \u001b[0;36mget_genai_cluster_name\u001b[0;34m(cluster_texts, top_keywords)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemini-1.5-flash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Category name:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"Category name:\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'less'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'more'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hours'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             response = GenerativeServiceRestTransport._GenerateContent._get_response(\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(host, metadata, query_params, session, timeout, transcoded_request, body)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             response = getattr(session, method)(\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0;34m\"{host}{uri}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    538\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Model of Supply"
      ],
      "metadata": {
        "id": "Nc5G-x8XEIL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Download NLTK stopwords\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Define stopwords (from your original code, ensure consistency)\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'area', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it', 'its', 'her', 'their', 'our',\n",
        "    'what', 'where', 'how', 'why', 'who', 'whom', 'which', 'whether',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hr', 'hrs', 'hour', 'hours', 'time', 'date', 'week', 'month', 'year', 'ago',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'code', 'id', 'location', 'address', 'phone', 'mobile', 'call', 'report', 'registered',\n",
        "    'ok', 'yes', 'no', 'not', 'hi', 'hello', 'sir', 'madam', 'pls', 'please', 'regards', 'type', 'urban', 'complaint', 'detail', 'general',\n",
        "    'kv', 'tf', 'na', 'service', 'request', 'feedback', 'query', 'regarding', 'about', 'given', 'areas', 'village'\n",
        "])\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Removes extra whitespace and standardizes text (generic version).\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text) # Ensure text is a string\n",
        "\n",
        "    # Remove any stray non-alphanumeric characters that might remain after emoji removal,\n",
        "    # then handle whitespace and lowercase.\n",
        "    # Keep alphanumeric characters and basic punctuation that might be relevant to remarks\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"-/]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text.strip()).lower()\n",
        "    return text\n",
        "\n",
        "def extract_time_features(remarks_series: pd.Series) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts numerical time features (hours, presence of AM/PM) from a Series of remarks.\n",
        "    Returns a DataFrame with these features.\n",
        "    \"\"\"\n",
        "    remarks_series = remarks_series.fillna(\"\").str.lower()\n",
        "\n",
        "    hour_pattern = r'(\\d+\\.?\\d*)\\s*(?:hr|hrs|hour|hours|h)'\n",
        "    day_pattern = r'(\\d+\\.?\\d*)\\s*(?:day|days)'\n",
        "    am_pm_pattern = r'\\b(\\d{1,2}(:\\d{2})?)\\s*(am|pm)\\b'\n",
        "\n",
        "    hours_extracted = remarks_series.str.extract(hour_pattern)[0].astype(float).fillna(0)\n",
        "    days_extracted = remarks_series.str.extract(day_pattern)[0].astype(float).fillna(0)\n",
        "\n",
        "    hours_from_days = days_extracted * 24\n",
        "    combined_hours = hours_extracted + hours_from_days\n",
        "\n",
        "    is_am_pm_mentioned = remarks_series.str.contains(am_pm_pattern, regex=True).astype(int)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'extracted_hours': combined_hours,\n",
        "        'is_am_pm_mentioned': is_am_pm_mentioned\n",
        "    })\n",
        "\n",
        "def main_classification_pipeline():\n",
        "    # --- Configuration ---\n",
        "    labeled_data_excel_path = \"./Book1.xlsx\"\n",
        "    new_remarks_excel_path = \"./Supply.xlsx\"\n",
        "    output_excel_path = \"./categorized_remarks_ML_model.xlsx\" # Output filename\n",
        "\n",
        "    # for supply\n",
        "    desired_category_labels = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"More than 4 hours\",\n",
        "        \"Consumer Power Supply Failures\",\n",
        "        \"Failed Pole Incident Category\",\n",
        "        \"Other Language Remarks\",\n",
        "        \"Partial Phase Supply Failure\",\n",
        "        \"Transformer Damage Causing Outages\"\n",
        "    ]\n",
        "\n",
        "    \"\"\"\n",
        "    desired_category_labels = [\n",
        "        \"Bill Accuracy and Discrepancies\",\n",
        "        \"Bill Content and Delivery Discrepancies\",\n",
        "        \"Bill Hold Preventing Online Payments\",\n",
        "        \"Billing Discrepancies Due to Meter Readings\",\n",
        "        \"Customer Billing Not Received or Available\",\n",
        "        \"Customer Reported Billing Inaccuracie\",\n",
        "        \"Other Language Remarks\",\n",
        "        \"Domestic Connection Billing Discrepancies\",\n",
        "        \"Domestic Meter Reading Collection Failure\"\n",
        "        \"Electricity Bill Discrepancies and Solar Units\"\n",
        "        \"Rectifying Account and Billing Discrepancies\"\n",
        "        \"Unacknowledged Customer Payments and Billing\"\n",
        "    ]\n",
        "    \"\"\"\n",
        "\n",
        "    # Column containing remarks in the NEW_REMARKS_EXCEL_PATH file (e.g., Supply.xlsx)\n",
        "    remarks_column_in_new_file = \"REMARKS\"\n",
        "\n",
        "    print(\"--- Starting Supervised ML Remark Categorization Pipeline (Time-Aware) ---\")\n",
        "    start_full_pipeline_time = time.time()\n",
        "\n",
        "    # --- Step 1: Data Preparation for Training ---\n",
        "    print(\"\\n--- Step 1: Data Preparation for Training ---\")\n",
        "    try:\n",
        "        df_raw_labeled = pd.read_excel(labeled_data_excel_path, header=0)\n",
        "        print(f\"Successfully loaded '{labeled_data_excel_path}'. Shape: {df_raw_labeled.shape}\")\n",
        "        print(\"First 5 rows of the raw labeled data:\")\n",
        "        print(df_raw_labeled.head())\n",
        "        print(\"Columns identified from the raw labeled data:\")\n",
        "        print(df_raw_labeled.columns.tolist())\n",
        "\n",
        "        loaded_to_label_map = {}\n",
        "        seen_labels = {}\n",
        "\n",
        "        for col in df_raw_labeled.columns:\n",
        "            normalized_col = col.split('.')[0]\n",
        "\n",
        "            if normalized_col in desired_category_labels:\n",
        "                if normalized_col in seen_labels:\n",
        "                    loaded_to_label_map[col] = normalized_col\n",
        "                else:\n",
        "                    loaded_to_label_map[col] = col\n",
        "                    seen_labels[normalized_col] = True\n",
        "            else:\n",
        "                print(f\"Warning: Column '{col}' from '{labeled_data_excel_path}' is not in the list of desired categories. It will be ignored for training data. If this is a category, please add it to 'desired_category_labels'.\")\n",
        "\n",
        "        active_category_columns_in_df = list(loaded_to_label_map.keys())\n",
        "\n",
        "        print(f\"\\nIdentified {len(set(loaded_to_label_map.values()))} Unique Category Labels for Training:\")\n",
        "        print(list(set(loaded_to_label_map.values())))\n",
        "        print(f\"Columns from '{labeled_data_excel_path}' that will be processed:\")\n",
        "        print(active_category_columns_in_df)\n",
        "\n",
        "        labeled_data_for_training = []\n",
        "        for category_col_in_df in active_category_columns_in_df:\n",
        "            standard_category_label = loaded_to_label_map[category_col_in_df]\n",
        "            for remark_entry in df_raw_labeled[category_col_in_df].dropna():\n",
        "                cleaned_remark = clean_text(str(remark_entry))\n",
        "                if cleaned_remark:\n",
        "                    labeled_data_for_training.append({\n",
        "                        'remark_original': str(remark_entry),\n",
        "                        'remark_cleaned': cleaned_remark,\n",
        "                        'category': standard_category_label\n",
        "                    })\n",
        "\n",
        "        df_labeled_for_training = pd.DataFrame(labeled_data_for_training)\n",
        "        print(f\"\\nTransformed data for training shape: {df_labeled_for_training.shape}\")\n",
        "        print(\"First 5 rows of the transformed labeled data for training:\")\n",
        "        print(df_labeled_for_training.head())\n",
        "        print(\"Value counts for 'category' (training labels):\")\n",
        "        print(df_labeled_for_training['category'].value_counts())\n",
        "        print(f\"Total unique categories identified in training data: {df_labeled_for_training['category'].nunique()}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: The file '{labeled_data_excel_path}' not found. Please ensure it's in the same directory.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 1: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- Step 2: Feature Extraction (Sentence Embeddings + Time Features) and Model Training ---\n",
        "    print(\"\\n--- Step 2: Feature Extraction (Sentence Embeddings + Time Features) and Model Training ---\")\n",
        "    try:\n",
        "        X_train_data = df_labeled_for_training['remark_cleaned'].tolist()\n",
        "        y_train_labels = df_labeled_for_training['category'].tolist()\n",
        "\n",
        "        X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "            pd.Series(X_train_data), y_train_labels, test_size=0.2, random_state=42, stratify=y_train_labels\n",
        "        )\n",
        "\n",
        "        # Extract and scale time features for training\n",
        "        scaler = StandardScaler() # Initialize scaler here\n",
        "        X_train_time_features_df = extract_time_features(X_train_text)\n",
        "        X_train_time_features_scaled = scaler.fit_transform(X_train_time_features_df)\n",
        "\n",
        "        X_test_time_features_df = extract_time_features(X_test_text)\n",
        "        X_test_time_features_scaled = scaler.transform(X_test_time_features_df) # Use fitted scaler\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Loading SentenceTransformer model on device: {device}...\")\n",
        "        model_embedding = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "        print(\"Generating embeddings for training text...\")\n",
        "        X_train_embeddings = model_embedding.encode(X_train_text.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
        "        print(\"Generating embeddings for testing text...\")\n",
        "        X_test_embeddings = model_embedding.encode(X_test_text.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "        X_train_combined_features = np.hstack((X_train_embeddings, X_train_time_features_scaled))\n",
        "        X_test_combined_features = np.hstack((X_test_embeddings, X_test_time_features_scaled))\n",
        "\n",
        "        print(\"\\nTraining Logistic Regression model on combined features...\")\n",
        "        classifier_model = LogisticRegression(\n",
        "            max_iter=1000, solver='lbfgs', multi_class='auto', class_weight='balanced', random_state=42, n_jobs=-1\n",
        "        )\n",
        "        classifier_model.fit(X_train_combined_features, y_train)\n",
        "        print(\"Logistic Regression model trained.\")\n",
        "\n",
        "        print(\"\\n--- Model Evaluation ---\")\n",
        "        y_pred = classifier_model.predict(X_test_combined_features)\n",
        "        print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 2: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- Step 3: Prediction and Output Structuring ---\n",
        "    print(\"\\n--- Step 3: Prediction and Output Structuring (Compacted Output) ---\")\n",
        "    try:\n",
        "        df_new_remarks_raw = pd.read_excel(new_remarks_excel_path)\n",
        "        print(f\"Successfully loaded new remarks from '{new_remarks_excel_path}'. Shape: {df_new_remarks_raw.shape}\")\n",
        "\n",
        "        if remarks_column_in_new_file not in df_new_remarks_raw.columns:\n",
        "            raise KeyError(f\"Column '{remarks_column_in_new_file}' not found in '{new_remarks_excel_path}'. Available columns: {df_new_remarks_raw.columns.tolist()}\")\n",
        "\n",
        "        new_remarks_data = []\n",
        "        for idx, remark_raw in df_new_remarks_raw[remarks_column_in_new_file].items():\n",
        "            cleaned = clean_text(remark_raw)\n",
        "            if cleaned:\n",
        "                new_remarks_data.append({'original_index': idx, 'remark_raw': remark_raw, 'remark_cleaned': cleaned})\n",
        "\n",
        "        df_remarks_to_classify = pd.DataFrame(new_remarks_data)\n",
        "        print(f\"Extracted {len(df_remarks_to_classify)} valid remarks for classification.\")\n",
        "\n",
        "        print(\"\\nGenerating embeddings for new remarks...\")\n",
        "        new_remarks_embeddings = model_embedding.encode(\n",
        "            df_remarks_to_classify['remark_cleaned'].tolist(),\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        # Extract and scale time features for new remarks\n",
        "        print(\"Extracting and scaling time features for new remarks...\")\n",
        "        new_remarks_time_features_df = extract_time_features(df_remarks_to_classify['remark_cleaned'])\n",
        "        new_remarks_time_features_scaled = scaler.transform(new_remarks_time_features_df) # Use the *same* scaler from training\n",
        "\n",
        "        # Concatenate for prediction\n",
        "        new_remarks_combined_features = np.hstack((new_remarks_embeddings, new_remarks_time_features_scaled))\n",
        "\n",
        "        print(\"\\nPredicting categories for new remarks...\")\n",
        "        predicted_categories = classifier_model.predict(new_remarks_combined_features)\n",
        "        df_remarks_to_classify['predicted_category'] = predicted_categories\n",
        "        print(\"First 5 remarks with predicted categories:\")\n",
        "        print(df_remarks_to_classify.head())\n",
        "\n",
        "        # --- MODIFIED PART FOR COMPACTED OUTPUT ---\n",
        "        print(\"\\nCompacting output into wide format by pushing remarks to the top of each category column...\")\n",
        "\n",
        "        categorized_remarks_by_column = defaultdict(list)\n",
        "\n",
        "        for idx, row_data in df_remarks_to_classify.iterrows():\n",
        "            remark = row_data['remark_raw']\n",
        "            predicted_cat = row_data['predicted_category']\n",
        "            categorized_remarks_by_column[predicted_cat].append(remark)\n",
        "\n",
        "        max_remarks_in_any_cat = 0\n",
        "        if categorized_remarks_by_column:\n",
        "            max_remarks_in_any_cat = max(len(v) for v in categorized_remarks_by_column.values())\n",
        "\n",
        "        df_output_compacted_wide = pd.DataFrame({\n",
        "            col: categorized_remarks_by_column.get(col, []) + [''] * (max_remarks_in_any_cat - len(categorized_remarks_by_column.get(col, [])))\n",
        "            for col in sorted(list(set(desired_category_labels)))\n",
        "        })\n",
        "\n",
        "        print(f\"\\nFinal Compacted Wide Output DataFrame shape: {df_output_compacted_wide.shape}\")\n",
        "        print(\"First 5 rows of the Final Compacted Wide Output DataFrame:\")\n",
        "        print(df_output_compacted_wide.head())\n",
        "\n",
        "        print(f\"\\nSaving results to '{output_excel_path}'...\")\n",
        "        df_output_compacted_wide.to_excel(output_excel_path, index=False)\n",
        "        print(\"Results saved successfully.\")\n",
        "\n",
        "        # --- NEW LINES ADDED HERE FOR MODEL SAVING ---\n",
        "        print(\"\\n--- Saving trained models for future use ---\")\n",
        "        try:\n",
        "            joblib.dump(model_embedding, 'sentence_transformer_model.pkl')\n",
        "            joblib.dump(classifier_model, 'logistic_regression_classifier.pkl')\n",
        "            joblib.dump(scaler, 'scaler_for_time_features.pkl')\n",
        "            print(\"Models (Sentence Transformer, Classifier, Scaler) saved successfully to .pkl files.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to save models: {e}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "        # --- END OF NEW LINES ---\n",
        "\n",
        "        # --- Final Verification of Total Count ---\n",
        "        print(\"\\n--- Final Verification of Total Count ---\")\n",
        "        df_categorized_check = pd.read_excel(output_excel_path)\n",
        "        print(f\"Shape of the re-loaded compacted categorized file: {df_categorized_check.shape}\")\n",
        "\n",
        "        total_categorized_remarks_from_file = 0\n",
        "        for col in df_categorized_check.columns:\n",
        "            total_categorized_remarks_from_file += df_categorized_check[col].apply(lambda x: pd.notna(x) and str(x).strip() != '').sum()\n",
        "\n",
        "        print(f\"Total number of remarks in the compacted categorized file (non-empty cell count): {total_categorized_remarks_from_file}\")\n",
        "        print(f\"Number of remarks successfully extracted and classified (excluding empty after cleaning): {len(df_remarks_to_classify)}\")\n",
        "\n",
        "        if total_categorized_remarks_from_file == len(df_remarks_to_classify):\n",
        "            print(\"All extracted and classified remarks successfully written to compacted output file.\")\n",
        "        else:\n",
        "            print(f\"Mismatch: {len(df_remarks_to_classify) - total_categorized_remarks_from_file} remarks missing from final file count. Investigate Excel saving/loading or if some cells are truly empty/NaN in source.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: The file '{new_remarks_excel_path}' not found. Please ensure it's in the same directory.\")\n",
        "        return\n",
        "    except KeyError as e:\n",
        "        print(f\"ERROR: Missing expected column in '{new_remarks_excel_path}'. Details: {e}\")\n",
        "        if 'df_new_remarks_raw' in locals():\n",
        "            print(\"Available columns in your Excel file are:\", df_new_remarks_raw.columns.tolist())\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 3: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Supervised ML Categorization Pipeline Completed in {time.time() - start_full_pipeline_time:.2f} seconds ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_classification_pipeline()"
      ],
      "metadata": {
        "id": "reEP9ZQmk8Au",
        "collapsed": true,
        "outputId": "f9a7fb96-9bfe-4275-ef82-909cc6a5e837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c1b09d0e6d7a4341af2f3737a21f5930",
            "a8b80081e3c44dd59bde54483d0efc4a",
            "2837aea60ac8459ab9584645cc1b6205",
            "ef9fb61eb15449cfbddab37242e4843d",
            "04f13ff9bc5a4531968edd8d4caa81de",
            "636175a6cea640a7af0bc7bccc25884f",
            "d4efe7b5afab433385141a459ccf3fff",
            "76aaeab8f2954eea83a4610c31adb129",
            "a7bd2a2c565e4218ae9fb803ad164508",
            "c2b7f8f9f06545298db81a5625c87aa3",
            "fedac15805ca4418805f61a7bbc56fe7",
            "15487d17555547b38a969000dfaa14df",
            "d20a62cb5ac94847915c9a28d9025a55",
            "5afd5c4897094ccbb3b18dc1245ad8d7",
            "941db28cc44449968c8869bb885ecec1",
            "703f17748ff54f87ac6e05135367f33a",
            "f0b59cf4ff90428fa938a521e143bcd4",
            "d6438f4cbdf34fcc9259f3a05fca0e0d",
            "e01c9b533c884fff9104d88dd7df7f6b",
            "4eba33ed74c6408ca981783a91320b79",
            "06a7e64d99b84837bee9dd8fd239933c",
            "72297e1d2cae42b191e365362c4e9e13",
            "4cc3f418393340f39c79c4b3f21a3ea8",
            "7c80c999838f4fadafa35255c0fb9190",
            "0be9bdc44e514bdb8fe97d80bb7fa16a",
            "f1d8fa1f749e4ad3b22711172d901b22",
            "4042d9eaa24f437facc1ac698109847a",
            "e51040a2765e4f8abc7c8f8d69da5ee6",
            "690088567139461ea3f5fa099401d135",
            "76a5deb1ffd64820aeaf13c6da44205e",
            "86f080139e8e439fb0e21e64d8514684",
            "799b2095212d47a0878afd4ab8cb1a99",
            "4a1c7def609b45d68239e34690a30951"
          ]
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Supervised ML Remark Categorization Pipeline (Time-Aware) ---\n",
            "\n",
            "--- Step 1: Data Preparation for Training ---\n",
            "Successfully loaded './Book1.xlsx'. Shape: (4901, 9)\n",
            "First 5 rows of the raw labeled data:\n",
            "                                   Less than 4 hours  \\\n",
            "0    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "1    \\t\\t\\nConsumer Problem- Supply Failed In His...   \n",
            "2   \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In H...   \n",
            "3   \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In H...   \n",
            "4   Consumer Problem - SUPPLY FAILED  in his vill...   \n",
            "\n",
            "                                  More than 12 hours  \\\n",
            "0  \\t\\t\\t\\t\\nConsumer Problem- Supply Failed In H...   \n",
            "1  \\t\\t\\t\\t\\nConsumer Problem- Supply Failed In H...   \n",
            "2  \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In Hi...   \n",
            "3  \\n\\t\\t\\t\\t\\t\\t\\t\\t\\nConsumer Problem-one phase...   \n",
            "4                12 hours se personal supply bhadhit   \n",
            "\n",
            "                                  More than 24 hours  \\\n",
            "0     \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In...   \n",
            "1    \\t\\t\\nConsumer Problem- Supply Failed In His...   \n",
            "2    \\t\\n  VILL Transformer damage from last- 5 D...   \n",
            "3    PTW-  Transformer Damage From Last - 20  day...   \n",
            "4    VILL Transformer damage from last- 2 DAYS   ...   \n",
            "\n",
            "                                   More than 4 hours  \\\n",
            "0    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "1    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "2    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "3   \\t\\t\\t \\t\\t\\t\\t\\nCONSUMER PROBLEM- SUPPLY FAI...   \n",
            "4   \\t\\t\\t\\nVILL Transformer damage from last day...   \n",
            "\n",
            "                      Consumer Power Supply Failures  \\\n",
            "0    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "1    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "2    \\t\\t\\nConsumer Problem- Supply Failed In His...   \n",
            "3    \\nConsumer Problem-Supply Failed In His  Hou...   \n",
            "4                                   Loose from pole    \n",
            "\n",
            "                       Failed Pole Incident Category  \\\n",
            "0                      Cable disconnected from pole.   \n",
            "1  Current is not coming in meter from electricit...   \n",
            "2                                   Loose from  pole   \n",
            "3                                   Loose from pole    \n",
            "4                                  11 kva line fault   \n",
            "\n",
            "                              Other Language Remarks  \\\n",
            "0                                     fault in pole    \n",
            "1                                      pole se khrab   \n",
            "2  \\tDV01072505347...................... pending\\n\\n   \n",
            "3                                 1 phase ni aa rha    \n",
            "4  2 PHASE NOT COMING AT BSNL BARUASAGAR TELEPHON...   \n",
            "\n",
            "                        Partial Phase Supply Failure  \\\n",
            "0  2 PHASE NOT COMING AT BSNL BARUASAGAR TELEPHON...   \n",
            "1  Low voltage in one phase thats why solar plant...   \n",
            "2                                 one phase not come   \n",
            "3  My area light mostly missing most of time some...   \n",
            "4                       SUPPLY NOT AVAILABLE IN BANK   \n",
            "\n",
            "                  Transformer Damage Causing Outages  \n",
            "0   PTW  Transformer damage from last days -\\t1 w...  \n",
            "1   PTW Transformer damage from last days - 1 mon...  \n",
            "2   PTW Transformer Damage From Last-1 MONTH \\t\\t...  \n",
            "3   VILL - tf damage from last\\t- Today Morning \\...  \n",
            "4    \\t\\t\\nConsumer Problem- Supply Failed In His...  \n",
            "Columns identified from the raw labeled data:\n",
            "['Less than 4 hours', 'More than 12 hours', 'More than 24 hours', 'More than 4 hours', 'Consumer Power Supply Failures', 'Failed Pole Incident Category', 'Other Language Remarks', 'Partial Phase Supply Failure', 'Transformer Damage Causing Outages']\n",
            "\n",
            "Identified 9 Unique Category Labels for Training:\n",
            "['Partial Phase Supply Failure', 'Transformer Damage Causing Outages', 'Consumer Power Supply Failures', 'More than 24 hours', 'More than 4 hours', 'Failed Pole Incident Category', 'Other Language Remarks', 'More than 12 hours', 'Less than 4 hours']\n",
            "Columns from './Book1.xlsx' that will be processed:\n",
            "['Less than 4 hours', 'More than 12 hours', 'More than 24 hours', 'More than 4 hours', 'Consumer Power Supply Failures', 'Failed Pole Incident Category', 'Other Language Remarks', 'Partial Phase Supply Failure', 'Transformer Damage Causing Outages']\n",
            "\n",
            "Transformed data for training shape: (8780, 3)\n",
            "First 5 rows of the transformed labeled data for training:\n",
            "                                     remark_original  \\\n",
            "0    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "1    \\t\\t\\nConsumer Problem- Supply Failed In His...   \n",
            "2   \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In H...   \n",
            "3   \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In H...   \n",
            "4   Consumer Problem - SUPPLY FAILED  in his vill...   \n",
            "\n",
            "                                      remark_cleaned           category  \n",
            "0  consumer problem-supply failed in his area fro...  Less than 4 hours  \n",
            "1  consumer problem- supply failed in his persona...  Less than 4 hours  \n",
            "2  consumer problem-supply failed in his area fro...  Less than 4 hours  \n",
            "3  consumer problem-supply failed in his half are...  Less than 4 hours  \n",
            "4  consumer problem - supply failed in his villag...  Less than 4 hours  \n",
            "Value counts for 'category' (training labels):\n",
            "category\n",
            "Consumer Power Supply Failures        4901\n",
            "Partial Phase Supply Failure          1062\n",
            "Failed Pole Incident Category          925\n",
            "Transformer Damage Causing Outages     753\n",
            "More than 24 hours                     347\n",
            "Less than 4 hours                      312\n",
            "More than 4 hours                      199\n",
            "More than 12 hours                     144\n",
            "Other Language Remarks                 137\n",
            "Name: count, dtype: int64\n",
            "Total unique categories identified in training data: 9\n",
            "\n",
            "--- Step 2: Feature Extraction (Sentence Embeddings + Time Features) and Model Training ---\n",
            "Loading SentenceTransformer model on device: cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2574403824.py:65: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  is_am_pm_mentioned = remarks_series.str.contains(am_pm_pattern, regex=True).astype(int)\n",
            "/tmp/ipython-input-2574403824.py:65: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  is_am_pm_mentioned = remarks_series.str.contains(am_pm_pattern, regex=True).astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for training text...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/220 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1b09d0e6d7a4341af2f3737a21f5930"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for testing text...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/55 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15487d17555547b38a969000dfaa14df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression model on combined features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Overall Accuracy: 0.9516\n",
            "\n",
            "Classification Report:\n",
            "                                    precision    recall  f1-score   support\n",
            "\n",
            "    Consumer Power Supply Failures       1.00      0.95      0.97       980\n",
            "     Failed Pole Incident Category       0.99      0.96      0.97       185\n",
            "                 Less than 4 hours       0.88      0.94      0.91        63\n",
            "                More than 12 hours       0.85      1.00      0.92        29\n",
            "                More than 24 hours       0.74      0.83      0.78        69\n",
            "                 More than 4 hours       0.65      0.85      0.74        40\n",
            "            Other Language Remarks       0.68      0.96      0.80        27\n",
            "      Partial Phase Supply Failure       0.98      0.98      0.98       212\n",
            "Transformer Damage Causing Outages       0.93      0.98      0.95       151\n",
            "\n",
            "                          accuracy                           0.95      1756\n",
            "                         macro avg       0.86      0.94      0.89      1756\n",
            "                      weighted avg       0.96      0.95      0.95      1756\n",
            "\n",
            "\n",
            "--- Step 3: Prediction and Output Structuring (Compacted Output) ---\n",
            "Successfully loaded new remarks from './Supply.xlsx'. Shape: (535446, 1)\n",
            "Extracted 502152 valid remarks for classification.\n",
            "\n",
            "Generating embeddings for new remarks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/15693 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cc3f418393340f39c79c4b3f21a3ea8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting and scaling time features for new remarks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2574403824.py:65: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  is_am_pm_mentioned = remarks_series.str.contains(am_pm_pattern, regex=True).astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicting categories for new remarks...\n",
            "First 5 remarks with predicted categories:\n",
            "   original_index                                         remark_raw  \\\n",
            "0               0     \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In...   \n",
            "1               1    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "2               2    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "3               3    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "4               4    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "\n",
            "                                      remark_cleaned  \\\n",
            "0  consumer problem-supply failed in his gali fro...   \n",
            "1  consumer problem-supply failed in his area fro...   \n",
            "2  consumer problem-supply failed in his area fro...   \n",
            "3  consumer problem-supply failed in his area fro...   \n",
            "4  consumer problem-supply failed in his area fro...   \n",
            "\n",
            "               predicted_category  \n",
            "0              More than 24 hours  \n",
            "1  Consumer Power Supply Failures  \n",
            "2               More than 4 hours  \n",
            "3               More than 4 hours  \n",
            "4               Less than 4 hours  \n",
            "\n",
            "Compacting output into wide format by pushing remarks to the top of each category column...\n",
            "\n",
            "Final Compacted Wide Output DataFrame shape: (180681, 9)\n",
            "First 5 rows of the Final Compacted Wide Output DataFrame:\n",
            "                      Consumer Power Supply Failures  \\\n",
            "0    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "1    \\t\\t\\nConsumer Problem- Supply Failed In His...   \n",
            "2    \\nConsumer Problem-Supply Failed In His  Hou...   \n",
            "3   \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In H...   \n",
            "4   \\t\\t\\t\\nConsumer Problem-  supply failed in h...   \n",
            "\n",
            "                       Failed Pole Incident Category  \\\n",
            "0                                     fault in pole    \n",
            "1                      Cable disconnected from pole.   \n",
            "2  Current is not coming in meter from electricit...   \n",
            "3                                   Loose from  pole   \n",
            "4                                   Loose from pole    \n",
            "\n",
            "                                   Less than 4 hours  \\\n",
            "0    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "1    \\t\\t\\nConsumer Problem- Supply Failed In His...   \n",
            "2   \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In H...   \n",
            "3   \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In H...   \n",
            "4   \\t\\t\\t\\nConsumer Problem- One phase damage in...   \n",
            "\n",
            "                                  More than 12 hours  \\\n",
            "0   \\nConsumer Problem-Supply Failed In His 20 ho...   \n",
            "1  \\t\\t\\t\\t\\nConsumer Problem- Supply Failed In H...   \n",
            "2  \\t\\t\\t\\t\\nConsumer Problem- Supply Failed In H...   \n",
            "3  \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In Hi...   \n",
            "4  \\n\\t\\t\\t\\t\\t\\t\\t\\t\\nConsumer Problem-one phase...   \n",
            "\n",
            "                                  More than 24 hours  \\\n",
            "0     \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In...   \n",
            "1    \\t\\t\\nConsumer Problem- Supply Failed In His...   \n",
            "2    \\nConsumer Problem-Supply Failed In His  4 h...   \n",
            "3   -\\t\\t\\t\\t\\nConsumer Problem -supply failed in...   \n",
            "4   \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In H...   \n",
            "\n",
            "                                   More than 4 hours  \\\n",
            "0    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "1    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "2    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "3    \\t\\t\\t\\t\\nConsumer Problem-Supply Failed In ...   \n",
            "4   \\t\\t\\t \\t\\t\\t\\t\\nCONSUMER PROBLEM- SUPPLY FAI...   \n",
            "\n",
            "                              Other Language Remarks  \\\n",
            "0                                      pole se khrab   \n",
            "1  \\tDV01072505347...................... pending\\n\\n   \n",
            "2                                 1 phase ni aa rha    \n",
            "3          11000 wali line ke fault ke sambandh mein   \n",
            "4  15 ghante se koi supply nhi a rhi h indus towe...   \n",
            "\n",
            "                        Partial Phase Supply Failure  \\\n",
            "0   Consumer Problem- One phase damage in his  Ar...   \n",
            "1  Consumer Problem - One phase damage in his HAL...   \n",
            "2  Consumer Problem- One phase damage  Area\\t\\t\\t...   \n",
            "3  Consumer Problem- One phase damage \\nFrom When...   \n",
            "4  Consumer Problem- One phase damage in his   fa...   \n",
            "\n",
            "                  Transformer Damage Causing Outages  \n",
            "0    \\t\\n  VILL Transformer damage from last- 5 D...  \n",
            "1    PTW-  Transformer Damage From Last - 20  day...  \n",
            "2    VILL Transformer damage from last- 2 DAYS   ...  \n",
            "3   \\t\\t\\t\\nVILL Transformer damage from last day...  \n",
            "4   \\nVill  Transformer Damage From Last- 2 days\\...  \n",
            "\n",
            "Saving results to './categorized_remarks_ML_model.xlsx'...\n",
            "Results saved successfully.\n",
            "\n",
            "--- Saving trained models for future use ---\n",
            "Models (Sentence Transformer, Classifier, Scaler) saved successfully to .pkl files.\n",
            "\n",
            "--- Final Verification of Total Count ---\n",
            "Shape of the re-loaded compacted categorized file: (180681, 9)\n",
            "Total number of remarks in the compacted categorized file (non-empty cell count): 502147\n",
            "Number of remarks successfully extracted and classified (excluding empty after cleaning): 502152\n",
            "Mismatch: 5 remarks missing from final file count. Investigate Excel saving/loading or if some cells are truly empty/NaN in source.\n",
            "\n",
            "--- Supervised ML Categorization Pipeline Completed in 205.34 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Define the filenames your code created\n",
        "files_to_download = [\n",
        "    'sentence_transformer_model.pkl',\n",
        "    'logistic_regression_classifier.pkl',\n",
        "    'scaler_for_time_features.pkl'\n",
        "]\n",
        "\n",
        "# Define your custom zip filename\n",
        "zip_filename = 'supervised_model_supply_remarks.zip'\n",
        "\n",
        "# Check if files exist before trying to zip them\n",
        "existing_files = [f for f in files_to_download if os.path.exists(f)]\n",
        "\n",
        "if len(existing_files) == 3:\n",
        "    print(f\"All model files found. Zipping them into '{zip_filename}'...\")\n",
        "\n",
        "    # Zip the files into one archive\n",
        "    # We use the exact filename you requested\n",
        "    !zip -r supervised_model_supply_remarks.zip sentence_transformer_model.pkl logistic_regression_classifier.pkl scaler_for_time_features.pkl\n",
        "\n",
        "    # Trigger the browser download\n",
        "    print(f\"Downloading '{zip_filename}'...\")\n",
        "    files.download(zip_filename)\n",
        "else:\n",
        "    print(f\"Error: Could not find all files. Found: {existing_files}\")\n",
        "    print(\"Did you run the previous cells successfully?\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uRVstpCcbpGn",
        "outputId": "d69c6b1c-645b-4f92-bda0-d9793f403d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All model files found. Zipping them into 'supervised_model_supply_remarks.zip'...\n",
            "  adding: sentence_transformer_model.pkl (deflated 9%)\n",
            "  adding: logistic_regression_classifier.pkl (deflated 7%)\n",
            "  adding: scaler_for_time_features.pkl (deflated 40%)\n",
            "Downloading 'supervised_model_supply_remarks.zip'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8e917fdb-46aa-4abb-a5b4-75f69515df53\", \"supervised_model_supply_remarks.zip\", 83379817)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Model of Billing"
      ],
      "metadata": {
        "id": "_H-SXxvyntJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Download NLTK stopwords\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Define stopwords\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'area', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it', 'its', 'her', 'their', 'our',\n",
        "    'what', 'where', 'how', 'why', 'who', 'whom', 'which', 'whether',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hr', 'hrs', 'hour', 'hours', 'time', 'date', 'week', 'month', 'year', 'ago',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'code', 'id', 'location', 'address', 'phone', 'mobile', 'call', 'report', 'registered',\n",
        "    'ok', 'yes', 'no', 'not', 'hi', 'hello', 'sir', 'madam', 'pls', 'please', 'regards', 'type', 'urban', 'complaint', 'detail', 'general',\n",
        "    'kv', 'tf', 'na', 'service', 'request', 'feedback', 'query', 'regarding', 'about', 'given', 'areas', 'village'\n",
        "])\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Removes extra whitespace and standardizes text.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"-/]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text.strip()).lower()\n",
        "    return text\n",
        "\n",
        "def main_classification_pipeline():\n",
        "    # --- Configuration ---\n",
        "    labeled_data_excel_path = \"./DATA.xlsx\"\n",
        "    new_remarks_excel_path = \"./Bill.xlsx\"\n",
        "    output_excel_path = \"./categorized_remarks_ML_model_Billing.xlsx\"\n",
        "\n",
        "    # I have added commas here to ensure these are treated as separate categories\n",
        "    desired_category_labels = [\n",
        "        \"Bill Accuracy and Discrepancies\",\n",
        "        \"Bill Hold Preventing Online Payments\",\n",
        "        \"Billing Discrepancies Due to Meter Readings\",\n",
        "        \"Customer Billing Not Received or Available\",\n",
        "        \"Domestic Meter Reading Collection Failure\",\n",
        "        \"Electricity Bill Discrepancies and Solar Units\",\n",
        "        \"Unclear Remarks\",\n",
        "        \"Unacknowledged Customer Payments and Billing\",\n",
        "        \"Other Language Remarks\"\n",
        "    ]\n",
        "\n",
        "    # Column containing remarks in the new file\n",
        "    remarks_column_in_new_file = \"REMARKS\"\n",
        "\n",
        "    print(\"--- Starting Supervised ML Remark Categorization Pipeline (Text Only) ---\")\n",
        "    start_full_pipeline_time = time.time()\n",
        "\n",
        "    # --- Step 1: Data Preparation for Training ---\n",
        "    print(\"\\n--- Step 1: Data Preparation for Training ---\")\n",
        "    try:\n",
        "        df_raw_labeled = pd.read_excel(labeled_data_excel_path, header=0)\n",
        "        print(f\"Successfully loaded '{labeled_data_excel_path}'. Shape: {df_raw_labeled.shape}\")\n",
        "\n",
        "        loaded_to_label_map = {}\n",
        "        seen_labels = {}\n",
        "\n",
        "        for col in df_raw_labeled.columns:\n",
        "            normalized_col = col.split('.')[0]\n",
        "\n",
        "            if normalized_col in desired_category_labels:\n",
        "                if normalized_col in seen_labels:\n",
        "                    loaded_to_label_map[col] = normalized_col\n",
        "                else:\n",
        "                    loaded_to_label_map[col] = col\n",
        "                    seen_labels[normalized_col] = True\n",
        "            else:\n",
        "                # Silent skip or optional warning\n",
        "                pass\n",
        "\n",
        "        active_category_columns_in_df = list(loaded_to_label_map.keys())\n",
        "\n",
        "        print(f\"\\nIdentified {len(set(loaded_to_label_map.values()))} Unique Category Labels for Training.\")\n",
        "\n",
        "        labeled_data_for_training = []\n",
        "        for category_col_in_df in active_category_columns_in_df:\n",
        "            standard_category_label = loaded_to_label_map[category_col_in_df]\n",
        "            for remark_entry in df_raw_labeled[category_col_in_df].dropna():\n",
        "                cleaned_remark = clean_text(str(remark_entry))\n",
        "                if cleaned_remark:\n",
        "                    labeled_data_for_training.append({\n",
        "                        'remark_original': str(remark_entry),\n",
        "                        'remark_cleaned': cleaned_remark,\n",
        "                        'category': standard_category_label\n",
        "                    })\n",
        "\n",
        "        df_labeled_for_training = pd.DataFrame(labeled_data_for_training)\n",
        "        print(f\"Transformed data for training shape: {df_labeled_for_training.shape}\")\n",
        "        print(f\"Total unique categories identified: {df_labeled_for_training['category'].nunique()}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: The file '{labeled_data_excel_path}' not found.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 1: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- Step 2: Feature Extraction and Model Training ---\n",
        "    print(\"\\n--- Step 2: Feature Extraction (Embeddings Only) and Model Training ---\")\n",
        "    try:\n",
        "        X_train_data = df_labeled_for_training['remark_cleaned'].tolist()\n",
        "        y_train_labels = df_labeled_for_training['category'].tolist()\n",
        "\n",
        "        # Split data\n",
        "        X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "            pd.Series(X_train_data), y_train_labels, test_size=0.2, random_state=42, stratify=y_train_labels\n",
        "        )\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Loading SentenceTransformer model on device: {device}...\")\n",
        "        model_embedding = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "        print(\"Generating embeddings for training text...\")\n",
        "        X_train_embeddings = model_embedding.encode(X_train_text.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "        print(\"Generating embeddings for testing text...\")\n",
        "        X_test_embeddings = model_embedding.encode(X_test_text.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "        # NOTE: Removed Time Feature Extraction and Scaling\n",
        "        # We train directly on the embeddings now\n",
        "\n",
        "        print(\"\\nTraining Logistic Regression model on text embeddings...\")\n",
        "        classifier_model = LogisticRegression(\n",
        "            max_iter=1000, solver='lbfgs', multi_class='auto', class_weight='balanced', random_state=42, n_jobs=-1\n",
        "        )\n",
        "        classifier_model.fit(X_train_embeddings, y_train)\n",
        "        print(\"Logistic Regression model trained.\")\n",
        "\n",
        "        print(\"\\n--- Model Evaluation ---\")\n",
        "        y_pred = classifier_model.predict(X_test_embeddings)\n",
        "        print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 2: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- Step 3: Prediction and Output Structuring ---\n",
        "    print(\"\\n--- Step 3: Prediction and Output Structuring ---\")\n",
        "    try:\n",
        "        df_new_remarks_raw = pd.read_excel(new_remarks_excel_path)\n",
        "        print(f\"Successfully loaded new remarks from '{new_remarks_excel_path}'. Shape: {df_new_remarks_raw.shape}\")\n",
        "\n",
        "        if remarks_column_in_new_file not in df_new_remarks_raw.columns:\n",
        "            raise KeyError(f\"Column '{remarks_column_in_new_file}' not found.\")\n",
        "\n",
        "        new_remarks_data = []\n",
        "        for idx, remark_raw in df_new_remarks_raw[remarks_column_in_new_file].items():\n",
        "            cleaned = clean_text(remark_raw)\n",
        "            if cleaned:\n",
        "                new_remarks_data.append({'original_index': idx, 'remark_raw': remark_raw, 'remark_cleaned': cleaned})\n",
        "\n",
        "        df_remarks_to_classify = pd.DataFrame(new_remarks_data)\n",
        "        print(f\"Extracted {len(df_remarks_to_classify)} valid remarks for classification.\")\n",
        "\n",
        "        print(\"\\nGenerating embeddings for new remarks...\")\n",
        "        new_remarks_embeddings = model_embedding.encode(\n",
        "            df_remarks_to_classify['remark_cleaned'].tolist(),\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        print(\"Predicting categories for new remarks...\")\n",
        "        predicted_categories = classifier_model.predict(new_remarks_embeddings)\n",
        "        df_remarks_to_classify['predicted_category'] = predicted_categories\n",
        "\n",
        "        # --- Compacted Output ---\n",
        "        print(\"Compacting output into wide format...\")\n",
        "        categorized_remarks_by_column = defaultdict(list)\n",
        "\n",
        "        for idx, row_data in df_remarks_to_classify.iterrows():\n",
        "            remark = row_data['remark_raw']\n",
        "            predicted_cat = row_data['predicted_category']\n",
        "            categorized_remarks_by_column[predicted_cat].append(remark)\n",
        "\n",
        "        max_remarks_in_any_cat = 0\n",
        "        if categorized_remarks_by_column:\n",
        "            max_remarks_in_any_cat = max(len(v) for v in categorized_remarks_by_column.values())\n",
        "\n",
        "        df_output_compacted_wide = pd.DataFrame({\n",
        "            col: categorized_remarks_by_column.get(col, []) + [''] * (max_remarks_in_any_cat - len(categorized_remarks_by_column.get(col, [])))\n",
        "            for col in sorted(list(set(desired_category_labels)))\n",
        "        })\n",
        "\n",
        "        print(f\"Saving results to '{output_excel_path}'...\")\n",
        "        df_output_compacted_wide.to_excel(output_excel_path, index=False)\n",
        "        print(\"Results saved successfully.\")\n",
        "\n",
        "        # --- Saving Models ---\n",
        "        print(\"\\n--- Saving trained models ---\")\n",
        "        try:\n",
        "            joblib.dump(model_embedding, 'sentence_transformer_model.pkl')\n",
        "            joblib.dump(classifier_model, 'logistic_regression_classifier.pkl')\n",
        "            # Note: No scaler saved because it was not used\n",
        "            print(\"Models (Sentence Transformer, Classifier) saved successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to save models: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 3: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Categorization Pipeline Completed in {time.time() - start_full_pipeline_time:.2f} seconds ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_classification_pipeline()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V_2pTMc1peIF",
        "outputId": "575cecec-8afd-4fa5-9a31-e54367f64a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2b8cbc78e9d7464a99735744994f2494",
            "c7d9b4d0cdfe411e9ca479172523b925",
            "372cb35e55964d0ca9de26fd6a48feb5",
            "682234fd273142b8beac59e4d5f8d754",
            "19a2ca48fd8346c0998770d3abb6ad93",
            "9fbd2a57f0784d77aa5a347c45804b1d",
            "4bb506592c58407db899d686a69a5fd5",
            "3b66d58384254d7483549403ec5b4c5c",
            "c1aa54fbde5c482798dfa3e3dae56035",
            "d8afba8c903e469d9dcb141585faefc9",
            "8ef4ca64de484eb3b486a0fe7f48fe2f",
            "8f7589fe642b48c7962c76a5e8bbfbb3",
            "4e37b229cf6046d9889e241af0242195",
            "dceedd7187fe4ec3b890381e13417a2d",
            "5b11d876d04248d5b14796c632efc4fe",
            "7038ebd521a84a90a1974c4076ec9dea",
            "84d1e2e13ae34bc2938187641f70d9d0",
            "ec30d0eef5fe444b951a70404ab481f3",
            "f1cb164be7ae4bbab5e2c1730ff37f9f",
            "89b415821880492aa10b865324b57176",
            "d20279d7a92f4d2595b449d8aee8db44",
            "12a8d3b693264125b40d5e654b6a614c",
            "eaecadd46c694a6395597b2fda3e1c63",
            "390b5a305b084450b2a5501a747df4d3",
            "9da04535614f41f592b9688337e9c83c",
            "a62baa279f6f4a73a6d09ccfd7f36aba",
            "300706aae8a146ae84513f4c58f6bc82",
            "11cea07d13474d6e858b230fc8b108f5",
            "18d0fb68909d444d969cba2626b3e03d",
            "4add0112152348ff8e632873d58dd546",
            "fccead6c599e4ed58f7c77aaf0ce22dd",
            "e799ad1586e947f9bc3d11803b6c57b7",
            "bf93bf1888f943278afb0f708a54ff23"
          ]
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Supervised ML Remark Categorization Pipeline (Text Only) ---\n",
            "\n",
            "--- Step 1: Data Preparation for Training ---\n",
            "Successfully loaded './DATA.xlsx'. Shape: (1152, 9)\n",
            "\n",
            "Identified 8 Unique Category Labels for Training.\n",
            "Transformed data for training shape: (2556, 3)\n",
            "Total unique categories identified: 8\n",
            "\n",
            "--- Step 2: Feature Extraction (Embeddings Only) and Model Training ---\n",
            "Loading SentenceTransformer model on device: cuda...\n",
            "Generating embeddings for training text...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/64 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b8cbc78e9d7464a99735744994f2494"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for testing text...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f7589fe642b48c7962c76a5e8bbfbb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression model on text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Overall Accuracy: 0.9609\n",
            "\n",
            "Classification Report:\n",
            "                                                precision    recall  f1-score   support\n",
            "\n",
            "               Bill Accuracy and Discrepancies       0.94      0.94      0.94       111\n",
            "          Bill Hold Preventing Online Payments       0.89      0.89      0.89         9\n",
            "   Billing Discrepancies Due to Meter Readings       1.00      1.00      1.00        54\n",
            "    Customer Billing Not Received or Available       0.99      0.97      0.98       115\n",
            "     Domestic Meter Reading Collection Failure       1.00      1.00      1.00        70\n",
            "Electricity Bill Discrepancies and Solar Units       0.98      0.97      0.97        59\n",
            "                        Other Language Remarks       0.84      0.86      0.85        44\n",
            "  Unacknowledged Customer Payments and Billing       0.94      0.98      0.96        50\n",
            "\n",
            "                                      accuracy                           0.96       512\n",
            "                                     macro avg       0.95      0.95      0.95       512\n",
            "                                  weighted avg       0.96      0.96      0.96       512\n",
            "\n",
            "\n",
            "--- Step 3: Prediction and Output Structuring ---\n",
            "Successfully loaded new remarks from './Bill.xlsx'. Shape: (60729, 1)\n",
            "Extracted 51459 valid remarks for classification.\n",
            "\n",
            "Generating embeddings for new remarks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1609 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaecadd46c694a6395597b2fda3e1c63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting categories for new remarks...\n",
            "Compacting output into wide format...\n",
            "Saving results to './categorized_remarks_ML_model_Billing.xlsx'...\n",
            "Results saved successfully.\n",
            "\n",
            "--- Saving trained models ---\n",
            "Models (Sentence Transformer, Classifier) saved successfully.\n",
            "\n",
            "--- Categorization Pipeline Completed in 27.76 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Define the filenames (Scaler is removed)\n",
        "files_to_download = [\n",
        "    'sentence_transformer_model.pkl',\n",
        "    'logistic_regression_classifier.pkl'\n",
        "]\n",
        "\n",
        "# Define your custom zip filename\n",
        "zip_filename = 'supervised_model_billing_remarks.zip'\n",
        "\n",
        "# Check if files exist before trying to zip them\n",
        "existing_files = [f for f in files_to_download if os.path.exists(f)]\n",
        "\n",
        "if len(existing_files) == 2:\n",
        "    print(f\"All model files found. Zipping them into '{zip_filename}'...\")\n",
        "\n",
        "    # Zip only the two relevant files\n",
        "    !zip -r supervised_model_billing_remarks.zip sentence_transformer_model.pkl logistic_regression_classifier.pkl\n",
        "\n",
        "    # Trigger the browser download\n",
        "    print(f\"Downloading '{zip_filename}'...\")\n",
        "    files.download(zip_filename)\n",
        "else:\n",
        "    print(f\"Error: Could not find all files. Found: {existing_files}\")"
      ],
      "metadata": {
        "id": "0553966Qnspk",
        "outputId": "6e2894be-a1b0-4c65-c15c-724c063be3df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All model files found. Zipping them into 'supervised_model_billing_remarks.zip'...\n",
            "updating: sentence_transformer_model.pkl (deflated 9%)\n",
            "updating: logistic_regression_classifier.pkl (deflated 9%)\n",
            "Downloading 'supervised_model_billing_remarks.zip'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1a86bc8d-2d6f-4549-bff4-97fa2551983b\", \"supervised_model_billing_remarks.zip\", 83376037)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Others Reallocation with GEMINI"
      ],
      "metadata": {
        "id": "E5_nV7FiV-l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from joblib import Parallel, delayed\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- Constants ---\n",
        "BATCH_SIZE = 500\n",
        "MIN_REMARKS_FOR_NEW_COLUMN = 5\n",
        "SIMILARITY_THRESHOLD = 0.6\n",
        "NUM_CLUSTERS = 8\n",
        "MAX_FEATURES = 2000\n",
        "N_COMPONENTS = 100\n",
        "GEMINI_API_KEY = \"AIzaSyB3877RwLYTNB9Mhi7HhH8CBdG8ua8QtsM\"\n",
        "\n",
        "# Configure Gemini API\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# Load spaCy for text processing\n",
        "print(\"Loading spaCy language model...\")\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "    print(\"spaCy model loaded successfully\")\n",
        "except OSError:\n",
        "    print(\"Warning: spaCy English model not found. Using simpler text processing.\")\n",
        "    print(\"For better results, install with: python -m spacy download en_core_web_sm\")\n",
        "    nlp = None\n",
        "\n",
        "# --- Text Preprocessing ---\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Enhanced text cleaning and normalization\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    if nlp is not None:\n",
        "        doc = nlp(text)\n",
        "        tokens = [token.lemma_ for token in doc if not token.is_stop and len(token.lemma_) > 2]\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# --- Data Loading ---\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load Excel file with error handling\"\"\"\n",
        "    print(f\"Loading data from {file_path}...\")\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        print(f\"Successfully loaded {len(df)} rows\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# --- Vectorization Pipeline ---\n",
        "def create_vectorizer():\n",
        "    \"\"\"Create TF-IDF vectorizer with dimensionality reduction\"\"\"\n",
        "    print(\"Creating vectorization pipeline...\")\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=MAX_FEATURES,\n",
        "        stop_words='english',\n",
        "        preprocessor=preprocess_text\n",
        "    )\n",
        "\n",
        "    svd = TruncatedSVD(n_components=N_COMPONENTS)\n",
        "    normalizer = Normalizer(copy=False)\n",
        "    pipeline = make_pipeline(vectorizer, svd, normalizer)\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "# --- Category Modeling ---\n",
        "def build_category_models(reference_df, existing_columns, vectorizer):\n",
        "    \"\"\"Build representative models for each category\"\"\"\n",
        "    print(\"\\nBuilding category models...\")\n",
        "    category_models = {}\n",
        "\n",
        "    all_text = []\n",
        "    for col in existing_columns:\n",
        "        all_text.extend(reference_df[col].dropna().astype(str).tolist())\n",
        "\n",
        "    print(\"Fitting vectorizer on reference data...\")\n",
        "    vectorizer.fit(all_text)\n",
        "\n",
        "    for col in tqdm(existing_columns, desc=\"Processing categories\"):\n",
        "        remarks = reference_df[col].dropna().astype(str).tolist()\n",
        "\n",
        "        if not remarks:\n",
        "            remarks = [preprocess_text(col)]\n",
        "\n",
        "        vectors = vectorizer.transform(remarks)\n",
        "        category_models[col] = vectors.mean(axis=0)\n",
        "\n",
        "    return category_models, vectorizer\n",
        "\n",
        "# --- Gemini API Functions ---\n",
        "def generate_meaningful_name_with_gemini(top_terms, examples):\n",
        "    \"\"\"Generate professional column names using Gemini\"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "        Analyze these key terms and customer remarks to create a professional category name:\n",
        "\n",
        "        Key Terms: {', '.join(top_terms)}\n",
        "        Example Remarks:\n",
        "        {examples}\n",
        "\n",
        "        Create a concise, professional column name (3-5 words) that:\n",
        "        1. Accurately represents the common theme\n",
        "        2. Uses clear business terminology\n",
        "        3. Is specific yet broad enough to cover similar cases\n",
        "        4. Follows title case formatting\n",
        "\n",
        "        Provide ONLY the column name, no additional text.\n",
        "        \"\"\"\n",
        "\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text.strip().strip('\"').strip(\"'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating column name: {e}\")\n",
        "        return f\"{' '.join(t.title() for t in top_terms[:2])} Issues\"\n",
        "\n",
        "# --- Remark Categorization ---\n",
        "def categorize_remark(remark, category_models, vectorizer):\n",
        "    \"\"\"Categorize a single remark\"\"\"\n",
        "    if not isinstance(remark, str) or not remark.strip():\n",
        "        return None\n",
        "\n",
        "    preprocessed = preprocess_text(remark)\n",
        "    if not preprocessed:\n",
        "        return None\n",
        "\n",
        "    remark_vector = vectorizer.transform([preprocessed])\n",
        "\n",
        "    similarities = {\n",
        "        cat: cosine_similarity(remark_vector, model.reshape(1, -1))[0][0]\n",
        "        for cat, model in category_models.items()\n",
        "    }\n",
        "\n",
        "    best_cat = max(similarities.items(), key=lambda x: x[1])\n",
        "    return best_cat[0] if best_cat[1] > SIMILARITY_THRESHOLD else None\n",
        "\n",
        "# --- New Column Suggestion ---\n",
        "def suggest_new_columns(remarks, vectorizer, min_remarks=5):\n",
        "    \"\"\"Suggest new columns with meaningful names using Gemini\"\"\"\n",
        "    print(\"\\nAnalyzing uncategorized remarks for new column suggestions...\")\n",
        "\n",
        "    if len(remarks) < min_remarks:\n",
        "        print(f\"Not enough remarks ({len(remarks)}) to suggest new columns (minimum {min_remarks})\")\n",
        "        return {}\n",
        "\n",
        "    print(\"Vectorizing remarks...\")\n",
        "    X = vectorizer.transform(remarks)\n",
        "\n",
        "    print(f\"Clustering {len(remarks)} remarks into {NUM_CLUSTERS} groups...\")\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=min(NUM_CLUSTERS, len(remarks)),\n",
        "        random_state=42,\n",
        "        n_init=10\n",
        "    )\n",
        "    clusters = kmeans.fit_predict(X)\n",
        "\n",
        "    new_columns = {}\n",
        "    feature_names = vectorizer.named_steps['tfidfvectorizer'].get_feature_names_out()\n",
        "\n",
        "    for cluster_id in range(kmeans.n_clusters):\n",
        "        cluster_indices = np.where(clusters == cluster_id)[0]\n",
        "        cluster_size = len(cluster_indices)\n",
        "\n",
        "        if cluster_size < min_remarks:\n",
        "            continue\n",
        "\n",
        "        center = kmeans.cluster_centers_[cluster_id]\n",
        "        top_terms = center.argsort()[-3:][::-1]\n",
        "        top_terms = [feature_names[i] for i in top_terms]\n",
        "\n",
        "        example_indices = np.random.choice(cluster_indices, size=min(3, cluster_size), replace=False)\n",
        "        examples = [remarks[i] for i in example_indices]\n",
        "\n",
        "        try:\n",
        "            column_name = generate_meaningful_name_with_gemini(top_terms, examples)\n",
        "\n",
        "            new_columns[column_name] = {\n",
        "                'count': cluster_size,\n",
        "                'examples': examples\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating name for cluster {cluster_id}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if new_columns:\n",
        "        print(f\"Found {len(new_columns)} potential new columns\")\n",
        "    else:\n",
        "        print(\"No clear patterns found for new columns\")\n",
        "\n",
        "    return new_columns\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def process_remarks(input_path, reference_path, output_path):\n",
        "    \"\"\"Main processing pipeline\"\"\"\n",
        "    print(f\"\\nStarting remark categorization process\")\n",
        "    print(f\"Input file: {input_path}\")\n",
        "    print(f\"Reference file: {reference_path}\")\n",
        "    print(f\"Output file: {output_path}\\n\")\n",
        "\n",
        "    df = load_data(input_path)\n",
        "    ref_df = load_data(reference_path)\n",
        "\n",
        "    if 'Others' not in df.columns:\n",
        "        print(\"Error: Input file must contain an 'Others' column\")\n",
        "        return\n",
        "\n",
        "    existing_columns = [col for col in df.columns if col != 'Others']\n",
        "\n",
        "    vectorizer = create_vectorizer()\n",
        "    category_models, fitted_vectorizer = build_category_models(ref_df, existing_columns, vectorizer)\n",
        "\n",
        "    print(\"\\nProcessing remarks...\")\n",
        "    uncategorized = []\n",
        "    total = len(df)\n",
        "\n",
        "    for start in tqdm(range(0, total, BATCH_SIZE), desc=\"Processing batches\"):\n",
        "        end = min(start + BATCH_SIZE, total)\n",
        "        batch = df.iloc[start:end]\n",
        "\n",
        "        for idx, row in batch.iterrows():\n",
        "            remark = row['Others']\n",
        "            if pd.isna(remark):\n",
        "                continue\n",
        "\n",
        "            category = categorize_remark(str(remark), category_models, fitted_vectorizer)\n",
        "\n",
        "            if category:\n",
        "                df.at[idx, category] = remark\n",
        "                df.at[idx, 'Others'] = None\n",
        "            else:\n",
        "                uncategorized.append(remark)\n",
        "\n",
        "    new_columns = suggest_new_columns(uncategorized, fitted_vectorizer, MIN_REMARKS_FOR_NEW_COLUMN)\n",
        "\n",
        "    print(\"\\nSaving results...\")\n",
        "    df.to_excel(output_path, index=False)\n",
        "    print(f\"Results saved to {output_path}\")\n",
        "\n",
        "    print(\"\\n=== Processing Complete ===\")\n",
        "    print(f\"Total remarks processed: {total}\")\n",
        "    print(f\"Categorized remarks: {total - len(uncategorized)}\")\n",
        "    print(f\"Remaining uncategorized: {len(uncategorized)}\")\n",
        "\n",
        "    if new_columns:\n",
        "        print(\"\\n=== Suggested New Columns ===\")\n",
        "        for col, info in new_columns.items():\n",
        "            print(f\"\\nColumn: {col}\")\n",
        "            print(f\"Count: {info['count']}\")\n",
        "            print(\"Example remarks:\")\n",
        "            for ex in info['examples']:\n",
        "                print(f\" - {ex[:80]}{'...' if len(ex) > 80 else ''}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = 'categorized_remarks_others.xlsx'\n",
        "    reference_file = 'Book1.xlsx'\n",
        "    output_file = 'output.xlsx'\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Error: Input file not found - {input_file}\")\n",
        "    elif not os.path.exists(reference_file):\n",
        "        print(f\"Error: Reference file not found - {reference_file}\")\n",
        "    else:\n",
        "        process_remarks(input_file, reference_file, output_file)"
      ],
      "metadata": {
        "id": "DPF_kYpYrsDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40734616-b85d-4482-8892-482adfbee1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading spaCy language model...\n",
            "spaCy model loaded successfully\n",
            "\n",
            "Starting remark categorization process\n",
            "Input file: categorized_remarks_others.xlsx\n",
            "Reference file: Book1.xlsx\n",
            "Output file: output.xlsx\n",
            "\n",
            "Loading data from categorized_remarks_others.xlsx...\n",
            "Successfully loaded 165448 rows\n",
            "Loading data from Book1.xlsx...\n",
            "Successfully loaded 4901 rows\n",
            "Creating vectorization pipeline...\n",
            "\n",
            "Building category models...\n",
            "Fitting vectorizer on reference data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing categories: 100%|██████████| 9/9 [00:23<00:00,  2.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing remarks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batches: 100%|██████████| 331/331 [00:09<00:00, 33.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing uncategorized remarks for new column suggestions...\n",
            "Vectorizing remarks...\n",
            "Clustering 159 remarks into 8 groups...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "f0dCuukkY1Ex"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO5SAQqzbauaUn1di8L/ZOO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1b09d0e6d7a4341af2f3737a21f5930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8b80081e3c44dd59bde54483d0efc4a",
              "IPY_MODEL_2837aea60ac8459ab9584645cc1b6205",
              "IPY_MODEL_ef9fb61eb15449cfbddab37242e4843d"
            ],
            "layout": "IPY_MODEL_04f13ff9bc5a4531968edd8d4caa81de"
          }
        },
        "a8b80081e3c44dd59bde54483d0efc4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_636175a6cea640a7af0bc7bccc25884f",
            "placeholder": "​",
            "style": "IPY_MODEL_d4efe7b5afab433385141a459ccf3fff",
            "value": "Batches: 100%"
          }
        },
        "2837aea60ac8459ab9584645cc1b6205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76aaeab8f2954eea83a4610c31adb129",
            "max": 220,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7bd2a2c565e4218ae9fb803ad164508",
            "value": 220
          }
        },
        "ef9fb61eb15449cfbddab37242e4843d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2b7f8f9f06545298db81a5625c87aa3",
            "placeholder": "​",
            "style": "IPY_MODEL_fedac15805ca4418805f61a7bbc56fe7",
            "value": " 220/220 [00:01&lt;00:00, 153.23it/s]"
          }
        },
        "04f13ff9bc5a4531968edd8d4caa81de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "636175a6cea640a7af0bc7bccc25884f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4efe7b5afab433385141a459ccf3fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76aaeab8f2954eea83a4610c31adb129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7bd2a2c565e4218ae9fb803ad164508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2b7f8f9f06545298db81a5625c87aa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fedac15805ca4418805f61a7bbc56fe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15487d17555547b38a969000dfaa14df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d20a62cb5ac94847915c9a28d9025a55",
              "IPY_MODEL_5afd5c4897094ccbb3b18dc1245ad8d7",
              "IPY_MODEL_941db28cc44449968c8869bb885ecec1"
            ],
            "layout": "IPY_MODEL_703f17748ff54f87ac6e05135367f33a"
          }
        },
        "d20a62cb5ac94847915c9a28d9025a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0b59cf4ff90428fa938a521e143bcd4",
            "placeholder": "​",
            "style": "IPY_MODEL_d6438f4cbdf34fcc9259f3a05fca0e0d",
            "value": "Batches: 100%"
          }
        },
        "5afd5c4897094ccbb3b18dc1245ad8d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e01c9b533c884fff9104d88dd7df7f6b",
            "max": 55,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4eba33ed74c6408ca981783a91320b79",
            "value": 55
          }
        },
        "941db28cc44449968c8869bb885ecec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06a7e64d99b84837bee9dd8fd239933c",
            "placeholder": "​",
            "style": "IPY_MODEL_72297e1d2cae42b191e365362c4e9e13",
            "value": " 55/55 [00:00&lt;00:00, 147.68it/s]"
          }
        },
        "703f17748ff54f87ac6e05135367f33a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0b59cf4ff90428fa938a521e143bcd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6438f4cbdf34fcc9259f3a05fca0e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e01c9b533c884fff9104d88dd7df7f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eba33ed74c6408ca981783a91320b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06a7e64d99b84837bee9dd8fd239933c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72297e1d2cae42b191e365362c4e9e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cc3f418393340f39c79c4b3f21a3ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c80c999838f4fadafa35255c0fb9190",
              "IPY_MODEL_0be9bdc44e514bdb8fe97d80bb7fa16a",
              "IPY_MODEL_f1d8fa1f749e4ad3b22711172d901b22"
            ],
            "layout": "IPY_MODEL_4042d9eaa24f437facc1ac698109847a"
          }
        },
        "7c80c999838f4fadafa35255c0fb9190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e51040a2765e4f8abc7c8f8d69da5ee6",
            "placeholder": "​",
            "style": "IPY_MODEL_690088567139461ea3f5fa099401d135",
            "value": "Batches: 100%"
          }
        },
        "0be9bdc44e514bdb8fe97d80bb7fa16a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76a5deb1ffd64820aeaf13c6da44205e",
            "max": 15693,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86f080139e8e439fb0e21e64d8514684",
            "value": 15693
          }
        },
        "f1d8fa1f749e4ad3b22711172d901b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_799b2095212d47a0878afd4ab8cb1a99",
            "placeholder": "​",
            "style": "IPY_MODEL_4a1c7def609b45d68239e34690a30951",
            "value": " 15693/15693 [01:47&lt;00:00, 182.34it/s]"
          }
        },
        "4042d9eaa24f437facc1ac698109847a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e51040a2765e4f8abc7c8f8d69da5ee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "690088567139461ea3f5fa099401d135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76a5deb1ffd64820aeaf13c6da44205e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86f080139e8e439fb0e21e64d8514684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "799b2095212d47a0878afd4ab8cb1a99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a1c7def609b45d68239e34690a30951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b8cbc78e9d7464a99735744994f2494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7d9b4d0cdfe411e9ca479172523b925",
              "IPY_MODEL_372cb35e55964d0ca9de26fd6a48feb5",
              "IPY_MODEL_682234fd273142b8beac59e4d5f8d754"
            ],
            "layout": "IPY_MODEL_19a2ca48fd8346c0998770d3abb6ad93"
          }
        },
        "c7d9b4d0cdfe411e9ca479172523b925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fbd2a57f0784d77aa5a347c45804b1d",
            "placeholder": "​",
            "style": "IPY_MODEL_4bb506592c58407db899d686a69a5fd5",
            "value": "Batches: 100%"
          }
        },
        "372cb35e55964d0ca9de26fd6a48feb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b66d58384254d7483549403ec5b4c5c",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1aa54fbde5c482798dfa3e3dae56035",
            "value": 64
          }
        },
        "682234fd273142b8beac59e4d5f8d754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8afba8c903e469d9dcb141585faefc9",
            "placeholder": "​",
            "style": "IPY_MODEL_8ef4ca64de484eb3b486a0fe7f48fe2f",
            "value": " 64/64 [00:00&lt;00:00, 140.12it/s]"
          }
        },
        "19a2ca48fd8346c0998770d3abb6ad93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fbd2a57f0784d77aa5a347c45804b1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bb506592c58407db899d686a69a5fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b66d58384254d7483549403ec5b4c5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1aa54fbde5c482798dfa3e3dae56035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8afba8c903e469d9dcb141585faefc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef4ca64de484eb3b486a0fe7f48fe2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f7589fe642b48c7962c76a5e8bbfbb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e37b229cf6046d9889e241af0242195",
              "IPY_MODEL_dceedd7187fe4ec3b890381e13417a2d",
              "IPY_MODEL_5b11d876d04248d5b14796c632efc4fe"
            ],
            "layout": "IPY_MODEL_7038ebd521a84a90a1974c4076ec9dea"
          }
        },
        "4e37b229cf6046d9889e241af0242195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d1e2e13ae34bc2938187641f70d9d0",
            "placeholder": "​",
            "style": "IPY_MODEL_ec30d0eef5fe444b951a70404ab481f3",
            "value": "Batches: 100%"
          }
        },
        "dceedd7187fe4ec3b890381e13417a2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1cb164be7ae4bbab5e2c1730ff37f9f",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89b415821880492aa10b865324b57176",
            "value": 16
          }
        },
        "5b11d876d04248d5b14796c632efc4fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d20279d7a92f4d2595b449d8aee8db44",
            "placeholder": "​",
            "style": "IPY_MODEL_12a8d3b693264125b40d5e654b6a614c",
            "value": " 16/16 [00:00&lt;00:00, 116.73it/s]"
          }
        },
        "7038ebd521a84a90a1974c4076ec9dea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84d1e2e13ae34bc2938187641f70d9d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec30d0eef5fe444b951a70404ab481f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1cb164be7ae4bbab5e2c1730ff37f9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89b415821880492aa10b865324b57176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d20279d7a92f4d2595b449d8aee8db44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12a8d3b693264125b40d5e654b6a614c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaecadd46c694a6395597b2fda3e1c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_390b5a305b084450b2a5501a747df4d3",
              "IPY_MODEL_9da04535614f41f592b9688337e9c83c",
              "IPY_MODEL_a62baa279f6f4a73a6d09ccfd7f36aba"
            ],
            "layout": "IPY_MODEL_300706aae8a146ae84513f4c58f6bc82"
          }
        },
        "390b5a305b084450b2a5501a747df4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11cea07d13474d6e858b230fc8b108f5",
            "placeholder": "​",
            "style": "IPY_MODEL_18d0fb68909d444d969cba2626b3e03d",
            "value": "Batches: 100%"
          }
        },
        "9da04535614f41f592b9688337e9c83c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4add0112152348ff8e632873d58dd546",
            "max": 1609,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fccead6c599e4ed58f7c77aaf0ce22dd",
            "value": 1609
          }
        },
        "a62baa279f6f4a73a6d09ccfd7f36aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e799ad1586e947f9bc3d11803b6c57b7",
            "placeholder": "​",
            "style": "IPY_MODEL_bf93bf1888f943278afb0f708a54ff23",
            "value": " 1609/1609 [00:12&lt;00:00, 164.18it/s]"
          }
        },
        "300706aae8a146ae84513f4c58f6bc82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11cea07d13474d6e858b230fc8b108f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d0fb68909d444d969cba2626b3e03d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4add0112152348ff8e632873d58dd546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fccead6c599e4ed58f7c77aaf0ce22dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e799ad1586e947f9bc3d11803b6c57b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf93bf1888f943278afb0f708a54ff23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
