{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadipatodia/Text-Classification_Using_AI/blob/main/TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QSF9MRBmIUun"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect\n",
        "!pip install --upgrade sentence-transformers transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNSUPERVISED MODEL WITH TIME COLUMNS**"
      ],
      "metadata": {
        "id": "cRasd_M9gvvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yNKhhC7hI6d",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "from langdetect import detect, DetectorFactory\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import cudf\n",
        "from cuml.cluster import KMeans\n",
        "import google.generativeai as genai\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Set Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBHwfAgTs-RzC7uF4QzUSA30_HfMR9MwZQ\"\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "except KeyError:\n",
        "    print(\" ERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "    print(\"Set it in PowerShell: $env:GEMINI_API_KEY = 'your_api_key_here'\")\n",
        "    exit()\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# Download NLTK stopwords\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Define stopwords These words are considered less meaningful for categorization and are typically removed or ignored during text analysis.\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'area', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it', 'its', 'her', 'their', 'our',\n",
        "    'what', 'where', 'how', 'why', 'who', 'whom', 'which', 'whether',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hr', 'hrs', 'hour', 'hours', 'time', 'date', 'week', 'month', 'year', 'ago',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'code', 'id', 'location', 'address', 'phone', 'mobile', 'call', 'report', 'registered',\n",
        "    'ok', 'yes', 'no', 'not', 'hi', 'hello', 'sir', 'madam', 'pls', 'please', 'regards', 'type', 'urban', 'complaint', 'detail', 'general',\n",
        "    'kv', 'tf', 'na', 'service', 'request', 'feedback', 'query', 'regarding', 'about', 'given', 'areas', 'village'\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Removes extra whitespace and standardizes text (generic version).\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', str(text).strip()).lower()\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def vectorized_time_categorization(df, remark_col, duration_col):\n",
        "\n",
        "    # Optimized time categorization using pandas str.extract.\n",
        "\n",
        "    print(\"     [Preprocessing] Vectorized time categorization...\")\n",
        "\n",
        "    # Applies regular expressions (hour_pattern, day_pattern) across the entire Series to extract numerical values associated with time units\n",
        "    # (e.g., (\\d+\\.?\\d*) captures numbers, (?:hr|hrs|hour|hours|h) matches various hour terms non-capturingly).\n",
        "    # This avoids slow row-by-row Python loops.\n",
        "\n",
        "    start_time = time.time()\n",
        "    duration_series = df[duration_col] if duration_col in df.columns else df[remark_col]\n",
        "    duration_series = duration_series.fillna(\"\").str.lower().apply(clean_text)\n",
        "    hour_pattern = r'(\\d+\\.?\\d*)\\s*(?:hr|hrs|hour|hours|h)'\n",
        "    day_pattern = r'(\\d+\\.?\\d*)\\s*(?:day|days)'\n",
        "\n",
        "    # Extract hours and days\n",
        "    hours_extracted = duration_series.str.extract(hour_pattern)\n",
        "    days_extracted = duration_series.str.extract(day_pattern)\n",
        "\n",
        "    # Initialize categories and hours\n",
        "    categories = pd.Series([\"No Time Specified\"] * len(duration_series), index=df.index)\n",
        "    extracted_hours = pd.Series([None] * len(duration_series), index=df.index, dtype=float)\n",
        "\n",
        "    # Process hours\n",
        "    # Boolean Masking : for efficient conditional assignment of categories and extracted numerical hours\n",
        "    hour_mask = hours_extracted[0].notna()\n",
        "    hours = hours_extracted[0].astype(float)\n",
        "    extracted_hours[hour_mask] = hours[hour_mask]\n",
        "    categories[hour_mask & (hours < 4)] = \"Less than 4 hours\"\n",
        "    categories[hour_mask & (hours >= 4) & (hours < 12)] = \"More than 4 hours\"\n",
        "    categories[hour_mask & (hours >= 12) & (hours < 24)] = \"More than 12 hours\"\n",
        "    categories[hour_mask & (hours >= 24)] = \"More than 24 hours\"\n",
        "\n",
        "    # Process days (where hours not already set)\n",
        "    day_mask = days_extracted[0].notna() & hours_extracted[0].isna()\n",
        "    days = days_extracted[0].astype(float)\n",
        "    hours_from_days = days * 24\n",
        "    extracted_hours[day_mask] = hours_from_days[day_mask]\n",
        "    categories[day_mask & (hours_from_days < 4)] = \"Less than 4 hours\"\n",
        "    categories[day_mask & (hours_from_days >= 4) & (hours_from_days < 12)] = \"More than 4 hours\"\n",
        "    categories[day_mask & (hours_from_days >= 12) & (hours_from_days < 24)] = \"More than 12 hours\"\n",
        "    categories[day_mask & (hours_from_days >= 24)] = \"More than 24 hours\"\n",
        "\n",
        "    print(f\"     [Preprocessing] Completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return categories, extracted_hours\n",
        "\n",
        "\n",
        "\n",
        "def get_top_keywords(remarks: list[str], n_keywords: int = 10) -> list[str]:\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts top keywords using TF-IDF.\n",
        "    A statistical measure that evaluates how relevant a word is to a document in a collection.\n",
        "    It assigns a higher score to words that appear frequently in a specific document but rarely in the overall corpus\n",
        "    (after removing common words like stopwords).\n",
        "    \"\"\"\n",
        "\n",
        "    if not remarks or len(remarks) < 2:\n",
        "        return []\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            stop_words=list(UNIVERSAL_STOPWORDS), ngram_range=(1, 3), min_df=5, max_features=1000\n",
        "        # ngram_range=(1, 3) : Considers unigrams (single words), bigrams (two-word phrases), and trigrams to capture more contextual meaning\n",
        "        # min_df=5: Ignores terms that appear in fewer than 5 documents, helping to filter out rare or noisy terms\n",
        "        # max_features=1000: Limits the total number of unique keywords considered, reducing dimensionality.\n",
        "        )\n",
        "        tfidf_matrix = vectorizer.fit_transform([r for r in remarks if r][:1000])\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()  # used as a proxy for the overall importance of each term in the collection\n",
        "        top_indices = scores.argsort()[-n_keywords:][::-1]\n",
        "        return [feature_names[i] for i in top_indices]\n",
        "    except ValueError as e:\n",
        "        print(f\"   [Warning] TF-IDF failed: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "def get_genai_cluster_name(cluster_texts: list[str], top_keywords: list[str]) -> str:\n",
        "    \"\"\"Generates a category name using Gemini API.\"\"\"\n",
        "    print(\"    [Gen AI Naming] Sending prompt to Gemini model...\")\n",
        "    if not cluster_texts:\n",
        "        return \"Uncategorized Remarks\"\n",
        "\n",
        "    sample_size = min(20, len(cluster_texts))\n",
        "    text_sample = \"\\n\".join([t[:100] for t in cluster_texts[:sample_size] if t])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert at analyzing customer feedback in the energy sector. Provide a single, concise, professional category name for a group of similar remarks. The name must be 4-7 words, reflect the primary issue accurately, and avoid generic terms like 'Issues', 'Problems', or 'Reports' unless critical. Do not overlap with time-based categories (e.g., 'Less than 4 hours').\n",
        "\n",
        "    Top keywords: {', '.join(top_keywords[:5])}.\n",
        "    Sample remarks:\n",
        "    {text_sample}\n",
        "\n",
        "    Category name:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        name = response.text.strip().split(\"Category name:\")[-1].strip() if \"Category name:\" in response.text else response.text.strip()\n",
        "        if not name or len(name.split()) < 4 or len(name.split()) > 7 or any(t.lower() in name.lower() for t in ['less', 'more', 'hours', 'time']):\n",
        "            name = f\"{top_keywords[0].replace('_', ' ').title()} Incident Category\" if top_keywords else \"Uncategorized Remarks\"\n",
        "        return name[:50].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"    [Gen AI Naming] ERROR: API call failed. Falling back to keywords. Error: {e}\")\n",
        "        return f\"{top_keywords[0].replace('_', ' ').title()} Incident Category\"[:50].strip() if top_keywords else \"Uncategorized Remarks\"\n",
        "\n",
        "\n",
        "\n",
        "def get_unique_name(base_name: str, existing_names: set, suffix_identifier: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Generates a unique name.\n",
        "    It iteratively appends alphabetical (A, B, C...) or alphanumeric (A1, A2...) suffixes to the base_name until a unique name is found that\n",
        "    does not exist in the existing_names set. re.sub is used for initial cleaning of the base name\n",
        "\n",
        "    \"\"\"\n",
        "    name = re.sub(r'[^a-zA-Z\\s]', '', base_name).strip()\n",
        "    name = re.sub(r'\\s+', ' ', name).strip()\n",
        "    if not name:\n",
        "        name = \"Generic Category\"\n",
        "    original_base = name\n",
        "    alpha_suffix_idx = 0\n",
        "    numeric_suffix_idx = 0\n",
        "    while name.lower() in existing_names:\n",
        "        if alpha_suffix_idx < 26:\n",
        "            name = f\"{original_base} {chr(65 + alpha_suffix_idx)}\"\n",
        "            alpha_suffix_idx += 1\n",
        "        else:\n",
        "            numeric_suffix_idx += 1\n",
        "            alpha_suffix_idx_for_num = (alpha_suffix_idx - 26) % 26\n",
        "            name = f\"{original_base} {chr(65 + alpha_suffix_idx_for_num)}{numeric_suffix_idx}\"\n",
        "            alpha_suffix_idx += 1\n",
        "    return name[:50].strip()\n",
        "\n",
        "\n",
        "\n",
        "def is_semantically_similar(name1: str, name2: str) -> bool:\n",
        "    \"\"\"Uses Gemini to check if two column names are semantically similar.\"\"\"\n",
        "    print(f\"   [Gen AI Merging] Checking similarity between '{name1}' and '{name2}'...\")\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert at analyzing customer feedback in the energy sector. Determine if the following two category names are synonyms or convey the same meaning. Answer with a single word: \"YES\" or \"NO\".\n",
        "\n",
        "    Category 1: \"{name1}\"\n",
        "    Category 2: \"{name2}\"\n",
        "\n",
        "    Recommendation:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        recommendation = response.text.strip().split(\"Recommendation:\")[-1].strip() if \"Recommendation:\" in response.text else response.text.strip()\n",
        "        return recommendation.lower() == \"yes\"\n",
        "    except Exception as e:\n",
        "        print(f\"    [Gen AI Merging] ERROR: API call failed. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def merge_similar_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Merges non-time columns with semantically similar names\"\"\"\n",
        "    print(\"\\n--- Merging similar non-time columns (Semantic Match) ---\")\n",
        "    time_columns = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"No Time Specified\"\n",
        "    ]\n",
        "    columns_to_process = [col for col in df.columns if col not in time_columns]\n",
        "    merged_mapping = {}\n",
        "\n",
        "    max_non_time_columns_target = 4\n",
        "\n",
        "    did_merge = True # loop allows for multiple rounds of merging until no more similar pairs are found or the target column count is reached.\n",
        "    while did_merge and len(set(columns_to_process) - set(merged_mapping.keys())) > max_non_time_columns_target:\n",
        "        did_merge = False\n",
        "        current_active_cols = sorted([col for col in columns_to_process if col not in merged_mapping])\n",
        "        # Sorting ensures a consistent order of comparison\n",
        "\n",
        "        for i in range(len(current_active_cols)):\n",
        "            col1 = current_active_cols[i]\n",
        "            if col1 in merged_mapping:                # This means col1 has already been chosen as a source in this iteration\n",
        "                continue\n",
        "\n",
        "            for j in range(i + 1, len(current_active_cols)):\n",
        "                col2 = current_active_cols[j]\n",
        "                if col2 in merged_mapping:            # If col2 has already been chosen as a source in this iteration\n",
        "                    continue\n",
        "\n",
        "                if is_semantically_similar(col1, col2):  # The AI call for similarity assessment.\n",
        "                    current_unmerged_count = len(set(columns_to_process) - set(merged_mapping.keys()))\n",
        "                    if current_unmerged_count > max_non_time_columns_target:\n",
        "                        print(f\"    Merging '{col2}' into '{col1}' (Semantic Match)\")\n",
        "                        merged_mapping[col2] = col1\n",
        "                        did_merge = True\n",
        "                        break                          # Found a merge, break inner loop to re-evaluate current_active_cols\n",
        "            if did_merge:                              # If a merge happened in inner loop, break outer loop to restart while loop\n",
        "                break\n",
        "\n",
        "    temp_df = df.copy()\n",
        "    for source_col, target_col in merged_mapping.items():         # merged_mapping: A dictionary stores source_column_name: target_column_name pairs.\n",
        "        if target_col not in temp_df.columns:\n",
        "            temp_df[target_col] = np.nan\n",
        "        temp_df.loc[:, target_col] = temp_df[target_col].fillna(temp_df[source_col])  # This is a key pandas operation, it takes all non-null\n",
        "                                                                                      # values from source_col and fills corresponding NaN (missing)\n",
        "                                                                                      # spots in target_col. This effectively moves remarks without\n",
        "                                                                                      # overwriting existing data in the target.\n",
        "        temp_df = temp_df.drop(columns=[source_col])               # Removes the source column after its data has been transferred.\n",
        "\n",
        "    final_columns = []\n",
        "    for col in df.columns:\n",
        "        if col in time_columns:\n",
        "            final_columns.append(col)\n",
        "        elif col not in merged_mapping.keys():                         # If it's a non-time column and not a source of a merge\n",
        "            if col not in merged_mapping.values():                     # Ensure it's not a target that was just created\n",
        "                final_columns.append(col)\n",
        "\n",
        "    for target_col in set(merged_mapping.values()):\n",
        "        if target_col not in final_columns:\n",
        "            final_columns.append(target_col)\n",
        "\n",
        "    final_column_order = sorted(final_columns, key=lambda x: (x not in time_columns, x))\n",
        "    final_column_order = [col for col in final_column_order if col in temp_df.columns]\n",
        "\n",
        "    df_merged = temp_df[final_column_order]\n",
        "    print(\"    Merging complete.\")\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "\n",
        "def load_excel_file(file_path: str, column: str) -> tuple[list[str], pd.DataFrame]:\n",
        "    \"\"\"Loads remarks from an Excel file.\"\"\"\n",
        "    print(f\"Loading data from '{file_path}'...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, usecols=[column, \"From When Issue Is Coming\"] if \"From When Issue Is Coming\" in pd.read_excel(file_path, nrows=1).columns else [column])\n",
        "        print(f\"Loaded {len(df)} rows in {time.time() - start_time:.2f} seconds.\")\n",
        "        remarks_list = [str(r) for r in df[column] if not pd.isna(r)]\n",
        "        print(f\"Extracted {len(remarks_list)} valid remarks from column '{column}'.\")\n",
        "        return remarks_list, df\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\" ERROR: File '{file_path}' not found. {e}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\" ERROR: Failed to load Excel file. {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "def save_results(df: pd.DataFrame, output_path: str):\n",
        "    \"\"\"Saves results to an Excel file.\"\"\"\n",
        "    print(f\"\\nSaving results to '{output_path}'...\")\n",
        "    start_time = time.time()\n",
        "    df.to_excel(output_path, index=False)        # saves the DataFrame to an Excel file without writing the pandas internal index as a column.\n",
        "    print(f\"Saved successfully in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "\n",
        "def segregate_remarks_by_language(raw_remarks: list[str], min_text_for_detection: int = 10) -> tuple[list[tuple[int, str]], list[tuple[int, str]]]:\n",
        "    \"\"\"Segregates remarks into English and other languages.\"\"\"\n",
        "    print(f\"Starting language segregation for {len(raw_remarks)} remarks...\")\n",
        "    start_time = time.time()\n",
        "    def detect_lang(i, remark):\n",
        "        cleaned_remark = clean_text(remark.lower())\n",
        "        if len(cleaned_remark) < min_text_for_detection or not any(char.isalpha() for char in cleaned_remark):\n",
        "            return i, remark, False\n",
        "        try:\n",
        "            return i, remark, detect(cleaned_remark) == 'en'\n",
        "        except Exception:\n",
        "            return i, remark, False\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(delayed(detect_lang)(i, r) for i, r in enumerate(raw_remarks))\n",
        "    english_remarks_with_indices = [(i, r) for i, r, is_en in results if is_en]\n",
        "    other_remarks_with_indices = [(i, r) for i, r, is_en in results if not is_en]\n",
        "    print(f\"Segregation complete in {time.time() - start_time:.2f} seconds. English: {len(english_remarks_with_indices)}, Other: {len(other_remarks_with_indices)}\")\n",
        "    return english_remarks_with_indices, other_remarks_with_indices\n",
        "\n",
        "\n",
        "\n",
        "def cluster_remarks(remarks: list[str], n_clusters: int = 10, batch_size: int = 512) -> list[int]:\n",
        "    \"\"\"\n",
        "    Clusters remarks using sentence transformers and cuML KMeans.\n",
        "    Loads a pre-trained Transformer model. This model converts full sentences into high-dimensional numerical vectors (embeddings).\n",
        "    The key idea is that sentences with similar meanings will have embeddings that are numerically \"close\" to each other in this vector space.\n",
        "    use an \"attention mechanism\" to weigh the importance of different words in a sentence relative to each other. This allows them to capture\n",
        "    complex contextual relationships and produce high-quality semantic representations for entire sentences.\n",
        "\n",
        "    \"\"\"\n",
        "    if not remarks:\n",
        "        return []\n",
        "    print(\"     [Clustering] Encoding remarks with sentence-transformers...\")\n",
        "    start_time = time.time()\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "    embeddings = []\n",
        "    for i in range(0, len(remarks), batch_size):\n",
        "        batch = remarks[i:i + batch_size]\n",
        "        batch_embeddings = model.encode(batch, batch_size=batch_size, show_progress_bar=False, convert_to_numpy=True)\n",
        "        embeddings.append(batch_embeddings)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    print(f\"     [Clustering] Encoding completed in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    K-Means is an unsupervised clustering algorithm that aims to partition n observations into k clusters. It works by iteratively assigning each\n",
        "    data point to the closest cluster centroid and then re-calculating the centroids as the mean of the points in the cluster\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"     [Clustering] Performing KMeans clustering with cuML...\")\n",
        "    start_time = time.time()\n",
        "    gdf = cudf.DataFrame(embeddings)\n",
        "    clustering = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    \"\"\"\n",
        "    n_clusters: The desired number of clusters.\n",
        "    random_state: Ensures the centroid initialization is reproducible.\n",
        "    n_init=10: Runs the algorithm 10 times with different centroid initializations and picks the best result\n",
        "    (minimizing inertia/sum of squared distances)\n",
        "    \"\"\"\n",
        "\n",
        "    cluster_labels = clustering.fit_predict(gdf).to_numpy() # Performs the clustering on the GPU (gdf) and returns the cluster assignments for\n",
        "                                                            # each remark as a NumPy array.\n",
        "    print(f\"     [Clustering] Clustering completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return cluster_labels\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    excel_file_path = \"./Supply.xlsx\"\n",
        "    text_column_name = \"REMARKS\"\n",
        "    duration_column_name = \"From When Issue Is Coming\"\n",
        "    output_excel_path = \"./categorized_remarks_01.xlsx\"\n",
        "    max_remark_clusters_limit = 8                    # This limits initial number of clusters for English remarks\n",
        "    batch_size = 500\n",
        "\n",
        "    time_columns = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"No Time Specified\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Remark Categorization Script ---\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        raw_remarks_list, df = load_excel_file(excel_file_path, text_column_name)\n",
        "\n",
        "        print(f\"\\n--- Categorizing remarks ---\")\n",
        "        time_categorized_remarks = {col: [] for col in time_columns}\n",
        "        non_time_remarks_with_indices = []\n",
        "\n",
        "        categories, _ = vectorized_time_categorization(df, text_column_name, duration_column_name)\n",
        "        # Handle cases where `categories` might not be directly iterable if df is empty etc.\n",
        "        # Ensure `zip` handles potential length mismatch safely if remarks are cleaned/filtered.\n",
        "        for i, (remark, category) in enumerate(zip(raw_remarks_list, categories)):\n",
        "            if category != \"No Time Specified\":\n",
        "                time_categorized_remarks[category].append((i, remark))\n",
        "            else:\n",
        "                non_time_remarks_with_indices.append((i, remark))\n",
        "\n",
        "        print(f\"Time-based categorization complete. Counts: { {k: len(v) for k, v in time_categorized_remarks.items()} }\")\n",
        "        print(f\"Non-time remarks for further processing: {len(non_time_remarks_with_indices)}\")\n",
        "\n",
        "        print(\"\\n--- Segregating non-time remarks by language ---\")\n",
        "        english_remarks_with_indices, other_remarks_with_indices = segregate_remarks_by_language(  # to check if non time remarks are english or not\n",
        "            [r for _, r in non_time_remarks_with_indices]\n",
        "        )\n",
        "        # Re-map indices to original dataframe index\n",
        "        english_remarks_with_indices_original = [(non_time_remarks_with_indices[local_idx][0], r) for local_idx, r in english_remarks_with_indices]\n",
        "        other_remarks_with_indices_original = [(non_time_remarks_with_indices[local_idx][0], r) for local_idx, r in other_remarks_with_indices]\n",
        "\n",
        "\n",
        "        print(f\"Non-time English remarks: {len(english_remarks_with_indices_original)}\")\n",
        "        print(f\"Non-time other language remarks: {len(other_remarks_with_indices_original)}\")\n",
        "\n",
        "        # Initialize final_wide_data_columns with time-based categories\n",
        "        final_wide_data_columns = {k: [r for _, r in v] for k, v in time_categorized_remarks.items()}\n",
        "\n",
        "        # original_indexed_cluster_labels is used to map back cluster labels to original df rows\n",
        "        original_indexed_cluster_labels = np.full(len(raw_remarks_list), -2, dtype=int) # -2 for unclustered non-time\n",
        "\n",
        "        # final_column_name_map maps cluster IDs to generated category names\n",
        "        final_column_name_map = {}\n",
        "\n",
        "        if english_remarks_with_indices_original:\n",
        "            print(\"\\n--- Processing non-time English remarks for clustering ---\")\n",
        "            english_remark_texts = [r for _, r in english_remarks_with_indices_original]\n",
        "            english_remark_original_indices = [i for i, _ in english_remarks_with_indices_original]\n",
        "\n",
        "            # Determine number of clusters for KMeans, capping at max_remark_clusters_limit\n",
        "            n_clusters_for_kmeans = min(max_remark_clusters_limit, len(english_remark_texts))\n",
        "            if n_clusters_for_kmeans > 0: # Ensure we don't try to cluster with 0 clusters\n",
        "                cluster_labels = cluster_remarks(english_remark_texts, n_clusters_for_kmeans, batch_size)\n",
        "                print(f\"    [Clustering] Found {len(set(cluster_labels))} initial clusters.\")\n",
        "\n",
        "                # Apply cluster labels back to original remark indices\n",
        "                for i, clustered_label in enumerate(cluster_labels):\n",
        "                    original_indexed_cluster_labels[english_remark_original_indices[i]] = clustered_label\n",
        "\n",
        "                # Get unique cluster IDs for naming\n",
        "                final_unique_clusters = sorted([c for c in set(cluster_labels) if c != -1]) # Exclude noise (-1 if DBSCAN was used)\n",
        "                print(f\"    [Gen AI Naming] Naming {len(final_unique_clusters)} final clusters.\")\n",
        "\n",
        "                used_final_names = set(time_columns) # Keep track of names already in use (including time categories)\n",
        "                for cluster_id in final_unique_clusters:\n",
        "                    cluster_texts_original = [english_remark_texts[j] for j, label in enumerate(cluster_labels) if label == cluster_id]\n",
        "                    top_keywords = get_top_keywords(cluster_texts_original)\n",
        "                    print(f\"    [Keywords] Top keywords for cluster {cluster_id}: {', '.join(top_keywords)}\")\n",
        "\n",
        "                    proposed_final_name = get_genai_cluster_name(cluster_texts_original, top_keywords)\n",
        "                    final_name = get_unique_name(proposed_final_name, used_final_names, str(cluster_id))\n",
        "                    final_column_name_map[cluster_id] = final_name\n",
        "                    used_final_names.add(final_name.lower())\n",
        "                    print(f\"    Final Category Name: '{final_name}'\")\n",
        "                    final_wide_data_columns[final_name] = cluster_texts_original\n",
        "            else:\n",
        "                print(\"    [Clustering] Not enough English remarks for clustering.\")\n",
        "\n",
        "        # Handle unclustered English remarks (if any)\n",
        "        uncategorized_english_remarks = [(original_idx, r) for original_idx, r in english_remarks_with_indices_original if original_indexed_cluster_labels[original_idx] == -1] # Assuming -1 for noise/unclustered\n",
        "        if uncategorized_remarks := [r for _, r in uncategorized_english_remarks]:\n",
        "            col_name = get_unique_name(\"Uncategorized English Remarks\", set(final_wide_data_columns.keys()).union(set(final_column_name_map.values())), \"uncat_en\")\n",
        "            final_wide_data_columns[col_name] = uncategorized_remarks\n",
        "            print(f\"\\nAdded column: '{col_name}' for {len(uncategorized_remarks)} remarks.\")\n",
        "\n",
        "        # Handle other language remarks\n",
        "        if other_remarks := [r for _, r in other_remarks_with_indices_original]:\n",
        "            col_name = get_unique_name(\"Other Language Remarks\", set(final_wide_data_columns.keys()).union(set(final_column_name_map.values())), \"other_lang\")\n",
        "            final_wide_data_columns[col_name] = other_remarks\n",
        "            print(f\"Added column: '{col_name}' for {len(other_remarks)} remarks.\")\n",
        "\n",
        "        # Create wide format DataFrame\n",
        "        max_rows = max(len(remarks) for remarks in final_wide_data_columns.values()) if final_wide_data_columns else 0\n",
        "        df_results_wide = pd.DataFrame({\n",
        "            col: remarks + [\"\"] * (max_rows - len(remarks))\n",
        "            for col, remarks in final_wide_data_columns.items()\n",
        "        })\n",
        "\n",
        "        print(\"\\n--- Merging non-time columns ---\")\n",
        "        non_time_columns_pre_merge = [col for col in df_results_wide.columns if col not in time_columns]\n",
        "        if len(non_time_columns_pre_merge) > 4: # Only attempt merge if there are more than 4 non-time columns\n",
        "            df_results_wide = merge_similar_columns(df_results_wide)\n",
        "        else:\n",
        "            print(f\"Skipping semantic merging. Number of non-time columns ({len(non_time_columns_pre_merge)}) is already at or below the target of 4.\")\n",
        "\n",
        "\n",
        "        print(f\"\\nCategorization complete. Column counts: { {k: len([x for x in df_results_wide[k] if x]) for k in df_results_wide.columns} }\")\n",
        "        print(\"\\n--- Validating Categories ---\")\n",
        "        for col in df_results_wide.columns:\n",
        "            print(f\"Category '{col}' ({len([x for x in df_results_wide[col] if x])} remarks):\")\n",
        "            for r in df_results_wide[col][:min(5, len(df_results_wide[col]))]:\n",
        "                if r:\n",
        "                    print(f\"   - {r[:100]}...\")\n",
        "\n",
        "        save_results(df_results_wide, output_excel_path)\n",
        "        print(\"\\n--- Sample Results ---\")\n",
        "        print(df_results_wide.head())\n",
        "        print(f\"\\n--- Script completed in {time.time() - start_time:.2f} seconds ---\")\n",
        "\n",
        "    except FileNotFoundError as fnfe:\n",
        "        print(f\"\\nERROR: File not found. Please check 'excel_file_path'. Details: {fnfe}\")\n",
        "        exit(1)\n",
        "    except KeyError as ke:\n",
        "        print(f\"\\nERROR: Column not found. Please check 'text_column_name' or 'duration_column_name'. Details: {ke}\")\n",
        "        exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNSUPERVISED MODEL WITHOUT TIME CATEGORIES**"
      ],
      "metadata": {
        "id": "GQmZjtrrD8ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "from langdetect import detect, DetectorFactory\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import cudf\n",
        "from cuml.cluster import KMeans\n",
        "import google.generativeai as genai\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import torch\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBHwfAgTs-RzC7uF4QzUSA30_HfMR9MwZQ\"\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "except KeyError:\n",
        "    print(\" ERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "    print(\"Set it in PowerShell: $env:GEMINI_API_KEY = 'your_api_key_here'\")\n",
        "    exit()\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'area', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it', 'its', 'her', 'their', 'our',\n",
        "    'what', 'where', 'how', 'why', 'who', 'whom', 'which', 'whether',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hr', 'hrs', 'hour', 'hours', 'time', 'date', 'week', 'month', 'year', 'ago',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'code', 'id', 'location', 'address', 'phone', 'mobile', 'call', 'report', 'registered',\n",
        "    'ok', 'yes', 'no', 'not', 'hi', 'hello', 'sir', 'madam', 'pls', 'please', 'regards', 'type', 'urban', 'complaint', 'detail', 'general',\n",
        "    'kv', 'tf', 'na', 'service', 'request', 'feedback', 'query', 'regarding', 'about', 'given', 'areas', 'village'\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', str(text).strip()).lower()\n",
        "    return text\n",
        "\n",
        "def get_top_keywords(remarks: list[str], n_keywords: int = 10) -> list[str]:\n",
        "\n",
        "    if not remarks or len(remarks) < 2:\n",
        "        return []\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            stop_words=list(UNIVERSAL_STOPWORDS), ngram_range=(1, 3), min_df=5, max_features=1000\n",
        "        )\n",
        "        tfidf_matrix = vectorizer.fit_transform([r for r in remarks if r][:1000])\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n",
        "        top_indices = scores.argsort()[-n_keywords:][::-1]\n",
        "        return [feature_names[i] for i in top_indices]\n",
        "    except ValueError as e:\n",
        "        print(f\"   [Warning] TF-IDF failed: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "def get_genai_cluster_name(cluster_texts: list[str], top_keywords: list[str]) -> str:\n",
        "    print(\"      [Gen AI Naming] Sending prompt to Gemini model...\")\n",
        "\n",
        "    sample_size = min(20, len(cluster_texts))\n",
        "    text_sample = \"\\n\".join([t[:100] for t in cluster_texts[:sample_size] if t])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert at analyzing customer feedback in the energy sector. Provide a single, concise, professional category name for a group of similar remarks. The name must be 4-7 words, reflect the primary issue accurately, and avoid generic terms like 'Issues', 'Problems', or 'Reports' unless critical. Do not overlap with time-based categories (e.g., 'Less than 4 hours').\n",
        "\n",
        "    Top keywords: {', '.join(top_keywords[:5])}.\n",
        "    Sample remarks:\n",
        "    {text_sample}\n",
        "\n",
        "    Category name:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        name = response.text.strip().split(\"Category name:\")[-1].strip() if \"Category name:\" in response.text else response.text.strip()\n",
        "        if not name or len(name.split()) < 4 or len(name.split()) > 7 or any(t.lower() in name.lower() for t in ['less', 'more', 'hours', 'time']):\n",
        "            name = f\"{top_keywords[0].replace('_', ' ').title()} Incident Category\" if top_keywords else \"Uncategorized Remarks\"\n",
        "        return name[:50].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"      [Gen AI Naming] ERROR: API call failed. Falling back to keywords. Error: {e}\")\n",
        "        return f\"{top_keywords[0].replace('_', ' ').title()} Incident Category\"[:50].strip() if top_keywords else \"Uncategorized Remarks\"\n",
        "\n",
        "\n",
        "\n",
        "def get_unique_name(base_name: str, existing_names: set, suffix_identifier: str = \"\") -> str:\n",
        "    name = re.sub(r'[^a-zA-Z\\s]', '', base_name).strip()\n",
        "    name = re.sub(r'\\s+', ' ', name).strip()\n",
        "    if not name:\n",
        "        name = \"Generic Category\"\n",
        "    original_base = name\n",
        "    alpha_suffix_idx = 0\n",
        "    numeric_suffix_idx = 0\n",
        "    while name.lower() in existing_names:\n",
        "        if alpha_suffix_idx < 26:\n",
        "            name = f\"{original_base} {chr(65 + alpha_suffix_idx)}\"\n",
        "            alpha_suffix_idx += 1\n",
        "        else:\n",
        "            numeric_suffix_idx += 1\n",
        "            alpha_suffix_idx_for_num = (alpha_suffix_idx - 26) % 26\n",
        "            name = f\"{original_base} {chr(65 + alpha_suffix_idx_for_num)}{numeric_suffix_idx}\"\n",
        "            alpha_suffix_idx += 1\n",
        "    return name[:50].strip()\n",
        "\n",
        "\n",
        "\n",
        "def is_semantically_similar(name1: str, name2: str) -> bool:\n",
        "    print(f\"    [Gen AI Merging] Checking similarity between '{name1}' and '{name2}'...\")\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert at analyzing customer feedback in the energy sector. Determine if the following two category names are synonyms or convey the same meaning. Answer with a single word: \"YES\" or \"NO\".\n",
        "\n",
        "    Category 1: \"{name1}\"\n",
        "    Category 2: \"{name2}\"\n",
        "\n",
        "    Recommendation:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        recommendation = response.text.strip().split(\"Recommendation:\")[-1].strip() if \"Recommendation:\" in response.text else response.text.strip()\n",
        "        return recommendation.lower() == \"yes\"\n",
        "    except Exception as e:\n",
        "        print(f\"      [Gen AI Merging] ERROR: API call failed. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def merge_similar_columns(df: pd.DataFrame, max_target_columns: int) -> pd.DataFrame:\n",
        "    print(\"\\n--- Merging similar columns (Semantic Match) ---\")\n",
        "    columns_to_process = list(df.columns)\n",
        "    merged_mapping = {}\n",
        "\n",
        "    did_merge = True\n",
        "    while did_merge and len(set(columns_to_process) - set(merged_mapping.keys())) > max_target_columns:\n",
        "        did_merge = False\n",
        "        current_active_cols = sorted([col for col in columns_to_process if col not in merged_mapping])\n",
        "\n",
        "        for i in range(len(current_active_cols)):\n",
        "            col1 = current_active_cols[i]\n",
        "            if col1 in merged_mapping:\n",
        "                continue\n",
        "\n",
        "            for j in range(i + 1, len(current_active_cols)):\n",
        "                col2 = current_active_cols[j]\n",
        "                if col2 in merged_mapping:\n",
        "                    continue\n",
        "\n",
        "                if is_semantically_similar(col1, col2):\n",
        "                    current_unmerged_count = len(set(columns_to_process) - set(merged_mapping.keys()))\n",
        "                    if current_unmerged_count > max_target_columns:\n",
        "                        print(f\"      Merging '{col2}' into '{col1}' (Semantic Match)\")\n",
        "                        merged_mapping[col2] = col1\n",
        "                        did_merge = True\n",
        "                        break\n",
        "            if did_merge:\n",
        "                break\n",
        "\n",
        "    temp_df = df.copy()\n",
        "    for source_col, target_col in merged_mapping.items():\n",
        "        if target_col not in temp_df.columns:\n",
        "            temp_df[target_col] = np.nan\n",
        "        temp_df.loc[:, target_col] = temp_df[target_col].fillna(temp_df[source_col])\n",
        "        temp_df = temp_df.drop(columns=[source_col])\n",
        "\n",
        "    final_columns = []\n",
        "    for col in df.columns:\n",
        "        if col not in merged_mapping.keys():\n",
        "            if col not in merged_mapping.values():\n",
        "                final_columns.append(col)\n",
        "\n",
        "    for target_col in set(merged_mapping.values()):\n",
        "        if target_col not in final_columns:\n",
        "            final_columns.append(target_col)\n",
        "\n",
        "    final_column_order = sorted(final_columns)\n",
        "    final_column_order = [col for col in final_column_order if col in temp_df.columns]\n",
        "\n",
        "    df_merged = temp_df[final_column_order]\n",
        "    print(\"      Merging complete.\")\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "\n",
        "def load_excel_file(file_path: str, column: str) -> tuple[list[str], pd.DataFrame]:\n",
        "    print(f\"Loading data from '{file_path}'...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, usecols=[column])\n",
        "        print(f\"Loaded {len(df)} rows in {time.time() - start_time:.2f} seconds.\")\n",
        "        remarks_list = [str(r) for r in df[column] if not pd.isna(r)]\n",
        "        print(f\"Extracted {len(remarks_list)} valid remarks from column '{column}'.\")\n",
        "        return remarks_list, df\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\" ERROR: File '{file_path}' not found. {e}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\" ERROR: Failed to load Excel file. {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "def save_results(df: pd.DataFrame, output_path: str):\n",
        "    print(f\"\\nSaving results to '{output_path}'...\")\n",
        "    start_time = time.time()\n",
        "    df.to_excel(output_path, index=False)\n",
        "    print(f\"Saved successfully in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "\n",
        "def segregate_remarks_by_language(raw_remarks: list[str], min_text_for_detection: int = 10) -> tuple[list[tuple[int, str]], list[tuple[int, str]]]:\n",
        "    print(f\"Starting language segregation for {len(raw_remarks)} remarks...\")\n",
        "    start_time = time.time()\n",
        "    def detect_lang(i, remark):\n",
        "        cleaned_remark = clean_text(remark.lower())\n",
        "        if len(cleaned_remark) < min_text_for_detection or not any(char.isalpha() for char in cleaned_remark):\n",
        "            return i, remark, False\n",
        "        try:\n",
        "            return i, remark, detect(cleaned_remark) == 'en'\n",
        "        except Exception:\n",
        "            return i, remark, False\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(delayed(detect_lang)(i, r) for i, r in enumerate(raw_remarks))\n",
        "    english_remarks_with_indices = [(i, r) for i, r, is_en in results if is_en]\n",
        "    other_remarks_with_indices = [(i, r) for i, r, is_en in results if not is_en]\n",
        "    print(f\"Segregation complete in {time.time() - start_time:.2f} seconds. English: {len(english_remarks_with_indices)}, Other: {len(other_remarks_with_indices)}\")\n",
        "    return english_remarks_with_indices, other_remarks_with_indices\n",
        "\n",
        "\n",
        "\n",
        "def cluster_remarks(remarks: list[str], n_clusters: int = 10, batch_size: int = 512) -> list[int]:\n",
        "    if not remarks:\n",
        "        return []\n",
        "    print(\"      [Clustering] Encoding remarks with sentence-transformers...\")\n",
        "    start_time = time.time()\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "    embeddings = []\n",
        "    for i in range(0, len(remarks), batch_size):\n",
        "        batch = remarks[i:i + batch_size]\n",
        "        batch_embeddings = model.encode(batch, batch_size=batch_size, show_progress_bar=False, convert_to_numpy=True)\n",
        "        embeddings.append(batch_embeddings)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    print(f\"      [Clustering] Encoding completed in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "    print(\"      [Clustering] Performing KMeans clustering with cuML...\")\n",
        "    start_time = time.time()\n",
        "    gdf = cudf.DataFrame(embeddings)\n",
        "    clustering = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "\n",
        "    cluster_labels = clustering.fit_predict(gdf).to_numpy()\n",
        "    print(f\"      [Clustering] Clustering completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return cluster_labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    excel_file_path = \"./Meter.xlsx\"\n",
        "    text_column_name = \"REMARKS\"\n",
        "    output_excel_path = \"./categorized_remarks_03.xlsx\"\n",
        "    max_remark_clusters_target = 10\n",
        "    batch_size = 500\n",
        "\n",
        "    print(\"\\n--- Starting Remark Categorization Script (No Time Categories) ---\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        raw_remarks_list, df = load_excel_file(excel_file_path, text_column_name)\n",
        "\n",
        "        print(\"\\n--- Segregating remarks by language ---\")\n",
        "        english_remarks_with_indices, other_remarks_with_indices = segregate_remarks_by_language(\n",
        "            raw_remarks_list\n",
        "        )\n",
        "\n",
        "        print(f\"English remarks for clustering: {len(english_remarks_with_indices)}\")\n",
        "        print(f\"Other language remarks: {len(other_remarks_with_indices)}\")\n",
        "\n",
        "        final_wide_data_columns = {}\n",
        "\n",
        "        original_indexed_cluster_labels = np.full(len(raw_remarks_list), -2, dtype=int)\n",
        "\n",
        "        final_column_name_map = {}\n",
        "\n",
        "        if english_remarks_with_indices:\n",
        "            print(\"\\n--- Processing English remarks for clustering ---\")\n",
        "            english_remark_texts = [r for _, r in english_remarks_with_indices]\n",
        "            english_remark_original_indices = [i for i, _ in english_remarks_with_indices]\n",
        "\n",
        "            n_clusters_initial = min(len(english_remark_texts), 20)\n",
        "\n",
        "            if n_clusters_initial > 0:\n",
        "                cluster_labels = cluster_remarks(english_remark_texts, n_clusters_initial, batch_size)\n",
        "                print(f\"      [Clustering] Found {len(set(cluster_labels))} initial clusters.\")\n",
        "\n",
        "                for i, clustered_label in enumerate(cluster_labels):\n",
        "                    original_indexed_cluster_labels[english_remark_original_indices[i]] = clustered_label\n",
        "\n",
        "                initial_unique_clusters = sorted([c for c in set(cluster_labels) if c != -1])\n",
        "                print(f\"      [Gen AI Naming] Naming {len(initial_unique_clusters)} initial clusters.\")\n",
        "\n",
        "                temp_cluster_data = defaultdict(list)\n",
        "                for i, r in english_remarks_with_indices:\n",
        "                    cluster_id = original_indexed_cluster_labels[i]\n",
        "                    if cluster_id != -1:\n",
        "                        temp_cluster_data[cluster_id].append(r)\n",
        "\n",
        "                used_initial_names = set()\n",
        "                initial_named_clusters = {}\n",
        "                for cluster_id in initial_unique_clusters:\n",
        "                    cluster_texts_original = temp_cluster_data[cluster_id]\n",
        "                    if cluster_texts_original:\n",
        "                        top_keywords = get_top_keywords(cluster_texts_original)\n",
        "                        print(f\"      [Keywords] Top keywords for cluster {cluster_id}: {', '.join(top_keywords)}\")\n",
        "\n",
        "                        proposed_name = get_genai_cluster_name(cluster_texts_original, top_keywords)\n",
        "                        unique_initial_name = get_unique_name(proposed_name, used_initial_names)\n",
        "                        initial_named_clusters[unique_initial_name] = cluster_texts_original\n",
        "                        used_initial_names.add(unique_initial_name.lower())\n",
        "                        print(f\"      Initial Category Name for cluster {cluster_id}: '{unique_initial_name}'\")\n",
        "            else:\n",
        "                print(\"      [Clustering] Not enough English remarks for clustering.\")\n",
        "                initial_named_clusters = {}\n",
        "        else:\n",
        "            initial_named_clusters = {}\n",
        "\n",
        "        if initial_named_clusters:\n",
        "            max_len = max(len(v) for v in initial_named_clusters.values())\n",
        "            df_for_merging = pd.DataFrame({\n",
        "                name: data + [''] * (max_len - len(data))\n",
        "                for name, data in initial_named_clusters.items()\n",
        "            })\n",
        "        else:\n",
        "            df_for_merging = pd.DataFrame()\n",
        "\n",
        "\n",
        "        if not df_for_merging.empty and len(df_for_merging.columns) > max_remark_clusters_target:\n",
        "            df_merged_clusters = merge_similar_columns(df_for_merging, max_remark_clusters_target)\n",
        "            final_wide_data_columns.update({col: list(df_merged_clusters[col].dropna()) for col in df_merged_clusters.columns})\n",
        "        elif not df_for_merging.empty:\n",
        "            print(f\"Skipping semantic merging. Number of clusters ({len(df_for_merging.columns)}) is already at or below the target of {max_remark_clusters_target}.\")\n",
        "            final_wide_data_columns.update({col: list(df_for_merging[col].dropna()) for col in df_for_merging.columns})\n",
        "        else:\n",
        "            print(\"No English remarks to form initial clusters.\")\n",
        "\n",
        "\n",
        "        all_categorized_english_remarks_texts = set()\n",
        "        for col in final_wide_data_columns:\n",
        "            all_categorized_english_remarks_texts.update(final_wide_data_columns[col])\n",
        "\n",
        "        uncategorized_english_remarks = [(original_idx, r) for original_idx, r in english_remarks_with_indices if r not in all_categorized_english_remarks_texts]\n",
        "\n",
        "        if uncategorized_remarks_texts := [r for _, r in uncategorized_english_remarks]:\n",
        "            col_name = get_unique_name(\"Uncategorized English Remarks\", set(final_wide_data_columns.keys()), \"uncat_en\")\n",
        "            final_wide_data_columns[col_name] = uncategorized_remarks_texts\n",
        "            print(f\"\\nAdded column: '{col_name}' for {len(uncategorized_remarks_texts)} remarks.\")\n",
        "\n",
        "\n",
        "        if other_remarks := [r for _, r in other_remarks_with_indices]:\n",
        "            col_name = get_unique_name(\"Other Language Remarks\", set(final_wide_data_columns.keys()), \"other_lang\")\n",
        "            final_wide_data_columns[col_name] = other_remarks\n",
        "            print(f\"Added column: '{col_name}' for {len(other_remarks)} remarks.\")\n",
        "\n",
        "\n",
        "        max_rows = max(len(remarks) for remarks in final_wide_data_columns.values()) if final_wide_data_columns else 0\n",
        "        df_results_wide = pd.DataFrame({\n",
        "            col: remarks + [\"\"] * (max_rows - len(remarks))\n",
        "            for col, remarks in final_wide_data_columns.items()\n",
        "        })\n",
        "\n",
        "\n",
        "        print(f\"\\nCategorization complete. Column counts: { {k: len([x for x in df_results_wide[k] if x]) for k in df_results_wide.columns} }\")\n",
        "        print(\"\\n--- Validating Categories ---\")\n",
        "        for col in df_results_wide.columns:\n",
        "            print(f\"Category '{col}' ({len([x for x in df_results_wide[col] if x])} remarks):\")\n",
        "            for r in df_results_wide[col][:min(5, len(df_results_wide[col]))]:\n",
        "                if r:\n",
        "                    print(f\"    - {r[:100]}...\")\n",
        "\n",
        "        save_results(df_results_wide, output_excel_path)\n",
        "        print(\"\\n--- Sample Results ---\")\n",
        "        print(df_results_wide.head())\n",
        "        print(f\"\\n--- Script completed in {time.time() - start_time:.2f} seconds ---\")\n",
        "\n",
        "    except FileNotFoundError as fnfe:\n",
        "        print(f\"\\nERROR: File not found. Please check 'excel_file_path'. Details: {fnfe}\")\n",
        "        exit(1)\n",
        "    except KeyError as ke:\n",
        "        print(f\"\\nERROR: Column not found. Please check 'text_column_name'. Details: {ke}\")\n",
        "        exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qRB7_gh7ix9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING SUPERVISED MODEL**"
      ],
      "metadata": {
        "id": "Nc5G-x8XEIL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK stopwords are downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Define stopwords (reduced to retain meaningful words)\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hour', 'hours', 'time',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'pls', 'please'\n",
        "])\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Removes extra whitespace and standardizes text.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"-/]', '', str(text).strip()).lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Load the Excel file\n",
        "excel_file_path = \"./Book1.xlsx\"\n",
        "\n",
        "try:\n",
        "    df_raw_labeled = pd.read_excel(excel_file_path, header=0)\n",
        "    print(f\"Successfully loaded '{excel_file_path}'. Shape: {df_raw_labeled.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the raw data:\")\n",
        "    print(df_raw_labeled.head())\n",
        "    print(\"\\nColumns identified from the raw data:\")\n",
        "    print(df_raw_labeled.columns.tolist())\n",
        "\n",
        "    # Define categories, including \"None\"\n",
        "    desired_category_labels = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"More than 4 hours\",\n",
        "        \"Consumer Power Supply Failures\",\n",
        "        \"Failed Pole Incident Category\",\n",
        "        \"Other Language Remarks\",\n",
        "        \"Partial Phase Supply Failure\",\n",
        "        \"Transformer Damage Causing Outages\",\n",
        "        \"None\"  # Added \"None\" for uncategorized remarks\n",
        "    ]\n",
        "\n",
        "    loaded_to_label_map = {}\n",
        "    seen_labels = {}\n",
        "\n",
        "    for col in df_raw_labeled.columns:\n",
        "        normalized_col = col.split('.')[0]\n",
        "        if normalized_col in desired_category_labels:\n",
        "            if normalized_col in seen_labels:\n",
        "                loaded_to_label_map[col] = normalized_col\n",
        "            else:\n",
        "                loaded_to_label_map[col] = col\n",
        "                seen_labels[normalized_col] = True\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' is not in desired categories. Ignored for training.\")\n",
        "\n",
        "    active_category_columns_in_df = list(loaded_to_label_map.keys())\n",
        "    print(f\"\\nIdentified {len(set(loaded_to_label_map.values()))} Unique Category Labels for Training:\")\n",
        "    print(list(set(loaded_to_label_map.values())))\n",
        "    print(f\"\\nColumns from Excel file that will be processed:\")\n",
        "    print(active_category_columns_in_df)\n",
        "\n",
        "    # Transform to long format\n",
        "    labeled_data_for_training = []\n",
        "    for category_col_in_df in active_category_columns_in_df:\n",
        "        standard_category_label = loaded_to_label_map[category_col_in_df]\n",
        "        for remark_entry in df_raw_labeled[category_col_in_df].dropna():\n",
        "            cleaned_remark = clean_text(str(remark_entry))\n",
        "            if cleaned_remark:\n",
        "                labeled_data_for_training.append({\n",
        "                    'remark_original': str(remark_entry),\n",
        "                    'remark_cleaned': cleaned_remark,\n",
        "                    'category': standard_category_label\n",
        "                })\n",
        "\n",
        "    df_labeled_for_training = pd.DataFrame(labeled_data_for_training)\n",
        "    print(f\"\\nTransformed data for training shape: {df_labeled_for_training.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the transformed labeled data:\")\n",
        "    print(df_labeled_for_training.head())\n",
        "    print(\"\\nValue counts for 'category':\")\n",
        "    print(df_labeled_for_training['category'].value_counts())\n",
        "    print(f\"\\nTotal unique categories: {df_labeled_for_training['category'].nunique()}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{excel_file_path}' was not found.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    exit()\n",
        "\n",
        "# Store for next steps\n",
        "global_df_labeled_for_training = df_labeled_for_training\n",
        "global_original_category_column_names = desired_category_labels"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1Mv0qdrvYHCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Ensure global_df_labeled_for_training is available\n",
        "if 'global_df_labeled_for_training' not in globals():\n",
        "    print(\"Error: global_df_labeled_for_training not found. Run Step 1 first.\")\n",
        "    exit()\n",
        "\n",
        "df_training = global_df_labeled_for_training.copy()\n",
        "\n",
        "print(\"\\n--- Step 2: Feature Extraction (Sentence Embeddings + Time Features) and Model Training ---\")\n",
        "\n",
        "# Extract time features\n",
        "def extract_time_features(remarks_series: pd.Series) -> pd.DataFrame:\n",
        "    remarks_series = remarks_series.fillna(\"\").str.lower()\n",
        "    hour_pattern = r'(\\d+\\.?\\d*)\\s*(?:hr|hrs|hour|hours|h)'\n",
        "    day_pattern = r'(\\d+\\.?\\d*)\\s*(?:day|days)'\n",
        "    am_pm_pattern = r'\\b(\\d{1,2}(:\\d{2})?)\\s*(am|pm)\\b'\n",
        "\n",
        "    hours_extracted = remarks_series.str.extract(hour_pattern)[0].astype(float).fillna(0)\n",
        "    days_extracted = remarks_series.str.extract(day_pattern)[0].astype(float).fillna(0)\n",
        "    hours_from_days = days_extracted * 24\n",
        "    combined_hours = hours_extracted + hours_from_days\n",
        "    is_am_pm_mentioned = remarks_series.str.contains(am_pm_pattern, regex=True).astype(int)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'extracted_hours': combined_hours,\n",
        "        'is_am_pm_mentioned': is_am_pm_mentioned\n",
        "    })\n",
        "\n",
        "# Split data\n",
        "X = df_training['remark_cleaned'].tolist()\n",
        "y = df_training['category'].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(f\"Dataset split: Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
        "print(f\"Unique categories in training set: {len(set(y_train))}\")\n",
        "print(f\"Unique categories in testing set: {len(set(y_test))}\")\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"Loading SentenceTransformer model...\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_embedding = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "print(f\"SentenceTransformer model loaded on device: {device}\")\n",
        "\n",
        "print(\"Generating embeddings for training data...\")\n",
        "start_time = time.time()\n",
        "X_train_embeddings = model_embedding.encode(X_train, show_progress_bar=True, convert_to_numpy=True)\n",
        "print(f\"Training embeddings generated in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "print(\"Generating embeddings for testing data...\")\n",
        "start_time = time.time()\n",
        "X_test_embeddings = model_embedding.encode(X_test, show_progress_bar=True, convert_to_numpy=True)\n",
        "print(f\"Testing embeddings generated in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "# Extract and scale time features\n",
        "scaler = StandardScaler()\n",
        "X_train_time_features_df = extract_time_features(pd.Series(X_train))\n",
        "X_train_time_features_scaled = scaler.fit_transform(X_train_time_features_df)\n",
        "X_test_time_features_df = extract_time_features(pd.Series(X_test))\n",
        "X_test_time_features_scaled = scaler.transform(X_test_time_features_df)\n",
        "\n",
        "# Combine features\n",
        "X_train_combined = np.hstack((X_train_embeddings, X_train_time_features_scaled))\n",
        "X_test_combined = np.hstack((X_test_embeddings, X_test_time_features_scaled))\n",
        "print(f\"Shape of training combined features: {X_train_combined.shape}\")\n",
        "print(f\"Shape of testing combined features: {X_test_combined.shape}\")\n",
        "\n",
        "# Train Logistic Regression\n",
        "print(\"\\nTraining Logistic Regression model...\")\n",
        "start_time = time.time()\n",
        "classifier_model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    solver='lbfgs',\n",
        "    multi_class='auto',\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "classifier_model.fit(X_train_combined, y_train)\n",
        "print(f\"Logistic Regression model trained in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "y_pred = classifier_model.predict(X_test_combined)\n",
        "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "# Save models\n",
        "print(\"\\nSaving models...\")\n",
        "joblib.dump(model_embedding, 'sentence_transformer_model.pkl')\n",
        "joblib.dump(classifier_model, 'logistic_regression_classifier.pkl')\n",
        "joblib.dump(scaler, 'scaler_for_time_features.pkl')\n",
        "print(\"Models saved successfully.\")\n",
        "\n",
        "# Store for inference\n",
        "global_model_embedding = model_embedding\n",
        "global_classifier_model = classifier_model\n",
        "global_scaler = scaler\n",
        "global_y_test = y_test\n",
        "global_y_pred = y_pred\n",
        "\n",
        "print(\"\\nStep 2 completed.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nxd82hgEYejU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Download NLTK stopwords\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Define stopwords (from your original code, ensure consistency)\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'area', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it', 'its', 'her', 'their', 'our',\n",
        "    'what', 'where', 'how', 'why', 'who', 'whom', 'which', 'whether',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hr', 'hrs', 'hour', 'hours', 'time', 'date', 'week', 'month', 'year', 'ago',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'code', 'id', 'location', 'address', 'phone', 'mobile', 'call', 'report', 'registered',\n",
        "    'ok', 'yes', 'no', 'not', 'hi', 'hello', 'sir', 'madam', 'pls', 'please', 'regards', 'type', 'urban', 'complaint', 'detail', 'general',\n",
        "    'kv', 'tf', 'na', 'service', 'request', 'feedback', 'query', 'regarding', 'about', 'given', 'areas', 'village'\n",
        "])\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Removes extra whitespace and standardizes text (generic version).\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text) # Ensure text is a string\n",
        "\n",
        "    # Remove any stray non-alphanumeric characters that might remain after emoji removal,\n",
        "    # then handle whitespace and lowercase.\n",
        "    # Keep alphanumeric characters and basic punctuation that might be relevant to remarks\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"-/]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text.strip()).lower()\n",
        "    return text\n",
        "\n",
        "def extract_time_features(remarks_series: pd.Series) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts numerical time features (hours, presence of AM/PM) from a Series of remarks.\n",
        "    Returns a DataFrame with these features.\n",
        "    \"\"\"\n",
        "    remarks_series = remarks_series.fillna(\"\").str.lower()\n",
        "\n",
        "    hour_pattern = r'(\\d+\\.?\\d*)\\s*(?:hr|hrs|hour|hours|h)'\n",
        "    day_pattern = r'(\\d+\\.?\\d*)\\s*(?:day|days)'\n",
        "    am_pm_pattern = r'\\b(\\d{1,2}(:\\d{2})?)\\s*(am|pm)\\b'\n",
        "\n",
        "    hours_extracted = remarks_series.str.extract(hour_pattern)[0].astype(float).fillna(0)\n",
        "    days_extracted = remarks_series.str.extract(day_pattern)[0].astype(float).fillna(0)\n",
        "\n",
        "    hours_from_days = days_extracted * 24\n",
        "    combined_hours = hours_extracted + hours_from_days\n",
        "\n",
        "    is_am_pm_mentioned = remarks_series.str.contains(am_pm_pattern, regex=True).astype(int)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'extracted_hours': combined_hours,\n",
        "        'is_am_pm_mentioned': is_am_pm_mentioned\n",
        "    })\n",
        "\n",
        "def main_classification_pipeline():\n",
        "    # --- Configuration ---\n",
        "    labeled_data_excel_path = \"./Book1.xlsx\"\n",
        "    new_remarks_excel_path = \"./Supply.xlsx\"\n",
        "    output_excel_path = \"./categorized_remarks_ML_model.xlsx\" # Output filename\n",
        "\n",
        "    # for supply\n",
        "    desired_category_labels = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"More than 4 hours\",\n",
        "        \"Consumer Power Supply Failures\",\n",
        "        \"Failed Pole Incident Category\",\n",
        "        \"Other Language Remarks\",\n",
        "        \"Partial Phase Supply Failure\",\n",
        "        \"Transformer Damage Causing Outages\"\n",
        "    ]\n",
        "    \"\"\"\n",
        "    desired_category_labels = [\n",
        "        \"Bill Accuracy and Discrepancies\",\n",
        "        \"Bill Content and Delivery Discrepancies\",\n",
        "        \"Bill Hold Preventing Online Payments\",\n",
        "        \"Billing Discrepancies Due to Meter Readings\",\n",
        "        \"Customer Billing Not Received or Available\",\n",
        "        \"Customer Reported Billing Inaccuracie\",\n",
        "        \"Other Language Remarks\",\n",
        "        \"Domestic Connection Billing Discrepancies\",\n",
        "        \"Domestic Meter Reading Collection Failure\"\n",
        "        \"Electricity Bill Discrepancies and Solar Units\"\n",
        "        \"Rectifying Account and Billing Discrepancies\"\n",
        "        \"Unacknowledged Customer Payments and Billing\"\n",
        "    ]\n",
        "    \"\"\"\n",
        "\n",
        "    # Column containing remarks in the NEW_REMARKS_EXCEL_PATH file (e.g., Supply.xlsx)\n",
        "    remarks_column_in_new_file = \"REMARKS\"\n",
        "\n",
        "    print(\"--- Starting Supervised ML Remark Categorization Pipeline (Time-Aware) ---\")\n",
        "    start_full_pipeline_time = time.time()\n",
        "\n",
        "    # --- Step 1: Data Preparation for Training ---\n",
        "    print(\"\\n--- Step 1: Data Preparation for Training ---\")\n",
        "    try:\n",
        "        df_raw_labeled = pd.read_excel(labeled_data_excel_path, header=0)\n",
        "        print(f\"Successfully loaded '{labeled_data_excel_path}'. Shape: {df_raw_labeled.shape}\")\n",
        "        print(\"First 5 rows of the raw labeled data:\")\n",
        "        print(df_raw_labeled.head())\n",
        "        print(\"Columns identified from the raw labeled data:\")\n",
        "        print(df_raw_labeled.columns.tolist())\n",
        "\n",
        "        loaded_to_label_map = {}\n",
        "        seen_labels = {}\n",
        "\n",
        "        for col in df_raw_labeled.columns:\n",
        "            normalized_col = col.split('.')[0]\n",
        "\n",
        "            if normalized_col in desired_category_labels:\n",
        "                if normalized_col in seen_labels:\n",
        "                    loaded_to_label_map[col] = normalized_col\n",
        "                else:\n",
        "                    loaded_to_label_map[col] = col\n",
        "                    seen_labels[normalized_col] = True\n",
        "            else:\n",
        "                print(f\"Warning: Column '{col}' from '{labeled_data_excel_path}' is not in the list of desired categories. It will be ignored for training data. If this is a category, please add it to 'desired_category_labels'.\")\n",
        "\n",
        "        active_category_columns_in_df = list(loaded_to_label_map.keys())\n",
        "\n",
        "        print(f\"\\nIdentified {len(set(loaded_to_label_map.values()))} Unique Category Labels for Training:\")\n",
        "        print(list(set(loaded_to_label_map.values())))\n",
        "        print(f\"Columns from '{labeled_data_excel_path}' that will be processed:\")\n",
        "        print(active_category_columns_in_df)\n",
        "\n",
        "        labeled_data_for_training = []\n",
        "        for category_col_in_df in active_category_columns_in_df:\n",
        "            standard_category_label = loaded_to_label_map[category_col_in_df]\n",
        "            for remark_entry in df_raw_labeled[category_col_in_df].dropna():\n",
        "                cleaned_remark = clean_text(str(remark_entry))\n",
        "                if cleaned_remark:\n",
        "                    labeled_data_for_training.append({\n",
        "                        'remark_original': str(remark_entry),\n",
        "                        'remark_cleaned': cleaned_remark,\n",
        "                        'category': standard_category_label\n",
        "                    })\n",
        "\n",
        "        df_labeled_for_training = pd.DataFrame(labeled_data_for_training)\n",
        "        print(f\"\\nTransformed data for training shape: {df_labeled_for_training.shape}\")\n",
        "        print(\"First 5 rows of the transformed labeled data for training:\")\n",
        "        print(df_labeled_for_training.head())\n",
        "        print(\"Value counts for 'category' (training labels):\")\n",
        "        print(df_labeled_for_training['category'].value_counts())\n",
        "        print(f\"Total unique categories identified in training data: {df_labeled_for_training['category'].nunique()}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: The file '{labeled_data_excel_path}' not found. Please ensure it's in the same directory.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 1: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- Step 2: Feature Extraction (Sentence Embeddings + Time Features) and Model Training ---\n",
        "    print(\"\\n--- Step 2: Feature Extraction (Sentence Embeddings + Time Features) and Model Training ---\")\n",
        "    try:\n",
        "        X_train_data = df_labeled_for_training['remark_cleaned'].tolist()\n",
        "        y_train_labels = df_labeled_for_training['category'].tolist()\n",
        "\n",
        "        X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "            pd.Series(X_train_data), y_train_labels, test_size=0.2, random_state=42, stratify=y_train_labels\n",
        "        )\n",
        "\n",
        "        # Extract and scale time features for training\n",
        "        scaler = StandardScaler() # Initialize scaler here\n",
        "        X_train_time_features_df = extract_time_features(X_train_text)\n",
        "        X_train_time_features_scaled = scaler.fit_transform(X_train_time_features_df)\n",
        "\n",
        "        X_test_time_features_df = extract_time_features(X_test_text)\n",
        "        X_test_time_features_scaled = scaler.transform(X_test_time_features_df) # Use fitted scaler\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Loading SentenceTransformer model on device: {device}...\")\n",
        "        model_embedding = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "        print(\"Generating embeddings for training text...\")\n",
        "        X_train_embeddings = model_embedding.encode(X_train_text.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
        "        print(\"Generating embeddings for testing text...\")\n",
        "        X_test_embeddings = model_embedding.encode(X_test_text.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "        X_train_combined_features = np.hstack((X_train_embeddings, X_train_time_features_scaled))\n",
        "        X_test_combined_features = np.hstack((X_test_embeddings, X_test_time_features_scaled))\n",
        "\n",
        "        print(\"\\nTraining Logistic Regression model on combined features...\")\n",
        "        classifier_model = LogisticRegression(\n",
        "            max_iter=1000, solver='lbfgs', multi_class='auto', class_weight='balanced', random_state=42, n_jobs=-1\n",
        "        )\n",
        "        classifier_model.fit(X_train_combined_features, y_train)\n",
        "        print(\"Logistic Regression model trained.\")\n",
        "\n",
        "        print(\"\\n--- Model Evaluation ---\")\n",
        "        y_pred = classifier_model.predict(X_test_combined_features)\n",
        "        print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 2: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- Step 3: Prediction and Output Structuring ---\n",
        "    print(\"\\n--- Step 3: Prediction and Output Structuring (Compacted Output) ---\")\n",
        "    try:\n",
        "        df_new_remarks_raw = pd.read_excel(new_remarks_excel_path)\n",
        "        print(f\"Successfully loaded new remarks from '{new_remarks_excel_path}'. Shape: {df_new_remarks_raw.shape}\")\n",
        "\n",
        "        if remarks_column_in_new_file not in df_new_remarks_raw.columns:\n",
        "            raise KeyError(f\"Column '{remarks_column_in_new_file}' not found in '{new_remarks_excel_path}'. Available columns: {df_new_remarks_raw.columns.tolist()}\")\n",
        "\n",
        "        new_remarks_data = []\n",
        "        for idx, remark_raw in df_new_remarks_raw[remarks_column_in_new_file].items():\n",
        "            cleaned = clean_text(remark_raw)\n",
        "            if cleaned:\n",
        "                new_remarks_data.append({'original_index': idx, 'remark_raw': remark_raw, 'remark_cleaned': cleaned})\n",
        "\n",
        "        df_remarks_to_classify = pd.DataFrame(new_remarks_data)\n",
        "        print(f\"Extracted {len(df_remarks_to_classify)} valid remarks for classification.\")\n",
        "\n",
        "        print(\"\\nGenerating embeddings for new remarks...\")\n",
        "        new_remarks_embeddings = model_embedding.encode(\n",
        "            df_remarks_to_classify['remark_cleaned'].tolist(),\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        # Extract and scale time features for new remarks\n",
        "        print(\"Extracting and scaling time features for new remarks...\")\n",
        "        new_remarks_time_features_df = extract_time_features(df_remarks_to_classify['remark_cleaned'])\n",
        "        new_remarks_time_features_scaled = scaler.transform(new_remarks_time_features_df) # Use the *same* scaler from training\n",
        "\n",
        "        # Concatenate for prediction\n",
        "        new_remarks_combined_features = np.hstack((new_remarks_embeddings, new_remarks_time_features_scaled))\n",
        "\n",
        "        print(\"\\nPredicting categories for new remarks...\")\n",
        "        predicted_categories = classifier_model.predict(new_remarks_combined_features)\n",
        "        df_remarks_to_classify['predicted_category'] = predicted_categories\n",
        "        print(\"First 5 remarks with predicted categories:\")\n",
        "        print(df_remarks_to_classify.head())\n",
        "\n",
        "        # --- MODIFIED PART FOR COMPACTED OUTPUT ---\n",
        "        print(\"\\nCompacting output into wide format by pushing remarks to the top of each category column...\")\n",
        "\n",
        "        categorized_remarks_by_column = defaultdict(list)\n",
        "\n",
        "        for idx, row_data in df_remarks_to_classify.iterrows():\n",
        "            remark = row_data['remark_raw']\n",
        "            predicted_cat = row_data['predicted_category']\n",
        "            categorized_remarks_by_column[predicted_cat].append(remark)\n",
        "\n",
        "        max_remarks_in_any_cat = 0\n",
        "        if categorized_remarks_by_column:\n",
        "            max_remarks_in_any_cat = max(len(v) for v in categorized_remarks_by_column.values())\n",
        "\n",
        "        df_output_compacted_wide = pd.DataFrame({\n",
        "            col: categorized_remarks_by_column.get(col, []) + [''] * (max_remarks_in_any_cat - len(categorized_remarks_by_column.get(col, [])))\n",
        "            for col in sorted(list(set(desired_category_labels)))\n",
        "        })\n",
        "\n",
        "        print(f\"\\nFinal Compacted Wide Output DataFrame shape: {df_output_compacted_wide.shape}\")\n",
        "        print(\"First 5 rows of the Final Compacted Wide Output DataFrame:\")\n",
        "        print(df_output_compacted_wide.head())\n",
        "\n",
        "        print(f\"\\nSaving results to '{output_excel_path}'...\")\n",
        "        df_output_compacted_wide.to_excel(output_excel_path, index=False)\n",
        "        print(\"Results saved successfully.\")\n",
        "\n",
        "        # --- NEW LINES ADDED HERE FOR MODEL SAVING ---\n",
        "        print(\"\\n--- Saving trained models for future use ---\")\n",
        "        try:\n",
        "            joblib.dump(model_embedding, 'sentence_transformer_model.pkl')\n",
        "            joblib.dump(classifier_model, 'logistic_regression_classifier.pkl')\n",
        "            joblib.dump(scaler, 'scaler_for_time_features.pkl')\n",
        "            print(\"Models (Sentence Transformer, Classifier, Scaler) saved successfully to .pkl files.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to save models: {e}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "        # --- END OF NEW LINES ---\n",
        "\n",
        "        # --- Final Verification of Total Count ---\n",
        "        print(\"\\n--- Final Verification of Total Count ---\")\n",
        "        df_categorized_check = pd.read_excel(output_excel_path)\n",
        "        print(f\"Shape of the re-loaded compacted categorized file: {df_categorized_check.shape}\")\n",
        "\n",
        "        total_categorized_remarks_from_file = 0\n",
        "        for col in df_categorized_check.columns:\n",
        "            total_categorized_remarks_from_file += df_categorized_check[col].apply(lambda x: pd.notna(x) and str(x).strip() != '').sum()\n",
        "\n",
        "        print(f\"Total number of remarks in the compacted categorized file (non-empty cell count): {total_categorized_remarks_from_file}\")\n",
        "        print(f\"Number of remarks successfully extracted and classified (excluding empty after cleaning): {len(df_remarks_to_classify)}\")\n",
        "\n",
        "        if total_categorized_remarks_from_file == len(df_remarks_to_classify):\n",
        "            print(\"All extracted and classified remarks successfully written to compacted output file.\")\n",
        "        else:\n",
        "            print(f\"Mismatch: {len(df_remarks_to_classify) - total_categorized_remarks_from_file} remarks missing from final file count. Investigate Excel saving/loading or if some cells are truly empty/NaN in source.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: The file '{new_remarks_excel_path}' not found. Please ensure it's in the same directory.\")\n",
        "        return\n",
        "    except KeyError as e:\n",
        "        print(f\"ERROR: Missing expected column in '{new_remarks_excel_path}'. Details: {e}\")\n",
        "        if 'df_new_remarks_raw' in locals():\n",
        "            print(\"Available columns in your Excel file are:\", df_new_remarks_raw.columns.tolist())\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 3: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Supervised ML Categorization Pipeline Completed in {time.time() - start_full_pipeline_time:.2f} seconds ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_classification_pipeline()"
      ],
      "metadata": {
        "id": "reEP9ZQmk8Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Others Reallocation without GEMINI**"
      ],
      "metadata": {
        "id": "E5_nV7FiV-l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from joblib import Parallel, delayed\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- Constants ---\n",
        "BATCH_SIZE = 500\n",
        "MIN_REMARKS_FOR_NEW_COLUMN = 5\n",
        "SIMILARITY_THRESHOLD = 0.6\n",
        "NUM_CLUSTERS = 8\n",
        "MAX_FEATURES = 2000\n",
        "N_COMPONENTS = 100\n",
        "GEMINI_API_KEY = \"AIzaSyBHwfAgTs-RzC7uF4QzUSA30_HfMR9MwZQ\"  # Replace with your actual API key\n",
        "\n",
        "# Configure Gemini API\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# Load spaCy for text processing\n",
        "print(\"Loading spaCy language model...\")\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "    print(\"spaCy model loaded successfully\")\n",
        "except OSError:\n",
        "    print(\"Warning: spaCy English model not found. Using simpler text processing.\")\n",
        "    print(\"For better results, install with: python -m spacy download en_core_web_sm\")\n",
        "    nlp = None\n",
        "\n",
        "# --- Text Preprocessing ---\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Enhanced text cleaning and normalization\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    if nlp is not None:\n",
        "        doc = nlp(text)\n",
        "        tokens = [token.lemma_ for token in doc if not token.is_stop and len(token.lemma_) > 2]\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# --- Data Loading ---\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load Excel file with error handling\"\"\"\n",
        "    print(f\"Loading data from {file_path}...\")\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        print(f\"Successfully loaded {len(df)} rows\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# --- Vectorization Pipeline ---\n",
        "def create_vectorizer():\n",
        "    \"\"\"Create TF-IDF vectorizer with dimensionality reduction\"\"\"\n",
        "    print(\"Creating vectorization pipeline...\")\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=MAX_FEATURES,\n",
        "        stop_words='english',\n",
        "        preprocessor=preprocess_text\n",
        "    )\n",
        "\n",
        "    svd = TruncatedSVD(n_components=N_COMPONENTS)\n",
        "    normalizer = Normalizer(copy=False)\n",
        "    pipeline = make_pipeline(vectorizer, svd, normalizer)\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "# --- Category Modeling ---\n",
        "def build_category_models(reference_df, existing_columns, vectorizer):\n",
        "    \"\"\"Build representative models for each category\"\"\"\n",
        "    print(\"\\nBuilding category models...\")\n",
        "    category_models = {}\n",
        "\n",
        "    all_text = []\n",
        "    for col in existing_columns:\n",
        "        all_text.extend(reference_df[col].dropna().astype(str).tolist())\n",
        "\n",
        "    print(\"Fitting vectorizer on reference data...\")\n",
        "    vectorizer.fit(all_text)\n",
        "\n",
        "    for col in tqdm(existing_columns, desc=\"Processing categories\"):\n",
        "        remarks = reference_df[col].dropna().astype(str).tolist()\n",
        "\n",
        "        if not remarks:\n",
        "            remarks = [preprocess_text(col)]\n",
        "\n",
        "        vectors = vectorizer.transform(remarks)\n",
        "        category_models[col] = vectors.mean(axis=0)\n",
        "\n",
        "    return category_models, vectorizer\n",
        "\n",
        "# --- Gemini API Functions ---\n",
        "def generate_meaningful_name_with_gemini(top_terms, examples):\n",
        "    \"\"\"Generate professional column names using Gemini\"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "        Analyze these key terms and customer remarks to create a professional category name:\n",
        "\n",
        "        Key Terms: {', '.join(top_terms)}\n",
        "        Example Remarks:\n",
        "        {examples}\n",
        "\n",
        "        Create a concise, professional column name (3-5 words) that:\n",
        "        1. Accurately represents the common theme\n",
        "        2. Uses clear business terminology\n",
        "        3. Is specific yet broad enough to cover similar cases\n",
        "        4. Follows title case formatting\n",
        "\n",
        "        Provide ONLY the column name, no additional text.\n",
        "        \"\"\"\n",
        "\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text.strip().strip('\"').strip(\"'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating column name: {e}\")\n",
        "        return f\"{' '.join(t.title() for t in top_terms[:2])} Issues\"\n",
        "\n",
        "# --- Remark Categorization ---\n",
        "def categorize_remark(remark, category_models, vectorizer):\n",
        "    \"\"\"Categorize a single remark\"\"\"\n",
        "    if not isinstance(remark, str) or not remark.strip():\n",
        "        return None\n",
        "\n",
        "    preprocessed = preprocess_text(remark)\n",
        "    if not preprocessed:\n",
        "        return None\n",
        "\n",
        "    remark_vector = vectorizer.transform([preprocessed])\n",
        "\n",
        "    similarities = {\n",
        "        cat: cosine_similarity(remark_vector, model.reshape(1, -1))[0][0]\n",
        "        for cat, model in category_models.items()\n",
        "    }\n",
        "\n",
        "    best_cat = max(similarities.items(), key=lambda x: x[1])\n",
        "    return best_cat[0] if best_cat[1] > SIMILARITY_THRESHOLD else None\n",
        "\n",
        "# --- New Column Suggestion ---\n",
        "def suggest_new_columns(remarks, vectorizer, min_remarks=5):\n",
        "    \"\"\"Suggest new columns with meaningful names using Gemini\"\"\"\n",
        "    print(\"\\nAnalyzing uncategorized remarks for new column suggestions...\")\n",
        "\n",
        "    if len(remarks) < min_remarks:\n",
        "        print(f\"Not enough remarks ({len(remarks)}) to suggest new columns (minimum {min_remarks})\")\n",
        "        return {}\n",
        "\n",
        "    print(\"Vectorizing remarks...\")\n",
        "    X = vectorizer.transform(remarks)\n",
        "\n",
        "    print(f\"Clustering {len(remarks)} remarks into {NUM_CLUSTERS} groups...\")\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=min(NUM_CLUSTERS, len(remarks)),\n",
        "        random_state=42,\n",
        "        n_init=10\n",
        "    )\n",
        "    clusters = kmeans.fit_predict(X)\n",
        "\n",
        "    new_columns = {}\n",
        "    feature_names = vectorizer.named_steps['tfidfvectorizer'].get_feature_names_out()\n",
        "\n",
        "    for cluster_id in range(kmeans.n_clusters):\n",
        "        cluster_indices = np.where(clusters == cluster_id)[0]\n",
        "        cluster_size = len(cluster_indices)\n",
        "\n",
        "        if cluster_size < min_remarks:\n",
        "            continue\n",
        "\n",
        "        center = kmeans.cluster_centers_[cluster_id]\n",
        "        top_terms = center.argsort()[-3:][::-1]\n",
        "        top_terms = [feature_names[i] for i in top_terms]\n",
        "\n",
        "        example_indices = np.random.choice(cluster_indices, size=min(3, cluster_size), replace=False)\n",
        "        examples = [remarks[i] for i in example_indices]\n",
        "\n",
        "        try:\n",
        "            column_name = generate_meaningful_name_with_gemini(top_terms, examples)\n",
        "\n",
        "            new_columns[column_name] = {\n",
        "                'count': cluster_size,\n",
        "                'examples': examples\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating name for cluster {cluster_id}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if new_columns:\n",
        "        print(f\"Found {len(new_columns)} potential new columns\")\n",
        "    else:\n",
        "        print(\"No clear patterns found for new columns\")\n",
        "\n",
        "    return new_columns\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def process_remarks(input_path, reference_path, output_path):\n",
        "    \"\"\"Main processing pipeline\"\"\"\n",
        "    print(f\"\\nStarting remark categorization process\")\n",
        "    print(f\"Input file: {input_path}\")\n",
        "    print(f\"Reference file: {reference_path}\")\n",
        "    print(f\"Output file: {output_path}\\n\")\n",
        "\n",
        "    df = load_data(input_path)\n",
        "    ref_df = load_data(reference_path)\n",
        "\n",
        "    if 'Others' not in df.columns:\n",
        "        print(\"Error: Input file must contain an 'Others' column\")\n",
        "        return\n",
        "\n",
        "    existing_columns = [col for col in df.columns if col != 'Others']\n",
        "\n",
        "    vectorizer = create_vectorizer()\n",
        "    category_models, fitted_vectorizer = build_category_models(ref_df, existing_columns, vectorizer)\n",
        "\n",
        "    print(\"\\nProcessing remarks...\")\n",
        "    uncategorized = []\n",
        "    total = len(df)\n",
        "\n",
        "    for start in tqdm(range(0, total, BATCH_SIZE), desc=\"Processing batches\"):\n",
        "        end = min(start + BATCH_SIZE, total)\n",
        "        batch = df.iloc[start:end]\n",
        "\n",
        "        for idx, row in batch.iterrows():\n",
        "            remark = row['Others']\n",
        "            if pd.isna(remark):\n",
        "                continue\n",
        "\n",
        "            category = categorize_remark(str(remark), category_models, fitted_vectorizer)\n",
        "\n",
        "            if category:\n",
        "                df.at[idx, category] = remark\n",
        "                df.at[idx, 'Others'] = None\n",
        "            else:\n",
        "                uncategorized.append(remark)\n",
        "\n",
        "    new_columns = suggest_new_columns(uncategorized, fitted_vectorizer, MIN_REMARKS_FOR_NEW_COLUMN)\n",
        "\n",
        "    print(\"\\nSaving results...\")\n",
        "    df.to_excel(output_path, index=False)\n",
        "    print(f\"Results saved to {output_path}\")\n",
        "\n",
        "    print(\"\\n=== Processing Complete ===\")\n",
        "    print(f\"Total remarks processed: {total}\")\n",
        "    print(f\"Categorized remarks: {total - len(uncategorized)}\")\n",
        "    print(f\"Remaining uncategorized: {len(uncategorized)}\")\n",
        "\n",
        "    if new_columns:\n",
        "        print(\"\\n=== Suggested New Columns ===\")\n",
        "        for col, info in new_columns.items():\n",
        "            print(f\"\\nColumn: {col}\")\n",
        "            print(f\"Count: {info['count']}\")\n",
        "            print(\"Example remarks:\")\n",
        "            for ex in info['examples']:\n",
        "                print(f\" - {ex[:80]}{'...' if len(ex) > 80 else ''}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = 'categorized_remarks_others.xlsx'\n",
        "    reference_file = 'Book1.xlsx'\n",
        "    output_file = 'output.xlsx'\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Error: Input file not found - {input_file}\")\n",
        "    elif not os.path.exists(reference_file):\n",
        "        print(f\"Error: Reference file not found - {reference_file}\")\n",
        "    else:\n",
        "        process_remarks(input_file, reference_file, output_file)"
      ],
      "metadata": {
        "id": "DPF_kYpYrsDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "f0dCuukkY1Ex"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOvKsvWHcQoJrqzFO84A6Xt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}