{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadipatodia/Text-Classification_Using_AI/blob/main/TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSF9MRBmIUun"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect\n",
        "!pip install --upgrade sentence-transformers transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0d87UZ_44p7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yNKhhC7hI6d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "from langdetect import detect, DetectorFactory\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import cudf\n",
        "from cuml.cluster import KMeans\n",
        "import google.generativeai as genai\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Set Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAwLVHm49YsJu4PK6ilxc7MiwLxI6sBU7E\"\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "except KeyError:\n",
        "    print(\" ERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "    print(\"Set it in PowerShell: $env:GEMINI_API_KEY = 'your_api_key_here'\")\n",
        "    exit()\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# Download NLTK stopwords\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Define stopwords These words are considered less meaningful for categorization and are typically removed or ignored during text analysis.\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'area', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it', 'its', 'her', 'their', 'our',\n",
        "    'what', 'where', 'how', 'why', 'who', 'whom', 'which', 'whether',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hr', 'hrs', 'hour', 'hours', 'time', 'date', 'week', 'month', 'year', 'ago',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'code', 'id', 'location', 'address', 'phone', 'mobile', 'call', 'report', 'registered',\n",
        "    'ok', 'yes', 'no', 'not', 'hi', 'hello', 'sir', 'madam', 'pls', 'please', 'regards', 'type', 'urban', 'complaint', 'detail', 'general',\n",
        "    'kv', 'tf', 'na', 'service', 'request', 'feedback', 'query', 'regarding', 'about', 'given', 'areas', 'village'\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Removes extra whitespace and standardizes text (generic version).\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', str(text).strip()).lower()\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def vectorized_time_categorization(df, remark_col, duration_col):\n",
        "\n",
        "    # Optimized time categorization using pandas str.extract.\n",
        "\n",
        "    print(\"     [Preprocessing] Vectorized time categorization...\")\n",
        "\n",
        "    # Applies regular expressions (hour_pattern, day_pattern) across the entire Series to extract numerical values associated with time units\n",
        "    # (e.g., (\\d+\\.?\\d*) captures numbers, (?:hr|hrs|hour|hours|h) matches various hour terms non-capturingly).\n",
        "    # This avoids slow row-by-row Python loops.\n",
        "\n",
        "    start_time = time.time()\n",
        "    duration_series = df[duration_col] if duration_col in df.columns else df[remark_col]\n",
        "    duration_series = duration_series.fillna(\"\").str.lower().apply(clean_text)\n",
        "    hour_pattern = r'(\\d+\\.?\\d*)\\s*(?:hr|hrs|hour|hours|h)'\n",
        "    day_pattern = r'(\\d+\\.?\\d*)\\s*(?:day|days)'\n",
        "\n",
        "    # Extract hours and days\n",
        "    hours_extracted = duration_series.str.extract(hour_pattern)\n",
        "    days_extracted = duration_series.str.extract(day_pattern)\n",
        "\n",
        "    # Initialize categories and hours\n",
        "    categories = pd.Series([\"No Time Specified\"] * len(duration_series), index=df.index)\n",
        "    extracted_hours = pd.Series([None] * len(duration_series), index=df.index, dtype=float)\n",
        "\n",
        "    # Process hours\n",
        "    # Boolean Masking : for efficient conditional assignment of categories and extracted numerical hours\n",
        "    hour_mask = hours_extracted[0].notna()\n",
        "    hours = hours_extracted[0].astype(float)\n",
        "    extracted_hours[hour_mask] = hours[hour_mask]\n",
        "    categories[hour_mask & (hours < 4)] = \"Less than 4 hours\"\n",
        "    categories[hour_mask & (hours >= 4) & (hours < 12)] = \"More than 4 hours\"\n",
        "    categories[hour_mask & (hours >= 12) & (hours < 24)] = \"More than 12 hours\"\n",
        "    categories[hour_mask & (hours >= 24)] = \"More than 24 hours\"\n",
        "\n",
        "    # Process days (where hours not already set)\n",
        "    day_mask = days_extracted[0].notna() & hours_extracted[0].isna()\n",
        "    days = days_extracted[0].astype(float)\n",
        "    hours_from_days = days * 24\n",
        "    extracted_hours[day_mask] = hours_from_days[day_mask]\n",
        "    categories[day_mask & (hours_from_days < 4)] = \"Less than 4 hours\"\n",
        "    categories[day_mask & (hours_from_days >= 4) & (hours_from_days < 12)] = \"More than 4 hours\"\n",
        "    categories[day_mask & (hours_from_days >= 12) & (hours_from_days < 24)] = \"More than 12 hours\"\n",
        "    categories[day_mask & (hours_from_days >= 24)] = \"More than 24 hours\"\n",
        "\n",
        "    print(f\"     [Preprocessing] Completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return categories, extracted_hours\n",
        "\n",
        "\n",
        "\n",
        "def get_top_keywords(remarks: list[str], n_keywords: int = 10) -> list[str]:\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts top keywords using TF-IDF.\n",
        "    A statistical measure that evaluates how relevant a word is to a document in a collection.\n",
        "    It assigns a higher score to words that appear frequently in a specific document but rarely in the overall corpus\n",
        "    (after removing common words like stopwords).\n",
        "    \"\"\"\n",
        "\n",
        "    if not remarks or len(remarks) < 2:\n",
        "        return []\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            stop_words=list(UNIVERSAL_STOPWORDS), ngram_range=(1, 3), min_df=5, max_features=1000\n",
        "        # ngram_range=(1, 3) : Considers unigrams (single words), bigrams (two-word phrases), and trigrams to capture more contextual meaning\n",
        "        # min_df=5: Ignores terms that appear in fewer than 5 documents, helping to filter out rare or noisy terms\n",
        "        # max_features=1000: Limits the total number of unique keywords considered, reducing dimensionality.\n",
        "        )\n",
        "        tfidf_matrix = vectorizer.fit_transform([r for r in remarks if r][:1000])\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()  # used as a proxy for the overall importance of each term in the collection\n",
        "        top_indices = scores.argsort()[-n_keywords:][::-1]\n",
        "        return [feature_names[i] for i in top_indices]\n",
        "    except ValueError as e:\n",
        "        print(f\"   [Warning] TF-IDF failed: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "def get_genai_cluster_name(cluster_texts: list[str], top_keywords: list[str]) -> str:\n",
        "    \"\"\"Generates a category name using Gemini API.\"\"\"\n",
        "    print(\"    [Gen AI Naming] Sending prompt to Gemini model...\")\n",
        "    if not cluster_texts:\n",
        "        return \"Uncategorized Remarks\"\n",
        "\n",
        "    sample_size = min(20, len(cluster_texts))\n",
        "    text_sample = \"\\n\".join([t[:100] for t in cluster_texts[:sample_size] if t])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert at analyzing customer feedback in the energy sector. Provide a single, concise, professional category name for a group of similar remarks. The name must be 4-7 words, reflect the primary issue accurately, and avoid generic terms like 'Issues', 'Problems', or 'Reports' unless critical. Do not overlap with time-based categories (e.g., 'Less than 4 hours').\n",
        "\n",
        "    Top keywords: {', '.join(top_keywords[:5])}.\n",
        "    Sample remarks:\n",
        "    {text_sample}\n",
        "\n",
        "    Category name:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        name = response.text.strip().split(\"Category name:\")[-1].strip() if \"Category name:\" in response.text else response.text.strip()\n",
        "        if not name or len(name.split()) < 4 or len(name.split()) > 7 or any(t.lower() in name.lower() for t in ['less', 'more', 'hours', 'time']):\n",
        "            name = f\"{top_keywords[0].replace('_', ' ').title()} Incident Category\" if top_keywords else \"Uncategorized Remarks\"\n",
        "        return name[:50].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"    [Gen AI Naming] ERROR: API call failed. Falling back to keywords. Error: {e}\")\n",
        "        return f\"{top_keywords[0].replace('_', ' ').title()} Incident Category\"[:50].strip() if top_keywords else \"Uncategorized Remarks\"\n",
        "\n",
        "\n",
        "\n",
        "def get_unique_name(base_name: str, existing_names: set, suffix_identifier: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Generates a unique name.\n",
        "    It iteratively appends alphabetical (A, B, C...) or alphanumeric (A1, A2...) suffixes to the base_name until a unique name is found that\n",
        "    does not exist in the existing_names set. re.sub is used for initial cleaning of the base name\n",
        "\n",
        "    \"\"\"\n",
        "    name = re.sub(r'[^a-zA-Z\\s]', '', base_name).strip()\n",
        "    name = re.sub(r'\\s+', ' ', name).strip()\n",
        "    if not name:\n",
        "        name = \"Generic Category\"\n",
        "    original_base = name\n",
        "    alpha_suffix_idx = 0\n",
        "    numeric_suffix_idx = 0\n",
        "    while name.lower() in existing_names:\n",
        "        if alpha_suffix_idx < 26:\n",
        "            name = f\"{original_base} {chr(65 + alpha_suffix_idx)}\"\n",
        "            alpha_suffix_idx += 1\n",
        "        else:\n",
        "            numeric_suffix_idx += 1\n",
        "            alpha_suffix_idx_for_num = (alpha_suffix_idx - 26) % 26\n",
        "            name = f\"{original_base} {chr(65 + alpha_suffix_idx_for_num)}{numeric_suffix_idx}\"\n",
        "            alpha_suffix_idx += 1\n",
        "    return name[:50].strip()\n",
        "\n",
        "\n",
        "\n",
        "def is_semantically_similar(name1: str, name2: str) -> bool:\n",
        "    \"\"\"Uses Gemini to check if two column names are semantically similar.\"\"\"\n",
        "    print(f\"   [Gen AI Merging] Checking similarity between '{name1}' and '{name2}'...\")\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert at analyzing customer feedback in the energy sector. Determine if the following two category names are synonyms or convey the same meaning. Answer with a single word: \"YES\" or \"NO\".\n",
        "\n",
        "    Category 1: \"{name1}\"\n",
        "    Category 2: \"{name2}\"\n",
        "\n",
        "    Recommendation:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        recommendation = response.text.strip().split(\"Recommendation:\")[-1].strip() if \"Recommendation:\" in response.text else response.text.strip()\n",
        "        return recommendation.lower() == \"yes\"\n",
        "    except Exception as e:\n",
        "        print(f\"    [Gen AI Merging] ERROR: API call failed. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def merge_similar_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Merges non-time columns with semantically similar names\"\"\"\n",
        "    print(\"\\n--- Merging similar non-time columns (Semantic Match) ---\")\n",
        "    time_columns = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"No Time Specified\"\n",
        "    ]\n",
        "    columns_to_process = [col for col in df.columns if col not in time_columns]\n",
        "    merged_mapping = {}\n",
        "\n",
        "    max_non_time_columns_target = 4\n",
        "\n",
        "    did_merge = True # loop allows for multiple rounds of merging until no more similar pairs are found or the target column count is reached.\n",
        "    while did_merge and len(set(columns_to_process) - set(merged_mapping.keys())) > max_non_time_columns_target:\n",
        "        did_merge = False\n",
        "        current_active_cols = sorted([col for col in columns_to_process if col not in merged_mapping])\n",
        "        # Sorting ensures a consistent order of comparison\n",
        "\n",
        "        for i in range(len(current_active_cols)):\n",
        "            col1 = current_active_cols[i]\n",
        "            if col1 in merged_mapping:                # This means col1 has already been chosen as a source in this iteration\n",
        "                continue\n",
        "\n",
        "            for j in range(i + 1, len(current_active_cols)):\n",
        "                col2 = current_active_cols[j]\n",
        "                if col2 in merged_mapping:            # If col2 has already been chosen as a source in this iteration\n",
        "                    continue\n",
        "\n",
        "                if is_semantically_similar(col1, col2):  # The AI call for similarity assessment.\n",
        "                    current_unmerged_count = len(set(columns_to_process) - set(merged_mapping.keys()))\n",
        "                    if current_unmerged_count > max_non_time_columns_target:\n",
        "                        print(f\"    Merging '{col2}' into '{col1}' (Semantic Match)\")\n",
        "                        merged_mapping[col2] = col1\n",
        "                        did_merge = True\n",
        "                        break                          # Found a merge, break inner loop to re-evaluate current_active_cols\n",
        "            if did_merge:                              # If a merge happened in inner loop, break outer loop to restart while loop\n",
        "                break\n",
        "\n",
        "    temp_df = df.copy()\n",
        "    for source_col, target_col in merged_mapping.items():         # merged_mapping: A dictionary stores source_column_name: target_column_name pairs.\n",
        "        if target_col not in temp_df.columns:\n",
        "            temp_df[target_col] = np.nan\n",
        "        temp_df.loc[:, target_col] = temp_df[target_col].fillna(temp_df[source_col])  # This is a key pandas operation, it takes all non-null\n",
        "                                                                                      # values from source_col and fills corresponding NaN (missing)\n",
        "                                                                                      # spots in target_col. This effectively moves remarks without\n",
        "                                                                                      # overwriting existing data in the target.\n",
        "        temp_df = temp_df.drop(columns=[source_col])               # Removes the source column after its data has been transferred.\n",
        "\n",
        "    final_columns = []\n",
        "    for col in df.columns:\n",
        "        if col in time_columns:\n",
        "            final_columns.append(col)\n",
        "        elif col not in merged_mapping.keys():                         # If it's a non-time column and not a source of a merge\n",
        "            if col not in merged_mapping.values():                     # Ensure it's not a target that was just created\n",
        "                final_columns.append(col)\n",
        "\n",
        "    for target_col in set(merged_mapping.values()):\n",
        "        if target_col not in final_columns:\n",
        "            final_columns.append(target_col)\n",
        "\n",
        "    final_column_order = sorted(final_columns, key=lambda x: (x not in time_columns, x))\n",
        "    final_column_order = [col for col in final_column_order if col in temp_df.columns]\n",
        "\n",
        "    df_merged = temp_df[final_column_order]\n",
        "    print(\"    Merging complete.\")\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "\n",
        "def load_excel_file(file_path: str, column: str) -> tuple[list[str], pd.DataFrame]:\n",
        "    \"\"\"Loads remarks from an Excel file.\"\"\"\n",
        "    print(f\"Loading data from '{file_path}'...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, usecols=[column, \"From When Issue Is Coming\"] if \"From When Issue Is Coming\" in pd.read_excel(file_path, nrows=1).columns else [column])\n",
        "        print(f\"Loaded {len(df)} rows in {time.time() - start_time:.2f} seconds.\")\n",
        "        remarks_list = [str(r) for r in df[column] if not pd.isna(r)]\n",
        "        print(f\"Extracted {len(remarks_list)} valid remarks from column '{column}'.\")\n",
        "        return remarks_list, df\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"❌ ERROR: File '{file_path}' not found. {e}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: Failed to load Excel file. {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "def save_results(df: pd.DataFrame, output_path: str):\n",
        "    \"\"\"Saves results to an Excel file.\"\"\"\n",
        "    print(f\"\\nSaving results to '{output_path}'...\")\n",
        "    start_time = time.time()\n",
        "    df.to_excel(output_path, index=False)        # saves the DataFrame to an Excel file without writing the pandas internal index as a column.\n",
        "    print(f\"Saved successfully in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "\n",
        "def segregate_remarks_by_language(raw_remarks: list[str], min_text_for_detection: int = 10) -> tuple[list[tuple[int, str]], list[tuple[int, str]]]:\n",
        "    \"\"\"Segregates remarks into English and other languages.\"\"\"\n",
        "    print(f\"Starting language segregation for {len(raw_remarks)} remarks...\")\n",
        "    start_time = time.time()\n",
        "    def detect_lang(i, remark):\n",
        "        cleaned_remark = clean_text(remark.lower())\n",
        "        if len(cleaned_remark) < min_text_for_detection or not any(char.isalpha() for char in cleaned_remark):\n",
        "            return i, remark, False\n",
        "        try:\n",
        "            return i, remark, detect(cleaned_remark) == 'en'\n",
        "        except Exception:\n",
        "            return i, remark, False\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(delayed(detect_lang)(i, r) for i, r in enumerate(raw_remarks))\n",
        "    english_remarks_with_indices = [(i, r) for i, r, is_en in results if is_en]\n",
        "    other_remarks_with_indices = [(i, r) for i, r, is_en in results if not is_en]\n",
        "    print(f\"Segregation complete in {time.time() - start_time:.2f} seconds. English: {len(english_remarks_with_indices)}, Other: {len(other_remarks_with_indices)}\")\n",
        "    return english_remarks_with_indices, other_remarks_with_indices\n",
        "\n",
        "\n",
        "\n",
        "def cluster_remarks(remarks: list[str], n_clusters: int = 10, batch_size: int = 512) -> list[int]:\n",
        "    \"\"\"\n",
        "    Clusters remarks using sentence transformers and cuML KMeans.\n",
        "    Loads a pre-trained Transformer model. This model converts full sentences into high-dimensional numerical vectors (embeddings).\n",
        "    The key idea is that sentences with similar meanings will have embeddings that are numerically \"close\" to each other in this vector space.\n",
        "    use an \"attention mechanism\" to weigh the importance of different words in a sentence relative to each other. This allows them to capture\n",
        "    complex contextual relationships and produce high-quality semantic representations for entire sentences.\n",
        "\n",
        "    \"\"\"\n",
        "    if not remarks:\n",
        "        return []\n",
        "    print(\"     [Clustering] Encoding remarks with sentence-transformers...\")\n",
        "    start_time = time.time()\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "    embeddings = []\n",
        "    for i in range(0, len(remarks), batch_size):\n",
        "        batch = remarks[i:i + batch_size]\n",
        "        batch_embeddings = model.encode(batch, batch_size=batch_size, show_progress_bar=False, convert_to_numpy=True)\n",
        "        embeddings.append(batch_embeddings)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    print(f\"     [Clustering] Encoding completed in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    K-Means is an unsupervised clustering algorithm that aims to partition n observations into k clusters. It works by iteratively assigning each\n",
        "    data point to the closest cluster centroid and then re-calculating the centroids as the mean of the points in the cluster\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"     [Clustering] Performing KMeans clustering with cuML...\")\n",
        "    start_time = time.time()\n",
        "    gdf = cudf.DataFrame(embeddings)\n",
        "    clustering = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    \"\"\"\n",
        "    n_clusters: The desired number of clusters.\n",
        "    random_state: Ensures the centroid initialization is reproducible.\n",
        "    n_init=10: Runs the algorithm 10 times with different centroid initializations and picks the best result\n",
        "    (minimizing inertia/sum of squared distances)\n",
        "    \"\"\"\n",
        "\n",
        "    cluster_labels = clustering.fit_predict(gdf).to_numpy() # Performs the clustering on the GPU (gdf) and returns the cluster assignments for\n",
        "                                                            # each remark as a NumPy array.\n",
        "    print(f\"     [Clustering] Clustering completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return cluster_labels\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    excel_file_path = \"./Supply.xlsx\"\n",
        "    text_column_name = \"REMARKS\"\n",
        "    duration_column_name = \"From When Issue Is Coming\"\n",
        "    output_excel_path = \"./categorized_remarks_01.xlsx\"\n",
        "    max_remark_clusters_limit = 8                    # This limits initial number of clusters for English remarks\n",
        "    batch_size = 500\n",
        "\n",
        "    time_columns = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"No Time Specified\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Remark Categorization Script ---\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        raw_remarks_list, df = load_excel_file(excel_file_path, text_column_name)\n",
        "\n",
        "        print(f\"\\n--- Categorizing remarks ---\")\n",
        "        time_categorized_remarks = {col: [] for col in time_columns}\n",
        "        non_time_remarks_with_indices = []\n",
        "\n",
        "        categories, _ = vectorized_time_categorization(df, text_column_name, duration_column_name)\n",
        "        # Handle cases where `categories` might not be directly iterable if df is empty etc.\n",
        "        # Ensure `zip` handles potential length mismatch safely if remarks are cleaned/filtered.\n",
        "        for i, (remark, category) in enumerate(zip(raw_remarks_list, categories)):\n",
        "            if category != \"No Time Specified\":\n",
        "                time_categorized_remarks[category].append((i, remark))\n",
        "            else:\n",
        "                non_time_remarks_with_indices.append((i, remark))\n",
        "\n",
        "        print(f\"Time-based categorization complete. Counts: { {k: len(v) for k, v in time_categorized_remarks.items()} }\")\n",
        "        print(f\"Non-time remarks for further processing: {len(non_time_remarks_with_indices)}\")\n",
        "\n",
        "        print(\"\\n--- Segregating non-time remarks by language ---\")\n",
        "        english_remarks_with_indices, other_remarks_with_indices = segregate_remarks_by_language(  # to check if non time remarks are english or not\n",
        "            [r for _, r in non_time_remarks_with_indices]\n",
        "        )\n",
        "        # Re-map indices to original dataframe index\n",
        "        english_remarks_with_indices_original = [(non_time_remarks_with_indices[local_idx][0], r) for local_idx, r in english_remarks_with_indices]\n",
        "        other_remarks_with_indices_original = [(non_time_remarks_with_indices[local_idx][0], r) for local_idx, r in other_remarks_with_indices]\n",
        "\n",
        "\n",
        "        print(f\"Non-time English remarks: {len(english_remarks_with_indices_original)}\")\n",
        "        print(f\"Non-time other language remarks: {len(other_remarks_with_indices_original)}\")\n",
        "\n",
        "        # Initialize final_wide_data_columns with time-based categories\n",
        "        final_wide_data_columns = {k: [r for _, r in v] for k, v in time_categorized_remarks.items()}\n",
        "\n",
        "        # original_indexed_cluster_labels is used to map back cluster labels to original df rows\n",
        "        original_indexed_cluster_labels = np.full(len(raw_remarks_list), -2, dtype=int) # -2 for unclustered non-time\n",
        "\n",
        "        # final_column_name_map maps cluster IDs to generated category names\n",
        "        final_column_name_map = {}\n",
        "\n",
        "        if english_remarks_with_indices_original:\n",
        "            print(\"\\n--- Processing non-time English remarks for clustering ---\")\n",
        "            english_remark_texts = [r for _, r in english_remarks_with_indices_original]\n",
        "            english_remark_original_indices = [i for i, _ in english_remarks_with_indices_original]\n",
        "\n",
        "            # Determine number of clusters for KMeans, capping at max_remark_clusters_limit\n",
        "            n_clusters_for_kmeans = min(max_remark_clusters_limit, len(english_remark_texts))\n",
        "            if n_clusters_for_kmeans > 0: # Ensure we don't try to cluster with 0 clusters\n",
        "                cluster_labels = cluster_remarks(english_remark_texts, n_clusters_for_kmeans, batch_size)\n",
        "                print(f\"    [Clustering] Found {len(set(cluster_labels))} initial clusters.\")\n",
        "\n",
        "                # Apply cluster labels back to original remark indices\n",
        "                for i, clustered_label in enumerate(cluster_labels):\n",
        "                    original_indexed_cluster_labels[english_remark_original_indices[i]] = clustered_label\n",
        "\n",
        "                # Get unique cluster IDs for naming\n",
        "                final_unique_clusters = sorted([c for c in set(cluster_labels) if c != -1]) # Exclude noise (-1 if DBSCAN was used)\n",
        "                print(f\"    [Gen AI Naming] Naming {len(final_unique_clusters)} final clusters.\")\n",
        "\n",
        "                used_final_names = set(time_columns) # Keep track of names already in use (including time categories)\n",
        "                for cluster_id in final_unique_clusters:\n",
        "                    cluster_texts_original = [english_remark_texts[j] for j, label in enumerate(cluster_labels) if label == cluster_id]\n",
        "                    top_keywords = get_top_keywords(cluster_texts_original)\n",
        "                    print(f\"    [Keywords] Top keywords for cluster {cluster_id}: {', '.join(top_keywords)}\")\n",
        "\n",
        "                    proposed_final_name = get_genai_cluster_name(cluster_texts_original, top_keywords)\n",
        "                    final_name = get_unique_name(proposed_final_name, used_final_names, str(cluster_id))\n",
        "                    final_column_name_map[cluster_id] = final_name\n",
        "                    used_final_names.add(final_name.lower())\n",
        "                    print(f\"    Final Category Name: '{final_name}'\")\n",
        "                    final_wide_data_columns[final_name] = cluster_texts_original\n",
        "            else:\n",
        "                print(\"    [Clustering] Not enough English remarks for clustering.\")\n",
        "\n",
        "        # Handle unclustered English remarks (if any)\n",
        "        uncategorized_english_remarks = [(original_idx, r) for original_idx, r in english_remarks_with_indices_original if original_indexed_cluster_labels[original_idx] == -1] # Assuming -1 for noise/unclustered\n",
        "        if uncategorized_remarks := [r for _, r in uncategorized_english_remarks]:\n",
        "            col_name = get_unique_name(\"Uncategorized English Remarks\", set(final_wide_data_columns.keys()).union(set(final_column_name_map.values())), \"uncat_en\")\n",
        "            final_wide_data_columns[col_name] = uncategorized_remarks\n",
        "            print(f\"\\nAdded column: '{col_name}' for {len(uncategorized_remarks)} remarks.\")\n",
        "\n",
        "        # Handle other language remarks\n",
        "        if other_remarks := [r for _, r in other_remarks_with_indices_original]:\n",
        "            col_name = get_unique_name(\"Other Language Remarks\", set(final_wide_data_columns.keys()).union(set(final_column_name_map.values())), \"other_lang\")\n",
        "            final_wide_data_columns[col_name] = other_remarks\n",
        "            print(f\"Added column: '{col_name}' for {len(other_remarks)} remarks.\")\n",
        "\n",
        "        # Create wide format DataFrame\n",
        "        max_rows = max(len(remarks) for remarks in final_wide_data_columns.values()) if final_wide_data_columns else 0\n",
        "        df_results_wide = pd.DataFrame({\n",
        "            col: remarks + [\"\"] * (max_rows - len(remarks))\n",
        "            for col, remarks in final_wide_data_columns.items()\n",
        "        })\n",
        "\n",
        "        print(\"\\n--- Merging non-time columns ---\")\n",
        "        non_time_columns_pre_merge = [col for col in df_results_wide.columns if col not in time_columns]\n",
        "        if len(non_time_columns_pre_merge) > 4: # Only attempt merge if there are more than 4 non-time columns\n",
        "            df_results_wide = merge_similar_columns(df_results_wide)\n",
        "        else:\n",
        "            print(f\"Skipping semantic merging. Number of non-time columns ({len(non_time_columns_pre_merge)}) is already at or below the target of 4.\")\n",
        "\n",
        "\n",
        "        print(f\"\\nCategorization complete. Column counts: { {k: len([x for x in df_results_wide[k] if x]) for k in df_results_wide.columns} }\")\n",
        "        print(\"\\n--- Validating Categories ---\")\n",
        "        for col in df_results_wide.columns:\n",
        "            print(f\"Category '{col}' ({len([x for x in df_results_wide[col] if x])} remarks):\")\n",
        "            for r in df_results_wide[col][:min(5, len(df_results_wide[col]))]:\n",
        "                if r:\n",
        "                    print(f\"   - {r[:100]}...\")\n",
        "\n",
        "        save_results(df_results_wide, output_excel_path)\n",
        "        print(\"\\n--- Sample Results ---\")\n",
        "        print(df_results_wide.head())\n",
        "        print(f\"\\n--- Script completed in {time.time() - start_time:.2f} seconds ---\")\n",
        "\n",
        "    except FileNotFoundError as fnfe:\n",
        "        print(f\"\\nERROR: File not found. Please check 'excel_file_path'. Details: {fnfe}\")\n",
        "        exit(1)\n",
        "    except KeyError as ke:\n",
        "        print(f\"\\nERROR: Column not found. Please check 'text_column_name' or 'duration_column_name'. Details: {ke}\")\n",
        "        exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK stopwords are downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Define stopwords (from your original code, ensure consistency)\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'area', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it', 'its', 'her', 'their', 'our',\n",
        "    'what', 'where', 'how', 'why', 'who', 'whom', 'which', 'whether',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hr', 'hrs', 'hour', 'hours', 'time', 'date', 'week', 'month', 'year', 'ago',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'code', 'id', 'location', 'address', 'phone', 'mobile', 'call', 'report', 'registered',\n",
        "    'ok', 'yes', 'no', 'not', 'hi', 'hello', 'sir', 'madam', 'pls', 'please', 'regards', 'type', 'urban', 'complaint', 'detail', 'general',\n",
        "    'kv', 'tf', 'na', 'service', 'request', 'feedback', 'query', 'regarding', 'about', 'given', 'areas', 'village'\n",
        "])\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Removes extra whitespace and standardizes text (generic version).\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', str(text).strip()).lower()\n",
        "    return text\n",
        "\n",
        "# --- Step 1.1: Load the Excel file ---\n",
        "excel_file_path = \"./Book1.xlsx\" # <--- Confirm this is the actual file name\n",
        "\n",
        "try:\n",
        "    df_raw_labeled = pd.read_excel(excel_file_path, header=0) # header=0 means first row is header\n",
        "    print(f\"Successfully loaded '{excel_file_path}'. Shape: {df_raw_labeled.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the raw data (Book1.xlsx content):\")\n",
        "    print(df_raw_labeled.head())\n",
        "    print(\"\\nColumns (Categories) identified from the raw data:\")\n",
        "    print(df_raw_labeled.columns.tolist())\n",
        "\n",
        "\n",
        "    desired_category_labels = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"More than 4 hours\", # The single target label for this category\n",
        "        \"Consumer Power Supply Failures\",\n",
        "        \"Failed Pole Incident Category\",\n",
        "        \"Other Language Remarks\",\n",
        "        \"Partial Phase Supply Failure\",\n",
        "        \"Transformer Damage Causing Outages\"\n",
        "    ]\n",
        "\n",
        "    loaded_to_label_map = {}\n",
        "    seen_labels = {} # To handle pandas' .1, .2 suffixes\n",
        "\n",
        "    for col in df_raw_labeled.columns:\n",
        "        # Normalize column name for comparison\n",
        "        normalized_col = col.split('.')[0] # Remove .1, .2 suffixes if present\n",
        "\n",
        "        if normalized_col in desired_category_labels:\n",
        "            if normalized_col in seen_labels:\n",
        "                loaded_to_label_map[col] = normalized_col\n",
        "            else:\n",
        "                loaded_to_label_map[col] = col # No suffix, direct map\n",
        "                seen_labels[normalized_col] = True\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' from Excel file is not in the list of desired categories. It will be ignored for training data. If this is a category, please add it to 'desired_category_labels'.\")\n",
        "\n",
        "    active_category_columns_in_df = list(loaded_to_label_map.keys())\n",
        "\n",
        "    print(f\"\\nIdentified {len(set(loaded_to_label_map.values()))} Unique Category Labels for Training:\")\n",
        "    print(list(set(loaded_to_label_map.values())))\n",
        "    print(f\"\\nColumns from Excel file that will be processed (including any pandas-generated suffixes):\")\n",
        "    print(active_category_columns_in_df)\n",
        "\n",
        "    # --- Step 1.3: Transform the wide format into a long format (remark_cleaned, category) ---\n",
        "    labeled_data_for_training = []\n",
        "\n",
        "    for category_col_in_df in active_category_columns_in_df:\n",
        "        # Get the standardized category label for this column\n",
        "        standard_category_label = loaded_to_label_map[category_col_in_df]\n",
        "\n",
        "        # Iterate through each non-empty remark in that column\n",
        "        for remark_entry in df_raw_labeled[category_col_in_df].dropna():\n",
        "            cleaned_remark = clean_text(str(remark_entry)) # Ensure it's a string before cleaning\n",
        "\n",
        "            if cleaned_remark: # Only add if the cleaned remark is not empty\n",
        "                labeled_data_for_training.append({\n",
        "                    'remark_original': str(remark_entry), # Keep original for reference\n",
        "                    'remark_cleaned': cleaned_remark,\n",
        "                    'category': standard_category_label # Use the standardized label\n",
        "                })\n",
        "\n",
        "    # Create the DataFrame for training\n",
        "    df_labeled_for_training = pd.DataFrame(labeled_data_for_training)\n",
        "\n",
        "    print(f\"\\nTransformed data for training shape: {df_labeled_for_training.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the transformed labeled data for training (remark_cleaned, category):\")\n",
        "    print(df_labeled_for_training.head())\n",
        "    print(\"\\nValue counts for 'category' (training labels):\")\n",
        "    print(df_labeled_for_training['category'].value_counts())\n",
        "    print(f\"\\nTotal unique categories identified in training data: {df_labeled_for_training['category'].nunique()}\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{excel_file_path}' was not found. Please ensure it's in the same directory as this script.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    exit()\n",
        "\n",
        "# Store relevant data for the next steps\n",
        "global_df_labeled_for_training = df_labeled_for_training\n",
        "global_original_category_column_names = desired_category_labels # These are the names we will use for output columns, ensuring uniqueness"
      ],
      "metadata": {
        "id": "1Mv0qdrvYHCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Ensure global_df_labeled_for_training is available from Step 1\n",
        "if 'global_df_labeled_for_training' not in globals():\n",
        "    print(\"Error: global_df_labeled_for_training not found. Please run Step 1 code first.\")\n",
        "    exit()\n",
        "\n",
        "df_training = global_df_labeled_for_training.copy()\n",
        "\n",
        "print(\"\\n--- Step 2: Feature Extraction (Sentence Embeddings) and Model Training (Logistic Regression) ---\")\n",
        "\n",
        "# --- Step 2.1: Split Data into Training and Testing Sets ---\n",
        "# X will be the cleaned remarks, y will be the categories\n",
        "X = df_training['remark_cleaned'].tolist() # Convert to list for SentenceTransformer\n",
        "y = df_training['category'].tolist()\n",
        "\n",
        "# Stratify ensures that the proportion of categories is maintained in both train and test sets\n",
        "# This is crucial for imbalanced datasets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Dataset split: Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
        "print(f\"Unique categories in training set: {len(set(y_train))}\")\n",
        "print(f\"Unique categories in testing set: {len(set(y_test))}\")\n",
        "\n",
        "\n",
        "# --- Step 2.2: Generate Sentence Embeddings ---\n",
        "print(\"Loading SentenceTransformer model...\")\n",
        "# Using 'all-MiniLM-L6-v2' as it's efficient and performs well.\n",
        "# Ensure you have a GPU for 'cuda' device, otherwise use 'cpu'.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_embedding = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "print(f\"SentenceTransformer model loaded on device: {device}\")\n",
        "\n",
        "print(\"Generating embeddings for training data...\")\n",
        "start_time_embed_train = time.time()\n",
        "X_train_embeddings = model_embedding.encode(X_train, show_progress_bar=True, convert_to_numpy=True)\n",
        "print(f\"Training embeddings generated in {time.time() - start_time_embed_train:.2f} seconds.\")\n",
        "\n",
        "print(\"Generating embeddings for testing data...\")\n",
        "start_time_embed_test = time.time()\n",
        "X_test_embeddings = model_embedding.encode(X_test, show_progress_bar=True, convert_to_numpy=True)\n",
        "print(f\"Testing embeddings generated in {time.time() - start_time_embed_test:.2f} seconds.\")\n",
        "\n",
        "print(f\"Shape of training embeddings: {X_train_embeddings.shape}\")\n",
        "print(f\"Shape of testing embeddings: {X_test_embeddings.shape}\")\n",
        "\n",
        "\n",
        "# --- Step 2.3: Train a Classification Model (Logistic Regression) ---\n",
        "print(\"\\nTraining Logistic Regression model...\")\n",
        "start_time_train_model = time.time()\n",
        "\n",
        "# Logistic Regression parameters:\n",
        "# max_iter: Increased for convergence with larger datasets/complex features\n",
        "# solver: 'liblinear' is good for smaller datasets; 'lbfgs' is good for larger ones.\n",
        "# 'lbfgs' also supports multi_class='multinomial' for true multi-class classification.\n",
        "# class_weight: 'balanced' can help with imbalanced datasets by giving more weight to minority classes.\n",
        "classifier_model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    solver='lbfgs', # 'liblinear' or 'lbfgs' or 'saga'\n",
        "    multi_class='auto', # or 'multinomial' for lbfgs\n",
        "    class_weight='balanced', # Important for imbalanced classes\n",
        "    random_state=42,\n",
        "    n_jobs=-1 # Use all available CPU cores\n",
        ")\n",
        "\n",
        "classifier_model.fit(X_train_embeddings, y_train)\n",
        "print(f\"Logistic Regression model trained in {time.time() - start_time_train_model:.2f} seconds.\")\n",
        "\n",
        "\n",
        "# --- Step 2.4: Evaluate the Model ---\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "y_pred = classifier_model.predict(X_test_embeddings)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0)) # zero_division=0 avoids warning for classes with no true samples in test set\n",
        "\n",
        "# Store models and data for next step\n",
        "global_model_embedding = model_embedding\n",
        "global_classifier_model = classifier_model\n",
        "global_y_test = y_test\n",
        "global_y_pred = y_pred\n",
        "\n",
        "print(\"\\nStep 2 completed. Review the accuracy and classification report.\")"
      ],
      "metadata": {
        "id": "nxd82hgEYejU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Download NLTK stopwords\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Define stopwords (from your original code, ensure consistency)\n",
        "UNIVERSAL_STOPWORDS = set(nltk_stopwords.words('english') + [\n",
        "    'the', 'area', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'from', 'by', 'this', 'that', 'it', 'its', 'her', 'their', 'our',\n",
        "    'what', 'where', 'how', 'why', 'who', 'whom', 'which', 'whether',\n",
        "    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night', 'day', 'days', 'hr', 'hrs', 'hour', 'hours', 'time', 'date', 'week', 'month', 'year', 'ago',\n",
        "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
        "    'consumer', 'customer', 'number', 'no', 'code', 'id', 'location', 'address', 'phone', 'mobile', 'call', 'report', 'registered',\n",
        "    'ok', 'yes', 'no', 'not', 'hi', 'hello', 'sir', 'madam', 'pls', 'please', 'regards', 'type', 'urban', 'complaint', 'detail', 'general',\n",
        "    'kv', 'tf', 'na', 'service', 'request', 'feedback', 'query', 'regarding', 'about', 'given', 'areas', 'village'\n",
        "])\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Removes extra whitespace and standardizes text (generic version).\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text) # Ensure text is a string\n",
        "\n",
        "    # Remove any stray non-alphanumeric characters that might remain after emoji removal,\n",
        "    # then handle whitespace and lowercase.\n",
        "    # Keep alphanumeric characters and basic punctuation that might be relevant to remarks\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"-/]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text.strip()).lower()\n",
        "    return text\n",
        "\n",
        "def extract_time_features(remarks_series: pd.Series) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts numerical time features (hours, presence of AM/PM) from a Series of remarks.\n",
        "    Returns a DataFrame with these features.\n",
        "    \"\"\"\n",
        "    remarks_series = remarks_series.fillna(\"\").str.lower()\n",
        "\n",
        "    hour_pattern = r'(\\d+\\.?\\d*)\\s*(?:hr|hrs|hour|hours|h)'\n",
        "    day_pattern = r'(\\d+\\.?\\d*)\\s*(?:day|days)'\n",
        "    am_pm_pattern = r'\\b(\\d{1,2}(:\\d{2})?)\\s*(am|pm)\\b'\n",
        "\n",
        "    hours_extracted = remarks_series.str.extract(hour_pattern)[0].astype(float).fillna(0)\n",
        "    days_extracted = remarks_series.str.extract(day_pattern)[0].astype(float).fillna(0)\n",
        "\n",
        "    hours_from_days = days_extracted * 24\n",
        "    combined_hours = hours_extracted + hours_from_days\n",
        "\n",
        "    is_am_pm_mentioned = remarks_series.str.contains(am_pm_pattern, regex=True).astype(int)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'extracted_hours': combined_hours,\n",
        "        'is_am_pm_mentioned': is_am_pm_mentioned\n",
        "    })\n",
        "\n",
        "\n",
        "def main_classification_pipeline():\n",
        "    # --- Configuration ---\n",
        "    labeled_data_excel_path = \"./Book1.xlsx\"\n",
        "    new_remarks_excel_path = \"./Supply.xlsx\"\n",
        "    output_excel_path = \"./categorized_remarks_ML_model_compacted_time_aware.xlsx\" # Output filename\n",
        "\n",
        "    # These are the exact column names from your Book1.xlsx image\n",
        "    desired_category_labels = [\n",
        "        \"Less than 4 hours\",\n",
        "        \"More than 12 hours\",\n",
        "        \"More than 24 hours\",\n",
        "        \"More than 4 hours\",\n",
        "        \"Consumer Power Supply Failures\",\n",
        "        \"Failed Pole Incident Category\",\n",
        "        \"Other Language Remarks\",\n",
        "        \"Partial Phase Supply Failure\",\n",
        "        \"Transformer Damage Causing Outages\"\n",
        "    ]\n",
        "\n",
        "    # Column containing remarks in the NEW_REMARKS_EXCEL_PATH file (e.g., Supply.xlsx)\n",
        "    remarks_column_in_new_file = \"REMARKS\"\n",
        "\n",
        "    print(\"--- Starting Supervised ML Remark Categorization Pipeline (Time-Aware) ---\")\n",
        "    start_full_pipeline_time = time.time()\n",
        "\n",
        "    # --- Step 1: Data Preparation for Training ---\n",
        "    print(\"\\n--- Step 1: Data Preparation for Training ---\")\n",
        "    try:\n",
        "        df_raw_labeled = pd.read_excel(labeled_data_excel_path, header=0)\n",
        "        print(f\"Successfully loaded '{labeled_data_excel_path}'. Shape: {df_raw_labeled.shape}\")\n",
        "        print(\"First 5 rows of the raw labeled data:\")\n",
        "        print(df_raw_labeled.head())\n",
        "        print(\"Columns identified from the raw labeled data:\")\n",
        "        print(df_raw_labeled.columns.tolist())\n",
        "\n",
        "        loaded_to_label_map = {}\n",
        "        seen_labels = {}\n",
        "\n",
        "        for col in df_raw_labeled.columns:\n",
        "            normalized_col = col.split('.')[0]\n",
        "\n",
        "            if normalized_col in desired_category_labels:\n",
        "                if normalized_col in seen_labels:\n",
        "                    loaded_to_label_map[col] = normalized_col\n",
        "                else:\n",
        "                    loaded_to_label_map[col] = col\n",
        "                    seen_labels[normalized_col] = True\n",
        "            else:\n",
        "                print(f\"Warning: Column '{col}' from '{labeled_data_excel_path}' is not in the list of desired categories. It will be ignored for training data. If this is a category, please add it to 'desired_category_labels'.\")\n",
        "\n",
        "        active_category_columns_in_df = list(loaded_to_label_map.keys())\n",
        "\n",
        "        print(f\"\\nIdentified {len(set(loaded_to_label_map.values()))} Unique Category Labels for Training:\")\n",
        "        print(list(set(loaded_to_label_map.values())))\n",
        "        print(f\"Columns from '{labeled_data_excel_path}' that will be processed:\")\n",
        "        print(active_category_columns_in_df)\n",
        "\n",
        "        labeled_data_for_training = []\n",
        "        for category_col_in_df in active_category_columns_in_df:\n",
        "            standard_category_label = loaded_to_label_map[category_col_in_df]\n",
        "            for remark_entry in df_raw_labeled[category_col_in_df].dropna():\n",
        "                cleaned_remark = clean_text(str(remark_entry))\n",
        "                if cleaned_remark:\n",
        "                    labeled_data_for_training.append({\n",
        "                        'remark_original': str(remark_entry),\n",
        "                        'remark_cleaned': cleaned_remark,\n",
        "                        'category': standard_category_label\n",
        "                    })\n",
        "\n",
        "        df_labeled_for_training = pd.DataFrame(labeled_data_for_training)\n",
        "        print(f\"\\nTransformed data for training shape: {df_labeled_for_training.shape}\")\n",
        "        print(\"First 5 rows of the transformed labeled data for training:\")\n",
        "        print(df_labeled_for_training.head())\n",
        "        print(\"Value counts for 'category' (training labels):\")\n",
        "        print(df_labeled_for_training['category'].value_counts())\n",
        "        print(f\"Total unique categories identified in training data: {df_labeled_for_training['category'].nunique()}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: The file '{labeled_data_excel_path}' not found. Please ensure it's in the same directory.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 1: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- Step 2: Feature Extraction (Sentence Embeddings + Time Features) and Model Training ---\n",
        "    print(\"\\n--- Step 2: Feature Extraction (Sentence Embeddings + Time Features) and Model Training ---\")\n",
        "    try:\n",
        "        X_train_data = df_labeled_for_training['remark_cleaned'].tolist()\n",
        "        y_train_labels = df_labeled_for_training['category'].tolist()\n",
        "\n",
        "        X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "            pd.Series(X_train_data), y_train_labels, test_size=0.2, random_state=42, stratify=y_train_labels\n",
        "        )\n",
        "\n",
        "        # Extract and scale time features for training\n",
        "        scaler = StandardScaler() # Initialize scaler here\n",
        "        X_train_time_features_df = extract_time_features(X_train_text)\n",
        "        X_train_time_features_scaled = scaler.fit_transform(X_train_time_features_df)\n",
        "\n",
        "        X_test_time_features_df = extract_time_features(X_test_text)\n",
        "        X_test_time_features_scaled = scaler.transform(X_test_time_features_df) # Use fitted scaler\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Loading SentenceTransformer model on device: {device}...\")\n",
        "        model_embedding = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "        print(\"Generating embeddings for training text...\")\n",
        "        X_train_embeddings = model_embedding.encode(X_train_text.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
        "        print(\"Generating embeddings for testing text...\")\n",
        "        X_test_embeddings = model_embedding.encode(X_test_text.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "        X_train_combined_features = np.hstack((X_train_embeddings, X_train_time_features_scaled))\n",
        "        X_test_combined_features = np.hstack((X_test_embeddings, X_test_time_features_scaled))\n",
        "\n",
        "        print(\"\\nTraining Logistic Regression model on combined features...\")\n",
        "        classifier_model = LogisticRegression(\n",
        "            max_iter=1000, solver='lbfgs', multi_class='auto', class_weight='balanced', random_state=42, n_jobs=-1\n",
        "        )\n",
        "        classifier_model.fit(X_train_combined_features, y_train)\n",
        "        print(\"Logistic Regression model trained.\")\n",
        "\n",
        "        print(\"\\n--- Model Evaluation ---\")\n",
        "        y_pred = classifier_model.predict(X_test_combined_features)\n",
        "        print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 2: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- Step 3: Prediction and Output Structuring ---\n",
        "    print(\"\\n--- Step 3: Prediction and Output Structuring (Compacted Output) ---\")\n",
        "    try:\n",
        "        df_new_remarks_raw = pd.read_excel(new_remarks_excel_path)\n",
        "        print(f\"Successfully loaded new remarks from '{new_remarks_excel_path}'. Shape: {df_new_remarks_raw.shape}\")\n",
        "\n",
        "        if remarks_column_in_new_file not in df_new_remarks_raw.columns:\n",
        "            raise KeyError(f\"Column '{remarks_column_in_new_file}' not found in '{new_remarks_excel_path}'. Available columns: {df_new_remarks_raw.columns.tolist()}\")\n",
        "\n",
        "        new_remarks_data = []\n",
        "        for idx, remark_raw in df_new_remarks_raw[remarks_column_in_new_file].items():\n",
        "            cleaned = clean_text(remark_raw)\n",
        "            if cleaned:\n",
        "                new_remarks_data.append({'original_index': idx, 'remark_raw': remark_raw, 'remark_cleaned': cleaned})\n",
        "\n",
        "        df_remarks_to_classify = pd.DataFrame(new_remarks_data)\n",
        "        print(f\"Extracted {len(df_remarks_to_classify)} valid remarks for classification.\")\n",
        "\n",
        "        print(\"\\nGenerating embeddings for new remarks...\")\n",
        "        new_remarks_embeddings = model_embedding.encode(\n",
        "            df_remarks_to_classify['remark_cleaned'].tolist(),\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        # Extract and scale time features for new remarks\n",
        "        print(\"Extracting and scaling time features for new remarks...\")\n",
        "        new_remarks_time_features_df = extract_time_features(df_remarks_to_classify['remark_cleaned'])\n",
        "        new_remarks_time_features_scaled = scaler.transform(new_remarks_time_features_df) # Use the *same* scaler from training\n",
        "\n",
        "        # Concatenate for prediction\n",
        "        new_remarks_combined_features = np.hstack((new_remarks_embeddings, new_remarks_time_features_scaled))\n",
        "\n",
        "        print(\"\\nPredicting categories for new remarks...\")\n",
        "        predicted_categories = classifier_model.predict(new_remarks_combined_features)\n",
        "        df_remarks_to_classify['predicted_category'] = predicted_categories\n",
        "        print(\"First 5 remarks with predicted categories:\")\n",
        "        print(df_remarks_to_classify.head())\n",
        "\n",
        "        # --- MODIFIED PART FOR COMPACTED OUTPUT ---\n",
        "        print(\"\\nCompacting output into wide format by pushing remarks to the top of each category column...\")\n",
        "\n",
        "        categorized_remarks_by_column = defaultdict(list)\n",
        "\n",
        "        for idx, row_data in df_remarks_to_classify.iterrows():\n",
        "            remark = row_data['remark_raw']\n",
        "            predicted_cat = row_data['predicted_category']\n",
        "            categorized_remarks_by_column[predicted_cat].append(remark)\n",
        "\n",
        "        max_remarks_in_any_cat = 0\n",
        "        if categorized_remarks_by_column:\n",
        "            max_remarks_in_any_cat = max(len(v) for v in categorized_remarks_by_column.values())\n",
        "\n",
        "        df_output_compacted_wide = pd.DataFrame({\n",
        "            col: categorized_remarks_by_column.get(col, []) + [''] * (max_remarks_in_any_cat - len(categorized_remarks_by_column.get(col, [])))\n",
        "            for col in sorted(list(set(desired_category_labels)))\n",
        "        })\n",
        "\n",
        "        print(f\"\\nFinal Compacted Wide Output DataFrame shape: {df_output_compacted_wide.shape}\")\n",
        "        print(\"First 5 rows of the Final Compacted Wide Output DataFrame:\")\n",
        "        print(df_output_compacted_wide.head())\n",
        "\n",
        "        print(f\"\\nSaving results to '{output_excel_path}'...\")\n",
        "        df_output_compacted_wide.to_excel(output_excel_path, index=False)\n",
        "        print(\"Results saved successfully.\")\n",
        "\n",
        "        # --- NEW LINES ADDED HERE FOR MODEL SAVING ---\n",
        "        print(\"\\n--- Saving trained models for future use ---\")\n",
        "        try:\n",
        "            joblib.dump(model_embedding, 'sentence_transformer_model.pkl')\n",
        "            joblib.dump(classifier_model, 'logistic_regression_classifier.pkl')\n",
        "            joblib.dump(scaler, 'scaler_for_time_features.pkl')\n",
        "            print(\"Models (Sentence Transformer, Classifier, Scaler) saved successfully to .pkl files.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to save models: {e}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "        # --- END OF NEW LINES ---\n",
        "\n",
        "        # --- Final Verification of Total Count ---\n",
        "        print(\"\\n--- Final Verification of Total Count ---\")\n",
        "        df_categorized_check = pd.read_excel(output_excel_path)\n",
        "        print(f\"Shape of the re-loaded compacted categorized file: {df_categorized_check.shape}\")\n",
        "\n",
        "        total_categorized_remarks_from_file = 0\n",
        "        for col in df_categorized_check.columns:\n",
        "            total_categorized_remarks_from_file += df_categorized_check[col].apply(lambda x: pd.notna(x) and str(x).strip() != '').sum()\n",
        "\n",
        "        print(f\"Total number of remarks in the compacted categorized file (non-empty cell count): {total_categorized_remarks_from_file}\")\n",
        "        print(f\"Number of remarks successfully extracted and classified (excluding empty after cleaning): {len(df_remarks_to_classify)}\")\n",
        "\n",
        "        if total_categorized_remarks_from_file == len(df_remarks_to_classify):\n",
        "            print(\"All extracted and classified remarks successfully written to compacted output file.\")\n",
        "        else:\n",
        "            print(f\"Mismatch: {len(df_remarks_to_classify) - total_categorized_remarks_from_file} remarks missing from final file count. Investigate Excel saving/loading or if some cells are truly empty/NaN in source.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: The file '{new_remarks_excel_path}' not found. Please ensure it's in the same directory.\")\n",
        "        return\n",
        "    except KeyError as e:\n",
        "        print(f\"ERROR: Missing expected column in '{new_remarks_excel_path}'. Details: {e}\")\n",
        "        if 'df_new_remarks_raw' in locals():\n",
        "            print(\"Available columns in your Excel file are:\", df_new_remarks_raw.columns.tolist())\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 3: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Supervised ML Categorization Pipeline Completed in {time.time() - start_full_pipeline_time:.2f} seconds ---\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_classification_pipeline()"
      ],
      "metadata": {
        "id": "S4o254cciJ4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Others Column Reallocation</h1>"
      ],
      "metadata": {
        "id": "5ygArxZKl0Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from joblib import Parallel, delayed\n",
        "import cudf\n",
        "\n",
        "# --- Constants ---\n",
        "API_KEY = \"AIzaSyAwLVHm49YsJu4PK6ilxc7MiwLxI6sBU7E\"\n",
        "GEMINI_MODEL_NAME = 'gemini-2.5-pro'\n",
        "MAX_RETRIES = 3\n",
        "INITIAL_DELAY_SECONDS = 2\n",
        "BATCH_SIZE = 100\n",
        "MIN_REMARKS_FOR_NEW_COLUMN_SUGGESTION = 3\n",
        "# MAX_REMARKS_FOR_NEW_COLUMN_SUGGESTION = 2000 # No longer directly limiting for the prompt, but be aware of total token limits!\n",
        "\n",
        "# --- Gemini API Configuration ---\n",
        "def configure_gemini_model():\n",
        "    try:\n",
        "        genai.configure(api_key=API_KEY)\n",
        "        return genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini API: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# --- Load Excel File ---\n",
        "def load_excel_data(file_path):\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        return cudf.DataFrame.from_pandas(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Excel file from '{file_path}': {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# --- Call Gemini with Retry ---\n",
        "def call_gemini_with_retries(prompt, model, task_description=\"processing\", max_retries=MAX_RETRIES, initial_delay=INITIAL_DELAY_SECONDS):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            if response and response.text:\n",
        "                return response.text.strip()\n",
        "            else:\n",
        "                raise ValueError(f\"Gemini returned empty or no text during {task_description}.\")\n",
        "        except (ConnectionResetError, genai.types.BlockedPromptException, genai.types.StopCandidateException) as e:\n",
        "            delay = initial_delay * (2 ** attempt)\n",
        "            print(f\"Error (attempt {attempt + 1}) during {task_description}: {e}. Retrying in {delay:.2f} seconds...\")\n",
        "            time.sleep(delay)\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error (attempt {attempt + 1}) during {task_description}: {e}.\")\n",
        "            raise\n",
        "    print(f\"Failed to complete {task_description} after {max_retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# --- Categorize a Single Remark ---\n",
        "def categorize_remark(remark, columns, model):\n",
        "    if not isinstance(remark, str):\n",
        "        return None\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Categorize the following remark into **one and only one** of the provided columns. The categorization must be an **exact, clear, unambiguous, and semantic match**.\n",
        "    This means the remark's core meaning and intent must align perfectly with the established definition and typical content of a single column.\n",
        "    **Do not infer, generalize, or make exceptions.** This is a data set of remarks RELATED TO SUPPLY (Like problem with Tranformers, Phase,\n",
        "    Pole, power cut etc), so make sure that the remarks which are categorized IN SOME WAY OR ANOTHER ARE RELATED TO SUPPLY, it should not be the case\n",
        "    that the remarks which are not of supply (example : meter related or bill related remarks) are categorized in one of the provided columns, for such\n",
        "    remarks return 'None'. If the remark does not precisely and directly fit *one specific column* without any doubt,\n",
        "    or if it could arguably fit more than one, return 'None'.\n",
        "    **Columns:** {', '.join(columns)}\n",
        "    **Remark:** {remark.strip()}\n",
        "    Return only the column name or 'None'.\n",
        "    \"\"\"\n",
        "    task_desc = f\"categorizing remark '{remark[:50]}...'\"\n",
        "    return call_gemini_with_retries(prompt, model, task_desc)\n",
        "\n",
        "\n",
        "# --- Suggest New Columns from Uncategorized Remarks ---\n",
        "def suggest_new_columns(uncategorized_remarks, model):\n",
        "    if not uncategorized_remarks:\n",
        "        return {}\n",
        "\n",
        "    # Modification: Send all remarks for suggestion (within reasonable prompt limits)\n",
        "    # Be aware: If 'uncategorized_remarks' is very large, this can exceed token limits.\n",
        "    # It's generally better to sample or batch here if remark count is high.\n",
        "    remarks_for_prompt = [r[:100].strip() for r in uncategorized_remarks] # Still truncate individual remarks to save tokens within the overall prompt\n",
        "\n",
        "    if len(remarks_for_prompt) < MIN_REMARKS_FOR_NEW_COLUMN_SUGGESTION:\n",
        "        return {}\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following uncategorized remarks that did not fit into existing columns.\n",
        "    Suggest new column names that effectively group similar issues present in these remarks.\n",
        "    Each suggested column must clearly apply to at least {MIN_REMARKS_FOR_NEW_COLUMN_SUGGESTION} of the provided remarks.\n",
        "    For each column, provide:\n",
        "    - Column name (concise and descriptive)\n",
        "    - Brief description of the types of issues it covers\n",
        "    - Estimated number of remarks from the provided list that would fit this new column\n",
        "\n",
        "    Remarks:\\n{chr(10).join(remarks_for_prompt)}\n",
        "\n",
        "    Format exactly as follows:\n",
        "    Column: [Column Name], Description: [Description], Estimated Count: [Count]\n",
        "    If no columns can be clearly identified that meet the minimum threshold, return 'No suggestions'.\n",
        "    \"\"\"\n",
        "    task_desc = \"suggesting new columns\"\n",
        "    response_text = call_gemini_with_retries(prompt, model, task_desc)\n",
        "\n",
        "    new_columns = {}\n",
        "    if response_text and response_text.lower() != 'no suggestions':\n",
        "        for line in response_text.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if line.startswith('Column:'):\n",
        "                try:\n",
        "                    parts = line.split(', Description:')\n",
        "                    column_name = parts[0].replace('Column:', '').strip()\n",
        "                    desc_parts = parts[1].split(', Estimated Count:')\n",
        "                    description = desc_parts[0].strip()\n",
        "                    count = int(desc_parts[1].strip())\n",
        "                    if count >= MIN_REMARKS_FOR_NEW_COLUMN_SUGGESTION:\n",
        "                        new_columns[column_name] = {'description': description, 'count': count}\n",
        "                except (IndexError, ValueError) as e:\n",
        "                    print(f\"Warning: Could not parse suggested column line: '{line}'. Error: {e}\")\n",
        "    return new_columns\n",
        "\n",
        "\n",
        "# --- Process Remarks ---\n",
        "def process_remarks_and_suggest_columns(input_file_path, output_file_path):\n",
        "    df_cudf = load_excel_data(input_file_path)\n",
        "\n",
        "    if 'Others' not in df_cudf.columns:\n",
        "        print(\"Error: 'Others' column not found in the Excel sheet.\")\n",
        "        return\n",
        "\n",
        "    df_pd = df_cudf.to_pandas()\n",
        "\n",
        "    model = configure_gemini_model()\n",
        "    existing_columns = [col for col in df_pd.columns if col != 'Others']\n",
        "\n",
        "    total_rows = len(df_pd)\n",
        "    print(f\"Starting remark categorization for {total_rows} rows...\")\n",
        "\n",
        "    for start_idx in range(0, total_rows, BATCH_SIZE):\n",
        "        end_idx = min(start_idx + BATCH_SIZE, total_rows)\n",
        "        print(f\"Processing batch: rows {start_idx + 1} to {end_idx}...\")\n",
        "\n",
        "        batch_remarks_data = df_pd.loc[start_idx:end_idx-1, ['Others']].dropna()\n",
        "\n",
        "        if batch_remarks_data.empty:\n",
        "            continue\n",
        "\n",
        "        remarks_to_categorize = [\n",
        "            (idx, str(remark)) for idx, remark in batch_remarks_data['Others'].items()\n",
        "        ]\n",
        "\n",
        "        results = Parallel(n_jobs=8)(\n",
        "            delayed(categorize_remark)(remark, existing_columns, model)\n",
        "            for _, remark in remarks_to_categorize\n",
        "        )\n",
        "\n",
        "        for (global_index, original_remark), target_column in zip(remarks_to_categorize, results):\n",
        "            if target_column in existing_columns:\n",
        "                df_pd.at[global_index, target_column] = original_remark\n",
        "                df_pd.at[global_index, 'Others'] = None\n",
        "            # else: the remark implicitly stays in 'Others'\n",
        "\n",
        "    print(\"\\n--- Categorization Complete ---\")\n",
        "\n",
        "    uncategorized_remarks_list = df_pd['Others'].dropna().tolist()\n",
        "\n",
        "    print(f\"\\nSuggesting new columns for {len(uncategorized_remarks_list)} uncategorized remarks...\")\n",
        "    suggested_new_columns = suggest_new_columns(uncategorized_remarks_list, model)\n",
        "\n",
        "    try:\n",
        "        df_pd.to_excel(output_file_path, index=False)\n",
        "        print(f\"Saved updated Excel to: {output_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving output file: {e}\")\n",
        "\n",
        "    if suggested_new_columns:\n",
        "        print(\"\\n--- Suggested New Columns ---\")\n",
        "        for col, info in suggested_new_columns.items():\n",
        "            print(f\"Column: **{col}**\\n  Description: {info['description']}\\n  Count: {info['count']}\\n\")\n",
        "    else:\n",
        "        print(f\"\\nNo new columns suggested (threshold: {MIN_REMARKS_FOR_NEW_COLUMN_SUGGESTION} remarks).\")\n",
        "\n",
        "    print(f\"\\nTotal remaining uncategorized remarks in 'Others' column: {len(uncategorized_remarks_list)}\")\n",
        "\n",
        "\n",
        "# --- Main ---\n",
        "def main():\n",
        "    input_excel_file = '/content/categorized_remarks_final.xlsx'\n",
        "    output_excel_file = './output.xlsx'\n",
        "\n",
        "    if not os.path.exists(input_excel_file):\n",
        "        print(f\"Input file not found: {input_excel_file}\")\n",
        "        return\n",
        "\n",
        "    process_remarks_and_suggest_columns(input_excel_file, output_excel_file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cN8CK82tlyVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM3B626VfYpc5qN1Gg7L5m3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}